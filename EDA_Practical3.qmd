---
title: '(5) Data Transformation - Python'
format: html
execute:
  engine: knitr
---

We are now continuing with Python and pre-processing functions. In this case, we will be exploring feature scaling and transformation.

### Data Set Used :

Breast Cancer Wisconsin (Diagnostic) Data Set

Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: \[K. P. Bennett and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets", Optimization Methods and Software 1, 1992, 23-34\].

```{r, include=FALSE}

library(reticulate)

# one-off setup (if you haven't done it yet)
# install_miniconda()

##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)

use_condaenv("hds-python", required = TRUE)
#py_config()

#conda_install("hds-python", c("jupyter", "plotly"))
```

```{python}
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn import preprocessing
from scipy import stats
import seaborn as sns
from sklearn import datasets
import matplotlib.pyplot as plt
```

```{python}
# Load the dataset
data = datasets.load_breast_cancer() #already included in libraries downloaded
df = pd.DataFrame(data.data, columns=data.feature_names) #pick just 5/30 features

```

```{python}
# Let's look at the first few rows of the dataframe
print(df.head())
```

Let's plot the data before transformation

```{python}

plt.figure(figsize=(10, 6))
sns.kdeplot(data=df)
plt.title("Non-normalised data")
plt.show()
```

It is very difficult to see anything right? This is because we have most data around 0 but then a very long tail to the right!

#### Strategy 1 : Min-Max scaling

Min-Max scaling: This transformation scales and translates each feature individually such that it is in the range \[0, 1\]. The transformed data is stored in df_minmax.

```{python}
#### Strategy 1 : Min-Max scaling
scaler = preprocessing.MinMaxScaler(feature_range=(0,100)) #what happens if you change the 100? 
df_minmax = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)
#print("\nAfter Min-Max Scaling:\n", df_minmax.head())


# Let's plot the data after min-max scaling
plt.figure(figsize=(10, 6))
sns.kdeplot(data=df_minmax)
plt.title("After Min-Max Scaling")
plt.show()


```

### Log transformation:

This transformation applies the natural logarithm to each value in the DataFrame. This is often used when the data is highly skewed, as it can help to make the data more "normal" (i.e., more closely approximate a normal distribution). The transformed data is stored in df_log.

```{python}
# Log Transformation
df_log = df.apply(np.log)
df_log = df_log.replace([np.inf, -np.inf], np.nan)
df_log = df_log.dropna()
print("\nAfter Log Transformation:\n", df_log.head())



plt.figure(figsize=(10, 6))
sns.kdeplot(data=df_log)
plt.title("After Log Transformation")
plt.show()

```

### Z-score normalization:

Also known as standardization, this transformation scales and translates each feature so that it has a mean of 0 and a standard deviation of 1. The transformed data is stored in df_zscore.

Z-score normalization is a type of data standardization where we convert all features in our dataset to have a mean (µ) of 0 and a standard deviation (σ) of 1. The purpose of this transformation is to remove the scale effect of measurements.

The formula for Z-score normalization is:

Z = (X - µ) / σ

where:

Z is the standardized (Z-score normalized) value, X is the original value, µ is the mean of the feature, σ is the standard deviation of the feature. Why do we do this? In machine learning, many algorithms (like K-nearest neighbors, Neural Networks, and others) perform better when their input features are roughly on the same scale and centered around zero. If one feature has a range of -1 to 1, while another feature has a range of -1000 to 1000, the second feature will completely dominate when these features are combined, even though the first feature might be just as important.

After Z-score normalization, every feature in the dataset will have a mean of 0 and a standard deviation of 1, putting them all on roughly the same scale. The data values in each column now represent how many standard deviations the original value was from the mean of that column. This makes it easier to compare different features, and helps many machine learning algorithms perform better.

```{python}

# Z-score normalization
scaler = preprocessing.StandardScaler()
df_zscore = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)
print("\nAfter Z-score Normalization:\n", df_zscore.head())

# Let's plot the data after Z-score normalization
plt.figure(figsize=(10, 6))
sns.kdeplot(data=df_zscore)
plt.title("After Z-score Normalization")
plt.show()

```


#### Your Task

## Question 1:

```         
1.1 Load the Bupa Data set and apply a new transformations from <https://scikit-learn.org/stable/modules/preprocessing.html> to a feature of your choice
1.2 Remove the Label column ("selector") from the data set.
1.3 Visualize the data before and after the feature transformation using boxplots. 
```

You can download the dataset through the link:

-   [bupa.csv](bupa.csv)

```{python}
#df_bupa = pd.read_csv('bupa.csv')
#df_bupa = df_bupa.dropna()


#print(df_bupa)


```

