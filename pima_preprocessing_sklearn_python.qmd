
---
title: "Pima – Imputation & Scaling with scikit-learn (Python)"
format:
  html:
    toc: true
    code-fold: true
execute:
  echo: true
  warning: false
  message: false
---

This document shows how to perform **mean imputation** and **standardisation** correctly in Python using **scikit-learn**, first **manually** and then using a **Pipeline**.

We load the same Pima Indians database and select the following columns:

- `age`, `glucose`, `insulin` → predictors  
- `pedigree` → outcome (continuous)


```{r, include=FALSE}

library(reticulate)

# one-off setup (if you haven't done it yet)
# install_miniconda()

##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)

use_condaenv("hds-python", required = TRUE)
#py_config()

#conda_install("hds-python", c("jupyter", "plotly"))

```


```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
```

## 1. Example data (placeholder)

Replace this with your real data load. Here we just create a tiny artificial example so the code runs.

```{python}


url="https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv"
df = pd.read_csv(url)

df.columns = df.columns.str.lower()
df = df.rename(columns={"diabetespedigreefunction": "pedigree"})


features = ["age", "glucose", "insulin"]
df[features] = df[features].replace(0, np.nan)
X = df[features]
y = df["pedigree"]

```

---

## 2. Manual preprocessing: impute + scale (no Pipeline)

Here we explicitly:

1. Split into train and test.
2. Fit `SimpleImputer` on the **training set** and transform train & test.
3. Fit `StandardScaler` on the **training set** and transform train & test.
4. Fit a `LinearRegression` model and compute MAE on the test set.

```{python}


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=123
)

# 1. Mean imputation (TRAIN only)
imputer = SimpleImputer(strategy="mean")
X_train_imp = imputer.fit_transform(X_train)   # learns means from TRAIN
X_test_imp  = imputer.transform(X_test)        # applies TRAIN means

# 2. Standardisation (TRAIN only)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imp)  # learns μ, σ from TRAIN
X_test_scaled  = scaler.transform(X_test_imp)       # uses TRAIN μ, σ
```

```{python}
# 3. Linear regression on preprocessed data
model = LinearRegression()
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
mae_manual = mean_absolute_error(y_test, y_pred)
mae_manual
```

---

## 3. Using a scikit-learn Pipeline (cleanest version)

Pipelines package **preprocessing + model** into a single object:

- Imputation and scaling are still fit on the **training data only**.
- The same transformations are automatically applied to any new data.

```{python}
numeric_features = ["age", "glucose", "insulin"]

numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="mean")),
        ("scaler", StandardScaler()),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features)
    ]
)

model = LinearRegression()

pipe = Pipeline(
    steps=[
        ("preprocess", preprocess),
        ("model", model),
    ]
)

X = df[numeric_features]
y = df["pedigree"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=123
)

pipe.fit(X_train, y_train)
y_pred_pipe = pipe.predict(X_test)

mae_pipe = mean_absolute_error(y_test, y_pred_pipe)
mae_pipe
```


```{python}
print("MAE (manual preprocessing):", mae_manual)
print("MAE (Pipeline):           ", mae_pipe)
```

In practice, you should prefer the **Pipeline** approach:

- lower risk of mistakes,
- easy extension to cross-validation and more complex models.

