[
  {
    "objectID": "AssessRelationships.html",
    "href": "AssessRelationships.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "AssessRelationships.html#visualize-higher-dimensional-associations",
    "href": "AssessRelationships.html#visualize-higher-dimensional-associations",
    "title": "",
    "section": "Visualize higher dimensional associations",
    "text": "Visualize higher dimensional associations\n\n# Load necessary library\nlibrary(ggplot2)\n\n—– 1. Two Categorical Variables —–\nExample data for two categorical variables\n\ndata_cat &lt;- data.frame(\n  Gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\"),\n  Preference = c(\"A\", \"B\", \"A\", \"A\", \"B\", \"B\")\n)\n\n\n# Bar plot\np1 &lt;- ggplot(data_cat, aes(x = Gender, fill = Preference)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Preference by Gender\", x = \"Gender\", y = \"Count\") +\n  theme_minimal()\n\np1\n\n\n\n\n—– 2. Two Numerical Variables —–\nExample data for two numerical variables\n\ndata_num &lt;- data.frame(\n  Age = c(21, 25, 30, 35, 40, 45),\n  Salary = c(3000, 3200, 4000, 5000, 6000, 6500)\n)\n\n\n# Scatter plot\np2 &lt;- ggplot(data_num, aes(x = Age, y = Salary)) +\n  geom_point(color = \"blue\", size = 3) +\n  labs(title = \"Scatter Plot: Age vs Salary\", x = \"Age\", y = \"Salary\") +\n  theme_minimal()\np2\n\n\n\n\n—– 3. Categorical and Numerical Variable —–\nExample data for a categorical and a numerical variable\n\ndata_cat_num &lt;- data.frame(\n  Group = c(\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"),\n  Score = c(80, 85, 78, 82, 90, 95)\n)\n\n# Box plot\np3 &lt;- ggplot(data_cat_num, aes(x = Group, y = Score, fill = Group)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot: Score by Group\", x = \"Group\", y = \"Score\") +\n  theme_minimal()\n\np3\n\n\n\n\nIn the histogram and boxplot exercise, you added an extra layer (graph) to a similar plot that allowed us to overlay the exact data points. Inlude it in this example\n\n#p3_new &lt;- ggplot(data_cat_num, aes(x = Group, y = Score, fill = Group)) +\n#  geom_boxplot() +\n#  # ------------- #\n#  labs(title = \"Box Plot: Score by Group\", x = \"Group\", y = \"Score\") +\n#  theme_minimal()\n\n#p3_new\n\n—– 4. Three Variables (Categorical + Numerical + Numerical) —–\nExample data for three variables (by overlying an exta aesthetic - colour - we can now acknowledge three different variables)\n\ndata_three &lt;- data.frame(\n  Age = c(21, 25, 30, 35, 40, 45),\n  Salary = c(3000, 3200, 4000, 5000, 6000, 6500),\n  Gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\")\n)\n\np4 &lt;- ggplot(data_three, aes(x = Age, y = Salary, color = Gender)) +\n  geom_point(size = 3) +\n  labs(title = \"Scatter Plot: Age vs Salary by Gender\", x = \"Age\", y = \"Salary\") +\n  theme_minimal()\n\np4\n\n\n\n\n—– 5. Three Variables (Two Numerical + One Categorical) —–\nExample data for faceted scatter plot\n\ndata_facet &lt;- data.frame(\n  Age = c(21, 25, 30, 35, 40, 45, 21, 25, 30, 35, 40, 45),\n  Salary = c(3000, 3200, 4000, 5000, 6000, 6500, 3100, 3300, 4100, 5100, 6100, 6600),\n  Gender = rep(c(\"Male\", \"Female\"), each = 6)\n)\n\n# Faceted scatter plot\np5 &lt;- ggplot(data_facet, aes(x = Age, y = Salary)) +\n  geom_point(size = 3, color = \"blue\") +\n  facet_wrap(~ Gender) +\n  labs(title = \"Faceted Scatter Plot: Age vs Salary by Gender\", x = \"Age\", y = \"Salary\") +\n  theme_minimal()\n\np5"
  },
  {
    "objectID": "Correlation.html",
    "href": "Correlation.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "EDA and Pre-Processing",
      "Correlation - R"
    ]
  },
  {
    "objectID": "Correlation.html#correlation---r",
    "href": "Correlation.html#correlation---r",
    "title": "",
    "section": "Correlation - R",
    "text": "Correlation - R\nAs you have learnt throughout these modules, Python and R offer packages and libraries that contain already predefined functions ready to implement into your code. For you to see the breadth of possibilities out there, here is an example of different ways in whcih to describe and learn about the correlation matrix of your dataset.\n\n# Install required libraries (if not already installed)\n#install.packages(c(\"ggplot2\", \"corrplot\", \"ggcorrplot\", \"PerformanceAnalytics\", \"GGally\", \"psych\", \"corrr\"))\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(PerformanceAnalytics)\nlibrary(GGally)\nlibrary(psych)\nlibrary(corrr)\n\nTo assess the association between two variables, you can produce a scatter plot like below:\n\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- x + rnorm(100, sd = 0.5)\n\nggplot(data = data.frame(x, y), aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Correlation Plot with ggplot2\",\n       x = \"X values\", y = \"Y values\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow we are going to produce the correlation matrix, where in a go, you get insight of all the association happening in your dataset.For this we are going to use the mtcars data (A built-in dataset that contains measurements on 11 different attributes for 32 different cars). More specifically, the chosen variables are:\n\nmpg (Miles per Gallon): Represents the fuel efficiency of the car. Higher values, better efficiency.\ndisp (Displacement): Represents the engine displacement, which is the total volume of all the cylinders in the engine.\nhp (Horsepower): Represents the power output of the car’s engine.\nwt (Weight): Represents the weight of the car.\n\n\ndata &lt;- mtcars[, c(\"mpg\", \"disp\", \"hp\", \"wt\")]\n\nBut first, produce plots of all the two variable associations combinations in the features studied c(\"mpg\", \"disp\", \"hp\", \"wt\"). How many plots do you need?\nCan you calculate the correlation between them? Remember there are different ways in which to calculate correlation https://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r and you can specify the type in your code cor(x, y, method = c(\"pearson\", \"kendall\", \"spearman\"))\nNow that you know what to expect, lets proceed with the correlation matrices (same thing as above, but all in a go!):\n—–corrplot Correlation Matrix —–\n\ncor_matrix &lt;- cor(data)\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         title = \"Correlation Matrix\", addCoef.col = \"black\")\n\n\n\n\n\n\n\n—–corrplot Correlation Matrix —–\n\nggcorrplot(cor_matrix, \n           method = \"square\", \n           type = \"lower\", \n           lab = TRUE, \n           title = \"mtcars\", \n           lab_size = 3) +\n  theme_minimal()\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggcorrplot package.\n  Please report the issue at &lt;https://github.com/kassambara/ggcorrplot/issues&gt;.\n\n\n\n\n\n\n\n\n—–PerformanceAnalytics Pairwise Correlation Plot —–\n\nchart.Correlation(data, histogram = TRUE, pch = 19)\n\n\n\n\n\n\n\n###—–GGally Pairwise Correlation Plot —–\n\nggpairs(data, \n        title = \"Pairwise Correlation Plot with GGally\",\n        lower = list(continuous = wrap(\"smooth\", method = \"lm\")),\n        upper = list(continuous = wrap(\"cor\", size = 3)))\n\n\n\n\n\n\n\n###—– psych Enhanced Pairwise Correlation Plot —–\n\npairs.panels(data, \n             method = \"pearson\", \n             hist.col = \"lightblue\", \n             density = TRUE, \n             ellipses = TRUE)\n\n\n\n\n\n\n\n###—– Base R Heatmap —–\n\n# ----- 8. Base R Heatmap -----\nheatmap(cor_matrix, symm = TRUE, \n        main = \"Correlation Heatmap\", \n        col = colorRampPalette(c(\"red\", \"white\", \"blue\"))(20))\n\n\n\n\n\n\n\n\n# ----- 10. corrr Tidy Correlation -----\ncor_matrix_tidy &lt;- correlate(data)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\ncor_matrix_tidy %&gt;% \n  fashion() %&gt;% \n  print()\n\n  term  mpg disp   hp   wt\n1  mpg      -.85 -.78 -.87\n2 disp -.85       .79  .89\n3   hp -.78  .79       .66\n4   wt -.87  .89  .66     \n\n# Network-style correlation plot\ncor_matrix_tidy %&gt;% \n  network_plot(min_cor = 0.3)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the corrr package.\n  Please report the issue at &lt;https://github.com/tidymodels/corrr/issues&gt;.\n\n\n\n\n\n\n\n\nWhat are your insights? Do they make sense? Remember:\n\nmpg (Miles per Gallon): Represents the fuel efficiency of the car. Higher values, better efficiency.\ndisp (Displacement): Represents the engine displacement, which is the total volume of all the cylinders in the engine.\nhp (Horsepower): Represents the power output of the car’s engine.\nwt (Weight): Represents the weight of the car.",
    "crumbs": [
      "EDA and Pre-Processing",
      "Correlation - R"
    ]
  },
  {
    "objectID": "EDA_Practical1_Final.html",
    "href": "EDA_Practical1_Final.html",
    "title": "Missing Data Imputation - Python",
    "section": "",
    "text": "library(reticulate)\n\n# one-off setup (if you haven't done it yet)\n# install_miniconda()\n\n##conda_create(\n##  envname = \"hds-python\",\n##  python_version = \"3.11\",\n##  packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\")\n##)\n\nuse_condaenv(\"hds-python\", required = TRUE)\n#py_config()\n\n#conda_install(\"hds-python\", c(\"jupyter\", \"plotly\"))",
    "crumbs": [
      "EDA and Pre-Processing",
      "Missing Data Imputation - Python"
    ]
  },
  {
    "objectID": "EDA_Practical1_Final.html#dummy-variables",
    "href": "EDA_Practical1_Final.html#dummy-variables",
    "title": "Missing Data Imputation - Python",
    "section": "Dummy variables",
    "text": "Dummy variables\nAs well as imputation we are interested in other transformations. For example, hot encoding categorical data. For extra insight https://www.kaggle.com/code/marcinrutecki/one-hot-encoding-everything-you-need-to-know\n\nX_reduced_Encoder = heart_df.loc[:, ['chol', 'thalach','sex','cp']]\nX_reduced_Encoder[categorical_features] = X_reduced_Encoder[categorical_features].astype('category')\n\n# Apply OneHotEncoder\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nencoded_categorical = encoder.fit_transform(X_reduced_Encoder[categorical_features])\n\n\nencoded_df = pd.DataFrame(\n    encoded_categorical,\n    columns=encoder.get_feature_names_out(categorical_features)\n)\n\n\n# Combine the encoded features with the original dataset (drop original categorical columns)\nvisualised_df = pd.concat([X_reduced_Encoder.drop(columns=categorical_features), encoded_df], axis=1)\nprint(visualised_df)\n\n     chol  thalach  sex_0.0  sex_1.0  ...  cp_2.0  cp_3.0  cp_4.0  cp_nan\n0    60.0     12.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n1     NaN      8.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n2    27.0     19.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n3    39.0     25.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n4    22.0     53.0      0.0      1.0  ...     0.0     1.0     0.0     0.0\n..    ...      ...      ...      ...  ...     ...     ...     ...     ...\n195  95.0      NaN      1.0      0.0  ...     0.0     0.0     1.0     0.0\n196  30.0      1.0      0.0      1.0  ...     0.0     0.0     0.0     0.0\n197  33.0      4.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n198   3.0      1.0      0.0      1.0  ...     0.0     0.0     0.0     1.0\n199   NaN     47.0      0.0      1.0  ...     1.0     0.0     0.0     0.0\n\n[200 rows x 10 columns]\n\n\nAs you have seen we have gone back to original data so one of the new columns created is nan! To circumvent this apply this to a dataframe AFTER imputation!!!\n\n\n#Impute dataset and then do the hot encoding: \n\nCheck out the concept of Pipeline https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html when do you think it can come in handy?",
    "crumbs": [
      "EDA and Pre-Processing",
      "Missing Data Imputation - Python"
    ]
  },
  {
    "objectID": "EDA_Practical2.html",
    "href": "EDA_Practical2.html",
    "title": "Outlier Detection - Python",
    "section": "",
    "text": "We are using both Pandas (data loading, processing, transformation and manipulation) and Scikit-learn (example data source, ML and statistical analysis)"
  },
  {
    "objectID": "EDA_Practical2.html#outlier-detection",
    "href": "EDA_Practical2.html#outlier-detection",
    "title": "Outlier Detection - Python",
    "section": "Outlier Detection",
    "text": "Outlier Detection\nThis example illustrates - How to identify outliers. - How to deal with outliers.\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom scipy import stats\n\n\n# Load the dataset\n# For this practical we use the breast cancer data set.  \ndata = datasets.load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Let's look at the first few rows of the dataframe\nprint(df)\n\n     mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0          17.99         10.38  ...          0.4601                  0.11890\n1          20.57         17.77  ...          0.2750                  0.08902\n2          19.69         21.25  ...          0.3613                  0.08758\n3          11.42         20.38  ...          0.6638                  0.17300\n4          20.29         14.34  ...          0.2364                  0.07678\n..           ...           ...  ...             ...                      ...\n564        21.56         22.39  ...          0.2060                  0.07115\n565        20.13         28.25  ...          0.2572                  0.06637\n566        16.60         28.08  ...          0.2218                  0.07820\n567        20.60         29.33  ...          0.4087                  0.12400\n568         7.76         24.54  ...          0.2871                  0.07039\n\n[569 rows x 30 columns]"
  },
  {
    "objectID": "EDA_Practical2.html#data-description",
    "href": "EDA_Practical2.html#data-description",
    "title": "Outlier Detection - Python",
    "section": "Data Description",
    "text": "Data Description\nThis is an analysis of the Breast Cancer Wisconsin (Diagnostic) DataSet, obtained from Kaggle. This data set was created by Dr. William H. Wolberg, physician at the University Of Wisconsin Hospital at Madison, Wisconsin,USA. To create the dataset Dr. Wolberg used fluid samples, taken from patients with solid breast masses and an easy-to-use graphical computer program called Xcyt, which is capable of perform the analysis of cytological features based on a digital scan. The program uses a curve-fitting algorithm, to compute ten features from each one of the cells in the sample, than it calculates the mean value, extreme value and standard error of each feature for the image, returning a 30 real-valuated vector\nAttribute Information:\n\nID number\nDiagnosis (M = malignant, B = benign) 3-32) Ten real-valued features are computed for each cell nucleus:\n\n\nradius (mean of distances from center to points on the perimeter)\ntexture (standard deviation of gray-scale values)\nperimeter\narea\nsmoothness (local variation in radius lengths)\ncompactness (perimeter^2 / area - 1.0)\nconcavity (severity of concave portions of the contour)\nconcave points (number of concave portions of the contour)\nsymmetry\nfractal dimension (“coastline approximation” - 1)\nThe mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nStrategy 1 : Outlier Removal\nWe will visualizes the original data, including outliers, using a boxplot. Each box in the boxplot represents one column (or feature) in the data, with outliers shown as points that are distant from the main body of the box.\nThe code then creates a new DataFrame, df_o, which is the same as the original DataFrame df but with all rows containing any outliers removed. The Z-scores for the new DataFrame are then calculated, to confirm that all outliers have been removed.\nThe Final code block visualizes the new, outlier-free data using another boxplot. This can be compared with the original boxplot to see the effect of removing the outliers.\n\n#### Strategy 1 : Removing Duplicates\n\n\n# Outlier detection - We will use Z-score function defined in scipy library to detect the outliers\n# We will calculate the Z-score for each value in the DataFrame, and identifies any values with a Z-score greater than 3 as outliers.\nz = np.abs(stats.zscore(df))\nprint('\\nZ-score array:\\n', z)\n#print(df)\n\n# Define a threshold to identify an outlier\n\n\nZ-score array:\n      mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0       1.097064      2.073335  ...        2.750622                 1.937015\n1       1.829821      0.353632  ...        0.243890                 0.281190\n2       1.579888      0.456187  ...        1.152255                 0.201391\n3       0.768909      0.253732  ...        6.046041                 4.935010\n4       1.750297      1.151816  ...        0.868353                 0.397100\n..           ...           ...  ...             ...                      ...\n564     2.110995      0.721473  ...        1.360158                 0.709091\n565     1.704854      2.085134  ...        0.531855                 0.973978\n566     0.702284      2.045574  ...        1.104549                 0.318409\n567     1.838341      2.336457  ...        1.919083                 2.219635\n568     1.808401      1.221792  ...        0.048138                 0.751207\n\n[569 rows x 30 columns]\n\nthreshold = 3\n#print('\\nOutliers:\\n', np.where(z &gt; threshold))\n\n# Visualizing the outliers using boxplot\nplt.figure(figsize=(20,10))\n\n# df_m = df.melt( )\n# g = sns.FacetGrid(df_m, col=\"variable\", height=4, aspect=.5, sharey=False )\n# g.map(sns.boxplot, \"variable\", \"value\")\n\nsns.boxplot(data=df)\nplt.title('Outlier visualization', fontsize=20)\nplt.xticks(rotation=90)\n#plt.show()\n\n## Removing the outliers\n\n([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [Text(0, 0, 'mean radius'), Text(1, 0, 'mean texture'), Text(2, 0, 'mean perimeter'), Text(3, 0, 'mean area'), Text(4, 0, 'mean smoothness'), Text(5, 0, 'mean compactness'), Text(6, 0, 'mean concavity'), Text(7, 0, 'mean concave points'), Text(8, 0, 'mean symmetry'), Text(9, 0, 'mean fractal dimension'), Text(10, 0, 'radius error'), Text(11, 0, 'texture error'), Text(12, 0, 'perimeter error'), Text(13, 0, 'area error'), Text(14, 0, 'smoothness error'), Text(15, 0, 'compactness error'), Text(16, 0, 'concavity error'), Text(17, 0, 'concave points error'), Text(18, 0, 'symmetry error'), Text(19, 0, 'fractal dimension error'), Text(20, 0, 'worst radius'), Text(21, 0, 'worst texture'), Text(22, 0, 'worst perimeter'), Text(23, 0, 'worst area'), Text(24, 0, 'worst smoothness'), Text(25, 0, 'worst compactness'), Text(26, 0, 'worst concavity'), Text(27, 0, 'worst concave points'), Text(28, 0, 'worst symmetry'), Text(29, 0, 'worst fractal dimension')])\n\ndf_o = df[(z &lt; threshold).all(axis=1)]\n\n# Checking if the outliers have been removed\nz = np.abs(stats.zscore(df_o))\nprint('\\nAfter removing outliers:\\n', np.where(z &gt; threshold))\n\n# Visualizing the data without outliers\n\n\nAfter removing outliers:\n (array([  6,  10,  10,  13,  17,  17,  18,  19,  19,  21,  25,  29,  29,\n        51,  51,  52,  63,  63,  97, 121, 122, 125, 134, 134, 134, 134,\n       135, 137, 137, 137, 141, 141, 141, 146, 155, 167, 176, 176, 178,\n       178, 182, 202, 202, 202, 205, 205, 207, 207, 210, 210, 210, 210,\n       212, 212, 216, 217, 217, 229, 229, 229, 229, 229, 229, 232, 255,\n       255, 255, 257, 257, 257, 281, 284, 314, 316, 316, 316, 316, 316,\n       316, 333, 335, 335, 363, 381, 388, 388, 402, 402, 405, 405, 406,\n       408, 415, 421, 421, 421, 421, 425, 440, 440, 453, 453, 453, 460,\n       460, 471, 471, 471, 491, 491, 491, 491, 491, 491, 492, 492, 492]), array([28, 25, 29, 23,  8, 28, 23, 25, 29, 25, 25, 10, 12, 15, 25, 18, 10,\n       13, 29, 19, 19,  8, 10, 12, 13, 17, 23,  0,  3, 23, 10, 12, 13, 14,\n       14, 28, 12, 17, 18, 28, 13, 16, 19, 29, 11, 14, 16, 26,  6, 10, 13,\n       16, 26, 29, 12,  5, 19,  3, 10, 12, 13, 22, 23, 14, 10, 12, 13, 10,\n       12, 13, 18, 18, 12,  7, 10, 12, 13, 22, 23, 14,  6,  7,  8, 11, 15,\n       19, 15, 29, 15, 19, 14, 11, 16, 15, 16, 17, 19, 28,  9, 19,  4, 14,\n       18, 11, 17, 14, 15, 16,  5,  6,  7, 12, 13, 17, 10, 12, 13]))\n\nplt.figure(figsize=(20,10))\nsns.boxplot(data=df_o)\nplt.title('After outlier removal', fontsize=20)\nplt.xticks(rotation=90)\n#plt.show()\n\n([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [Text(0, 0, 'mean radius'), Text(1, 0, 'mean texture'), Text(2, 0, 'mean perimeter'), Text(3, 0, 'mean area'), Text(4, 0, 'mean smoothness'), Text(5, 0, 'mean compactness'), Text(6, 0, 'mean concavity'), Text(7, 0, 'mean concave points'), Text(8, 0, 'mean symmetry'), Text(9, 0, 'mean fractal dimension'), Text(10, 0, 'radius error'), Text(11, 0, 'texture error'), Text(12, 0, 'perimeter error'), Text(13, 0, 'area error'), Text(14, 0, 'smoothness error'), Text(15, 0, 'compactness error'), Text(16, 0, 'concavity error'), Text(17, 0, 'concave points error'), Text(18, 0, 'symmetry error'), Text(19, 0, 'fractal dimension error'), Text(20, 0, 'worst radius'), Text(21, 0, 'worst texture'), Text(22, 0, 'worst perimeter'), Text(23, 0, 'worst area'), Text(24, 0, 'worst smoothness'), Text(25, 0, 'worst compactness'), Text(26, 0, 'worst concavity'), Text(27, 0, 'worst concave points'), Text(28, 0, 'worst symmetry'), Text(29, 0, 'worst fractal dimension')])\n\nprint(df.shape)\n\n(569, 30)\n\nprint(df_o.shape)\n\n(495, 30)\n\n\nmean compactness mean concavity mean concave points mean symmetry\n0 0.27760 0.30010 0.14710 0.2419\n1 0.07864 0.08690 0.07017 0.1812\n2 0.15990 0.19740 0.12790 0.2069\n3 0.28390 0.24140 0.10520 0.2597\n4 0.13280 0.19800 0.10430 0.1809\n.. … … … …\n564 0.11590 0.24390 0.13890 0.1726\n565 0.10340 0.14400 0.09791 0.1752\n566 0.10230 0.09251 0.05302 0.1590\n567 0.27700 0.35140 0.15200 0.2397\n568 0.04362 0.00000 0.00000 0.1587\nStrategy 2 : Winsorize Method\nIn the Winsorize Method, we limit outliers with an upper and lower limit. We will set the limits. We will make our upper and lower limits for data our new maximum and minimum points.\nThis data set consists of 345 observations and 6 predictors representing the blood test result liver disorder status of 345 patients. The three predictors are mean corpuscular volume (MCV), alkaline phosphotase (ALKPHOS), alamine aminotransferase (SGPT), aspartate aminotransferase (SGOT), gamma-glutamyl transpeptidase (GAMMAGT), and the number of alcoholic beverage drinks per day (DRINKS).\nFor this practical we use the BUPA Liver data set.\nYou can download the dataset through the link:\n\nbupa.csv\n\n\n#### Strategy 2 : Winsorize Method\n\n\ndf_bupa = pd.read_csv('bupa.csv')\ndf_bupa = df_bupa.dropna()\n\nsns.boxplot( x = df_bupa['drinks'] )\n#sns.distplot( x = df_bupa['drinks'], bins = 10, kde = False )\n\n\n## For outliers, \n##   our upper limit is 0 and \n##   our lower limit is 12.\n##\n## For the Winsorize Method, we have to import winsorize from Scipy. \n## We need boundaries to apply winsorize. \n## We will limit our data between 53 and 63. These values are within limits for outliers. \n## We need precise points of these values in the percentile and we can use the quantile method of Pandas.\n\n\nfrom scipy.stats.mstats import winsorize\n\nprint( \"Lower Limit : \" + str(df_bupa['drinks'].quantile(0.05)))\n\nLower Limit : 0.5\n\nprint( \"Upper Limit : \" + str(df_bupa['drinks'].quantile(0.95)))\n\nUpper Limit : 10.0\n\n\nLower Limit : 0.5\nUpper Limit : 10.0\nWe will create a new variable with Winsorize Method. To implement the Winsorize Method, we write the exact boundary points as a tuple on the percentile. For example, we will write (0.05, 0.05). This means we want to apply quantile(0.05) and quantile(0.95) as a boundary. The first one is the exact point on percentile from the beginning, the second one is exact point on percentile from the end.\n\ndf_bupa_drinks = winsorize(df_bupa['drinks'], (0.05, 0.05) )\nsns.boxplot( df_bupa_drinks )\n\n\n\n\nYour Task\nQuestion 1: Remove outliers in the breast cancer dataset by applying Z-score threshold = 1.\n1.1 How many samples are removed after the outlier removal ?\nQuestion 2: Apply Winsorize method threshold of lower limit = 0.10 and upper limit = 0.80 on BUPA Liver data set.\nUse the \"drinks\" feature.\n2.1 What is the shape of the dataset after applying Winsorize method ?"
  },
  {
    "objectID": "EDA_Practical2.html#question-1-remove-outliers-in-the-breast-cancer-dataset-by-applying-z-score-threshold-1.-1",
    "href": "EDA_Practical2.html#question-1-remove-outliers-in-the-breast-cancer-dataset-by-applying-z-score-threshold-1.-1",
    "title": "Outlier Detection - Python",
    "section": "Question 1: Remove outliers in the breast cancer dataset by applying Z-score threshold = 1.",
    "text": "Question 1: Remove outliers in the breast cancer dataset by applying Z-score threshold = 1.\n\n## Question 1: Remove outliers in the breast cancer dataset by applying Z-score threshold = 1.\n\n\nz = np.abs(stats.zscore(df))\n#print('\\nZ-score array:\\n', z)\n\nprint(df.shape)\n# Define a threshold to identify an outlier\n\n(569, 30)\n\nthreshold = 1\n#print('\\nOutliers:\\n', np.where(z &gt; threshold))\n\n# Visualizing the outliers using boxplot\nplt.figure(figsize=(20,10))\nsns.boxplot(data=df)\nplt.title('Outlier visualization', fontsize=20)\nplt.xticks(rotation=90)\n\n([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [Text(0, 0, 'mean radius'), Text(1, 0, 'mean texture'), Text(2, 0, 'mean perimeter'), Text(3, 0, 'mean area'), Text(4, 0, 'mean smoothness'), Text(5, 0, 'mean compactness'), Text(6, 0, 'mean concavity'), Text(7, 0, 'mean concave points'), Text(8, 0, 'mean symmetry'), Text(9, 0, 'mean fractal dimension'), Text(10, 0, 'radius error'), Text(11, 0, 'texture error'), Text(12, 0, 'perimeter error'), Text(13, 0, 'area error'), Text(14, 0, 'smoothness error'), Text(15, 0, 'compactness error'), Text(16, 0, 'concavity error'), Text(17, 0, 'concave points error'), Text(18, 0, 'symmetry error'), Text(19, 0, 'fractal dimension error'), Text(20, 0, 'worst radius'), Text(21, 0, 'worst texture'), Text(22, 0, 'worst perimeter'), Text(23, 0, 'worst area'), Text(24, 0, 'worst smoothness'), Text(25, 0, 'worst compactness'), Text(26, 0, 'worst concavity'), Text(27, 0, 'worst concave points'), Text(28, 0, 'worst symmetry'), Text(29, 0, 'worst fractal dimension')])\n\nplt.show()\n\n# Removing the outliers\n\n\n\ndf_o = df[(z &lt; threshold).all(axis=1)]\nprint(df_o.shape)\n# Checking if the outliers have been removed\n\n(42, 30)\n\nz = np.abs(stats.zscore(df_o))\n#print('\\nAfter removing outliers:\\n', np.where(z &gt; threshold))\n\n# Visualizing the data without outliers\nplt.figure(figsize=(20,10))\nsns.boxplot(data=df_o)\nplt.title('After outlier removal', fontsize=20)\nplt.xticks(rotation=90)\n\n([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [Text(0, 0, 'mean radius'), Text(1, 0, 'mean texture'), Text(2, 0, 'mean perimeter'), Text(3, 0, 'mean area'), Text(4, 0, 'mean smoothness'), Text(5, 0, 'mean compactness'), Text(6, 0, 'mean concavity'), Text(7, 0, 'mean concave points'), Text(8, 0, 'mean symmetry'), Text(9, 0, 'mean fractal dimension'), Text(10, 0, 'radius error'), Text(11, 0, 'texture error'), Text(12, 0, 'perimeter error'), Text(13, 0, 'area error'), Text(14, 0, 'smoothness error'), Text(15, 0, 'compactness error'), Text(16, 0, 'concavity error'), Text(17, 0, 'concave points error'), Text(18, 0, 'symmetry error'), Text(19, 0, 'fractal dimension error'), Text(20, 0, 'worst radius'), Text(21, 0, 'worst texture'), Text(22, 0, 'worst perimeter'), Text(23, 0, 'worst area'), Text(24, 0, 'worst smoothness'), Text(25, 0, 'worst compactness'), Text(26, 0, 'worst concavity'), Text(27, 0, 'worst concave points'), Text(28, 0, 'worst symmetry'), Text(29, 0, 'worst fractal dimension')])\n\nplt.show()\n\n\n\n\nQuestion 2: Apply Winsorize method threshold of lower limit = 0.10 and upper limit = 0.80 on BUPA Liver data set\n2.1 What is the shape of the dataset after applying Winsorize method ?\n\ndf_bupa_winsorize = df_bupa.copy()\ndf_bupa_winsorize['drinks'] = winsorize(df_bupa_winsorize['drinks'], (0.10, 0.10) )\n#df_bupa_winsorize['drinks']\n\n\n## Answer 2.1 \n##   winsorize clips the outlier data points to the upper and lower limits."
  },
  {
    "objectID": "EDA_Practical3.html",
    "href": "EDA_Practical3.html",
    "title": "Data Transformation - Python",
    "section": "",
    "text": "We are using both Pandas (data loading, processing, transformation and manipulation) and Scikit-learn (example data source, ML and statistical analysis)",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data Transformation - Python"
    ]
  },
  {
    "objectID": "EDA_Practical3.html#data-scaling-and-normalization",
    "href": "EDA_Practical3.html#data-scaling-and-normalization",
    "title": "Data Transformation - Python",
    "section": "Data Scaling and Normalization",
    "text": "Data Scaling and Normalization\n\nDifferent Types of Normalization.\n\nData Set Used :\nBreast Cancer Wisconsin (Diagnostic) Data Set\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”, Optimization Methods and Software 1, 1992, 23-34].\n\nlibrary(reticulate)\n\n# one-off setup (if you haven't done it yet)\n# install_miniconda()\n\n##conda_create(\n##  envname = \"hds-python\",\n##  python_version = \"3.11\",\n##  packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\")\n##)\n\nuse_condaenv(\"hds-python\", required = TRUE)\n#py_config()\n\n#conda_install(\"hds-python\", c(\"jupyter\", \"plotly\"))\n\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\n\n# Load the dataset\ndata = datasets.load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\n\n\n# Let's look at the first few rows of the dataframe\nprint(df.head())\n\n   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0        17.99         10.38  ...          0.4601                  0.11890\n1        20.57         17.77  ...          0.2750                  0.08902\n2        19.69         21.25  ...          0.3613                  0.08758\n3        11.42         20.38  ...          0.6638                  0.17300\n4        20.29         14.34  ...          0.2364                  0.07678\n\n[5 rows x 30 columns]\n\n\n\n# Let's plot the data before transformation\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df)\nplt.title(\"Non-normalised data\")\nplt.show()\n\n\n\n\n\n\n\nStrategy 1 : Min-Max scaling\nMin-Max scaling: This transformation scales and translates each feature individually such that it is in the range [0, 1]. The transformed data is stored in df_minmax.\n\n#### Strategy 1 : Min-Max scaling\nscaler = preprocessing.MinMaxScaler(feature_range=(0,100))\ndf_minmax = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)\n#print(\"\\nAfter Min-Max Scaling:\\n\", df_minmax.head())\n\n\n# Let's plot the data after min-max scaling\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_minmax)\nplt.title(\"After Min-Max Scaling\")\nplt.show()\n\n\n\n\n\n\n\nLog transformation:\nThis transformation applies the natural logarithm to each value in the DataFrame. This is often used when the data is highly skewed, as it can help to make the data more “normal” (i.e., more closely approximate a normal distribution). The transformed data is stored in df_log.\n\n# Log Transformation\ndf_log = df.apply(np.log)\ndf_log = df_log.replace([np.inf, -np.inf], np.nan)\ndf_log = df_log.dropna()\nprint(\"\\nAfter Log Transformation:\\n\", df_log.head())\n\n\nAfter Log Transformation:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0     2.889816      2.339881  ...       -0.776311                -2.129472\n1     3.023834      2.877512  ...       -1.290984                -2.418894\n2     2.980111      3.056357  ...       -1.018047                -2.435203\n3     2.435366      3.014554  ...       -0.409774                -1.754464\n4     3.010128      2.663053  ...       -1.442230                -2.566811\n\n[5 rows x 30 columns]\n\n# Let's plot the data after min-max scaling\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_log)\nplt.title(\"After Log Transformation\")\nplt.show()\n\n\n\n\n\n\n\nZ-score normalization:\nAlso known as standardization, this transformation scales and translates each feature so that it has a mean of 0 and a standard deviation of 1. The transformed data is stored in df_zscore.\nZ-score normalization is a type of data standardization where we convert all features in our dataset to have a mean (µ) of 0 and a standard deviation (σ) of 1. The purpose of this transformation is to remove the scale effect of measurements.\nThe formula for Z-score normalization is:\nZ = (X - µ) / σ\nwhere:\nZ is the standardized (Z-score normalized) value, X is the original value, µ is the mean of the feature, σ is the standard deviation of the feature. Why do we do this? In machine learning, many algorithms (like K-nearest neighbors, Neural Networks, and others) perform better when their input features are roughly on the same scale and centered around zero. If one feature has a range of -1 to 1, while another feature has a range of -1000 to 1000, the second feature will completely dominate when these features are combined, even though the first feature might be just as important.\nAfter Z-score normalization, every feature in the dataset will have a mean of 0 and a standard deviation of 1, putting them all on roughly the same scale. The data values in each column now represent how many standard deviations the original value was from the mean of that column. This makes it easier to compare different features, and helps many machine learning algorithms perform better.\n\n# Z-score normalization\nscaler = preprocessing.StandardScaler()\ndf_zscore = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)\nprint(\"\\nAfter Z-score Normalization:\\n\", df_zscore.head())\n\n\nAfter Z-score Normalization:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0     1.097064     -2.073335  ...        2.750622                 1.937015\n1     1.829821     -0.353632  ...       -0.243890                 0.281190\n2     1.579888      0.456187  ...        1.152255                 0.201391\n3    -0.768909      0.253732  ...        6.046041                 4.935010\n4     1.750297     -1.151816  ...       -0.868353                -0.397100\n\n[5 rows x 30 columns]\n\n# Let's plot the data after Z-score normalization\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_zscore)\nplt.title(\"After Z-score Normalization\")\nplt.show()\n\n\n\n\n\n\n\nPower transformation (Cube):\nThis transformation raises each value in the DataFrame to the power of 3. It can be used to increase the skewness in the data.\n\n# Power transformation (cube)\ndf_power = df.apply(lambda x: x**3)\nprint(\"\\nAfter Power Transformation:\\n\", df_power.head())\n\n\nAfter Power Transformation:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0  5822.285399   1118.386872  ...        0.097399                 0.001681\n1  8703.679193   5611.284433  ...        0.020797                 0.000705\n2  7633.736209   9595.703125  ...        0.047163                 0.000672\n3  1489.355288   8464.718872  ...        0.292490                 0.005178\n4  8353.070389   2948.814504  ...        0.013211                 0.000453\n\n[5 rows x 30 columns]\n\n# Let's plot the data after power transformation\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_power)\nplt.title(\"After Power Transformation\")\nplt.show()\n\n\n\n\n\n\n\nQuantile transformation:\nQuantile transformation, also known as quantile normalization, is a technique for making two distributions identical in statistical properties. To achieve this, it maps the values from the input distribution to a desired output distribution, such as a uniform or a normal distribution.\nThe process can be summarized as follows:\nFor each feature, sort the values in ascending order.\nThe smallest value is replaced with the smallest value from the desired distribution, the second smallest with the second smallest, and so on.\nThis way, the transformed data will have the same distribution as the desired output distribution, while preserving the rank of the original data.\nOne of the main uses of quantile transformation is to reduce the impact of outliers. Because it’s based on the rank of the data, not the actual values, extreme values will be closer to the rest of the data after transformation.\nAnother use is to make non-linear data more suitable for linear models. If a feature has a distribution that’s skewed or has a heavy tail, quantile transformation can help make it more “normal”, so that linear models can make better predictions.\n\n# Quantile transformation\nscaler = preprocessing.QuantileTransformer(output_distribution='normal')\ndf_quantile = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)\n\n/Users/l.bravo@bham.ac.uk/Library/r-miniconda-arm64/envs/hds-python/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2846: UserWarning: n_quantiles (1000) is greater than the total number of samples (569). n_quantiles is set to n_samples.\n  warnings.warn(\n\nprint(\"\\nAfter Quantile Transformation:\\n\", df_quantile.head())\n\n\nAfter Quantile Transformation:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0     0.982803     -2.918152  ...        1.966025                 1.618134\n1     1.634697     -0.277117  ...       -0.137235                 0.526427\n2     1.352998      0.513796  ...        1.304975                 0.451806\n3    -0.820429      0.356013  ...        5.199338                 2.918152\n4     1.512992     -1.221674  ...       -1.004493                -0.263385\n\n[5 rows x 30 columns]\n\n# Let's plot the data after quantile transformation\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_quantile)\nplt.title(\"After Quantile Transformation\")\nplt.show()\n\n\n\n\n\n\n\nYour Task",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data Transformation - Python"
    ]
  },
  {
    "objectID": "EDA_Practical3.html#question-1",
    "href": "EDA_Practical3.html#question-1",
    "title": "Data Transformation - Python",
    "section": "Question 1:",
    "text": "Question 1:\n1.1 Load the Bupa Data set and apply Quantile Normalization on each feature.\n1.2 Remove the Label column (\"selector\") from the data set.\n1.3 Visualize the data before and after normalization using boxplot. \nYou can download the dataset through the link:\n\nbupa.csv\n\n\n#df_bupa = pd.read_csv('bupa.csv')\n#df_bupa = df_bupa.dropna()\n\n\n#print(df_bupa)\n\n#print(\"\\Before Quantile Transformation:\\n\")\n# Let's plot the data after quantile transformation\n#plt.figure(figsize=(10, 6))\n#sns.boxplot(data=df_bupa)\n#plt.title(\"After Quantile Transformation\")\n#plt.show()\n\n## ------------------- Complete This ------------------------- ##\n## scaler = preprocessing. ... \n## df_bupa_quantile = pd.DataFrame(scaler.fit_transform( ... ), columns =  ... )\n\n#print(\"\\nAfter Quantile Transformation:\\n\")\n## Let's plot the data after quantile transformation\n#plt.figure(figsize=(10, 6))\n#sns.boxplot(data=df_bupa_quantile)\n#plt.title(\"After Quantile Transformation\")\n#plt.show()",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data Transformation - Python"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html",
    "href": "HealthyR_Summary.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#data",
    "href": "HealthyR_Summary.html#data",
    "title": "",
    "section": "Data",
    "text": "Data\nWe are using a condensed version of the Global Burden of Disease Data dataset.\n\n#Can load the dataset through many ways but quickest and one i always resort to, is data.table function\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\ngbd_full &lt;- fread(\"global_burden_disease_cause-year-sex-income.csv\") #add your working directory\n\n\nhead(gbd_full)\n\n                   cause year    sex       income deaths_millions\n1: Communicable diseases 1990 Female         High            0.21\n2: Communicable diseases 1990 Female Upper-Middle            1.15\n3: Communicable diseases 1990 Female Lower-Middle            4.43\n4: Communicable diseases 1990 Female          Low            1.51\n5: Communicable diseases 1990   Male         High            0.26\n6: Communicable diseases 1990   Male Upper-Middle            1.35\n\n\n\n# Creating a single-year tibble for printing and simple examples:\ngbd2017 &lt;- gbd_full %&gt;% \n  filter(year == \"2017\")",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#plot-the-data",
    "href": "HealthyR_Summary.html#plot-the-data",
    "title": "",
    "section": "Plot the data",
    "text": "Plot the data\nThe best way to investigate a dataset is to plot it (do not worry if do not understand the code, will do a separate exercise on plotting later)\n\ngbd2017 %&gt;% \n  # without the mutate(... = fct_relevel()) \n  # the panels get ordered alphabetically\n  mutate(income = fct_relevel(income,\n                              \"Low\",\n                              \"Lower-Middle\",\n                              \"Upper-Middle\",\n                              \"High\")) %&gt;% \n  # defining the variables using ggplot(aes(...)):\n  ggplot(aes(x = sex, y = deaths_millions, fill = cause)) +\n  # type of geom to be used: column (that's a type of barplot):\n  geom_col(position = \"dodge\") +\n  # facets for the income groups:\n  facet_wrap(~income, ncol = 4) +\n  # move the legend to the top of the plot (default is \"right\"):\n  theme(legend.position = \"top\")",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#aggregating-group_by-summarise",
    "href": "HealthyR_Summary.html#aggregating-group_by-summarise",
    "title": "",
    "section": "Aggregating: group_by(), summarise()\n",
    "text": "Aggregating: group_by(), summarise()\n\nHealth data analysis is frequently concerned with making comparisons between groups. Groups of genes, or diseases, or patients, or populations, etc. An easy approach to the comparison of data by a categorical grouping is therefore essential.\nWe will introduce flexible functions from tidyverse that you can apply in any setting.\nTo quickly calculate the total number of deaths in 2017, we can select the column and send it into the sum() function:\n\ngbd2017$deaths_millions %&gt;% sum()\n\n[1] 55.74\n\n\nBut a much cleverer way of summarising data is using the summarise() function:\n\ngbd2017 %&gt;% \n  summarise(sum(deaths_millions))\n\n  sum(deaths_millions)\n1                55.74\n\n\nsum() is a function that adds numbers together, whereas summarise() is an efficient way of creating summarised dataframes. The main strength of summarise() is how it works with the group_by() function. group_by() and summarise() are a perfect complement for each other and used togetehr most of the time.\nWe use group_by() to tell summarise() which subgroups to apply the calculations on. In the above example, without group_by(), summarise just works on the whole dataset, yielding the same result as just sending a single column into the sum() function.\nWe can subset on the cause variable using group_by():\n\ngbd2017 %&gt;% \n  group_by(cause) %&gt;% \n  summarise(sum(deaths_millions))\n\n# A tibble: 3 × 2\n  cause                     `sum(deaths_millions)`\n  &lt;chr&gt;                                      &lt;dbl&gt;\n1 Communicable diseases                      10.4 \n2 Injuries                                    4.47\n3 Non-communicable diseases                  40.9 \n\n\nFurthermore, group_by() is happy to accept multiple grouping variables. So by just copying and editing the above code, we can quickly get summarised totals across multiple grouping variables (by just adding sex inside the group_by() after cause):\n\ngbd2017 %&gt;% \n  group_by(cause, sex) %&gt;% \n  summarise(sum(deaths_millions))\n\n`summarise()` has grouped output by 'cause'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   cause [3]\n  cause                     sex    `sum(deaths_millions)`\n  &lt;chr&gt;                     &lt;chr&gt;                   &lt;dbl&gt;\n1 Communicable diseases     Female                   4.91\n2 Communicable diseases     Male                     5.47\n3 Injuries                  Female                   1.42\n4 Injuries                  Male                     3.05\n5 Non-communicable diseases Female                  19.2 \n6 Non-communicable diseases Male                    21.7",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#add-new-columns-mutate",
    "href": "HealthyR_Summary.html#add-new-columns-mutate",
    "title": "",
    "section": "Add new columns: mutate()\n",
    "text": "Add new columns: mutate()\n\nLet’s first give the summarised column a better name, e.g., deaths_per_group. We can remove groupings by using ungroup(). This is important to remember if you want to manipulate the dataset in its original format. We can combine ungroup() with mutate() to add a total deaths column, which will be used below to calculate a percentage:\n\ngbd2017 %&gt;% \n  group_by(cause, sex) %&gt;% \n  summarise(deaths_per_group = sum(deaths_millions)) %&gt;% \n  ungroup() %&gt;% \n  mutate(deaths_total = sum(deaths_per_group))\n\n`summarise()` has grouped output by 'cause'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 4\n  cause                     sex    deaths_per_group deaths_total\n  &lt;chr&gt;                     &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;\n1 Communicable diseases     Female             4.91         55.7\n2 Communicable diseases     Male               5.47         55.7\n3 Injuries                  Female             1.42         55.7\n4 Injuries                  Male               3.05         55.7\n5 Non-communicable diseases Female            19.2          55.7\n6 Non-communicable diseases Male              21.7          55.7\n\n\nSo summarise() condenses a dataframe, whereas mutate() retains its current size and adds columns.\nPercentages formatting: percent()\n\nWe can also add further lines to mutate() to calculate the percentage of each group:\n\n# percent() function for formatting percentages come from library(scales)\nlibrary(scales)\ngbd2017_summarised &lt;- gbd2017 %&gt;% \n  group_by(cause, sex) %&gt;% \n  summarise(deaths_per_group = sum(deaths_millions)) %&gt;% \n  ungroup() %&gt;% \n  mutate(deaths_total    = sum(deaths_per_group),\n        deaths_relative = deaths_per_group/deaths_total)\ngbd2017_summarised\n\n# A tibble: 6 × 5\n  cause                     sex    deaths_per_group deaths_total deaths_relative\n  &lt;chr&gt;                     &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1 Communicable diseases     Female             4.91         55.7          0.0881\n2 Communicable diseases     Male               5.47         55.7          0.0981\n3 Injuries                  Female             1.42         55.7          0.0255\n4 Injuries                  Male               3.05         55.7          0.0547\n5 Non-communicable diseases Female            19.2          55.7          0.344 \n6 Non-communicable diseases Male              21.7          55.7          0.390",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#summarise-vs-mutate",
    "href": "HealthyR_Summary.html#summarise-vs-mutate",
    "title": "",
    "section": "\nsummarise() vs mutate()\n",
    "text": "summarise() vs mutate()\n\nSo far we’ve shown you examples of using summarise() on grouped data (following group_by()) and mutate() on the whole dataset (without using group_by()).\nBut here’s the thing: mutate() is also happy to work on grouped data.\nLet’s save the aggregated example from above in a new dataframe. We will then sort the rows using arrange() based on sex, just for easier viewing (it was previously sorted by cause).\nThe arrange() function sorts the rows within a tibble:\n\ngbd_summarised &lt;- gbd2017 %&gt;% \n  group_by(cause, sex) %&gt;% \n  summarise(deaths_per_group = sum(deaths_millions)) %&gt;% \n  arrange(sex)\n\n`summarise()` has grouped output by 'cause'. You can override using the\n`.groups` argument.\n\ngbd_summarised\n\n# A tibble: 6 × 3\n# Groups:   cause [3]\n  cause                     sex    deaths_per_group\n  &lt;chr&gt;                     &lt;chr&gt;             &lt;dbl&gt;\n1 Communicable diseases     Female             4.91\n2 Injuries                  Female             1.42\n3 Non-communicable diseases Female            19.2 \n4 Communicable diseases     Male               5.47\n5 Injuries                  Male               3.05\n6 Non-communicable diseases Male              21.7 \n\n\nYou should also notice that summarise() drops all variables that are not listed in group_by() or created inside it. So year, income, and deaths_millions exist in gbd2017, but they do not exist in gbd_summarised.\nWe now want to calculate the percentage of deaths from each cause for each gender. We could use summarise() to calculate the totals:\n\ngbd_summarised_sex &lt;- gbd_summarised %&gt;% \n  group_by(sex) %&gt;% \n  summarise(deaths_per_sex = sum(deaths_per_group))\n\ngbd_summarised_sex\n\n# A tibble: 2 × 2\n  sex    deaths_per_sex\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Female           25.5\n2 Male             30.3\n\n\nBut that drops the cause and deaths_per_group columns. One way would be to now use a join on gbd_summarised and gbd_summarised_sex:\n\nfull_join(gbd_summarised, gbd_summarised_sex)\n\nJoining with `by = join_by(sex)`\n\n\n# A tibble: 6 × 4\n# Groups:   cause [3]\n  cause                     sex    deaths_per_group deaths_per_sex\n  &lt;chr&gt;                     &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1 Communicable diseases     Female             4.91           25.5\n2 Injuries                  Female             1.42           25.5\n3 Non-communicable diseases Female            19.2            25.5\n4 Communicable diseases     Male               5.47           30.3\n5 Injuries                  Male               3.05           30.3\n6 Non-communicable diseases Male              21.7            30.3\n\n\nJoining different summaries together can be useful, especially if the individual pipelines are quite long (e.g., over 5 lines of %&gt;%). However, it does increase the chance of mistakes creeping in and is best avoided if possible.\nAn alternative is to use mutate() with group_by() to achieve the same result as the full_join() above:\n\ngbd_summarised %&gt;% \n  group_by(sex) %&gt;% \n  mutate(deaths_per_sex = sum(deaths_per_group))\n\n# A tibble: 6 × 4\n# Groups:   sex [2]\n  cause                     sex    deaths_per_group deaths_per_sex\n  &lt;chr&gt;                     &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1 Communicable diseases     Female             4.91           25.5\n2 Injuries                  Female             1.42           25.5\n3 Non-communicable diseases Female            19.2            25.5\n4 Communicable diseases     Male               5.47           30.3\n5 Injuries                  Male               3.05           30.3\n6 Non-communicable diseases Male              21.7            30.3\n\n\nSo mutate() calculates the sums within each grouping variable (in this example just group_by(sex)) and puts the results in a new column without condensing the tibble down or removing any of the existing columns.\nLet’s combine all of this together into a single pipeline and calculate the percentages per cause for each gender:\n\ngbd2017 %&gt;% \n  group_by(cause, sex) %&gt;% \n  summarise(deaths_per_group = sum(deaths_millions)) %&gt;% \n  group_by(sex) %&gt;% \n  mutate(deaths_per_sex  = sum(deaths_per_group),\n         sex_cause_perc = percent(deaths_per_group/deaths_per_sex)) %&gt;% \n  arrange(sex, deaths_per_group)\n\n`summarise()` has grouped output by 'cause'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 5\n# Groups:   sex [2]\n  cause                     sex   deaths_per_group deaths_per_sex sex_cause_perc\n  &lt;chr&gt;                     &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;         \n1 Injuries                  Fema…             1.42           25.5 6%            \n2 Communicable diseases     Fema…             4.91           25.5 19%           \n3 Non-communicable diseases Fema…            19.2            25.5 75%           \n4 Injuries                  Male              3.05           30.3 10.1%         \n5 Communicable diseases     Male              5.47           30.3 18.1%         \n6 Non-communicable diseases Male             21.7            30.3 71.8%",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#common-arithmetic-functions---sum-mean-median-etc.",
    "href": "HealthyR_Summary.html#common-arithmetic-functions---sum-mean-median-etc.",
    "title": "",
    "section": "Common arithmetic functions - sum(), mean(), median(), etc.",
    "text": "Common arithmetic functions - sum(), mean(), median(), etc.\nStatistics is an R strength, so if there is an arithmetic function you can think of, it probably exists in R.\nThe most common ones are:\n\nsum()\nmean()\nmedian()\n\nmin(), max()\n\n\nsd() - standard deviation\n\nIQR() - interquartile range\n\nAn import thing to remember relates to missing data: if any of your values is NA (not available; missing), these functions will return an NA. Either deal with your missing values beforehand (recommended) or add the na.rm = TRUE argument into any of the functions to ask R to ignore missing values.\n\nmynumbers &lt;- c(1, 2, NA)\nsum(mynumbers)\n\n[1] NA\n\nsum(mynumbers, na.rm = TRUE)\n\n[1] 3",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#select-columns",
    "href": "HealthyR_Summary.html#select-columns",
    "title": "",
    "section": "\nselect() columns",
    "text": "select() columns\nThe select() function can be used to choose, rename, or reorder columns of a tibble.\nFor the following select() examples, let’s create a new tibble called gbd_2rows by taking the first 2 rows of gbd_full (just for shorter printing):\n\ngbd_2rows &lt;- gbd_full %&gt;% \n  slice(1:2)\n\ngbd_2rows\n\n                   cause year    sex       income deaths_millions\n1: Communicable diseases 1990 Female         High            0.21\n2: Communicable diseases 1990 Female Upper-Middle            1.15\n\n\nLet’s select() two of these columns:\n\ngbd_2rows %&gt;% \n  select(cause, deaths_millions)\n\n                   cause deaths_millions\n1: Communicable diseases            0.21\n2: Communicable diseases            1.15\n\n\nWe can also use select() to rename the columns we are choosing:\n\ngbd_2rows %&gt;% \n  select(cause, deaths = deaths_millions)\n\n                   cause deaths\n1: Communicable diseases   0.21\n2: Communicable diseases   1.15\n\n\nThe function rename() is similar to select(), but it keeps all variables whereas select() only kept the ones we mentioned:\n\ngbd_2rows %&gt;% \n  rename(deaths = deaths_millions)\n\n                   cause year    sex       income deaths\n1: Communicable diseases 1990 Female         High   0.21\n2: Communicable diseases 1990 Female Upper-Middle   1.15\n\n\nselect() can also be used to reorder the columns in your tibble. Moving columns around is not relevant in data analysis (as any of the functions we showed you above, as well as plotting, only look at the column names, and not their positions in the tibble), but it is useful for organising your tibble for easier viewing.\nSo if we use select like this:\n\ngbd_2rows %&gt;% \n  select(year, sex, income, cause, deaths_millions)\n\n   year    sex       income                 cause deaths_millions\n1: 1990 Female         High Communicable diseases            0.21\n2: 1990 Female Upper-Middle Communicable diseases            1.15\n\n\nThe columns are reordered.\nIf you want to move specific column(s) to the front of the tibble, do:\n\ngbd_2rows %&gt;% \n  select(year, sex, everything())\n\n   year    sex                 cause       income deaths_millions\n1: 1990 Female Communicable diseases         High            0.21\n2: 1990 Female Communicable diseases Upper-Middle            1.15\n\n\nAnd this is where the true power of select() starts to come out. In addition to listing the columns explicitly (e.g., mydata %&gt;% select(year, cause...)) there are several special functions that can be used inside select(). These special functions are called select helpers, and the first select helper we used is everything().\nThe most common select helpers are starts_with(), ends_with(), contains(), matches() (but there are several others that may be useful to you, so press F1 on select() for a full list, or search the web for more examples).\nLet’s say you can’t remember whether the deaths column was called deaths_millions or just deaths or deaths_mil, or maybe there are other columns that include the word “deaths” that you want to select():\n\ngbd_2rows %&gt;% \n  select(starts_with(\"deaths\"))\n\n   deaths_millions\n1:            0.21\n2:            1.15\n\n\nNote how “deaths” needs to be quoted inside starts_with() - as it’s a word to look for, not the real name of a column/variable.",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#reshaping-data---long-vs-wide-format",
    "href": "HealthyR_Summary.html#reshaping-data---long-vs-wide-format",
    "title": "",
    "section": "Reshaping data - long vs wide format",
    "text": "Reshaping data - long vs wide format\nSo far, all of the examples we’ve shown you have been using ‘tidy’ data. Data is ‘tidy’ when it follows a couple of rules: each variable is in its own column, and each observation is in its own row. Making data ‘tidy’ often means transforming the table from a “wide” format into a “long” format. Long format is efficient to use in data analysis and visualisation and can also be considered “computer readable”.\nBut sometimes when presenting data in tables for humans to read, or when collecting data directly into a spreadsheet, it can be convenient to have data in a wide format. Data is ‘wide’ when some or all of the columns are levels of a factor. An example makes this easier to see.\n\ngbd_wide &lt;- read_csv(\"global_burden_disease_wide-format.csv\") #add your wd\ngbd_long &lt;- read_csv(\"global_burden_disease_cause-year-sex.csv\")  ##add your wd\n\n\nhead(gbd_wide)\n\n# A tibble: 3 × 5\n  cause                     Female_1990 Female_2017 Male_1990 Male_2017\n  &lt;chr&gt;                           &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Communicable diseases            7.3         4.91      8.06      5.47\n2 Injuries                         1.41        1.42      2.84      3.05\n3 Non-communicable diseases       12.8        19.2      13.9      21.7 \n\n\n\nhead(gbd_long)\n\n# A tibble: 6 × 4\n  cause                  year sex    deaths_millions\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 Communicable diseases  1990 Female            7.3 \n2 Communicable diseases  2017 Female            4.91\n3 Communicable diseases  1990 Male              8.06\n4 Communicable diseases  2017 Male              5.47\n5 Injuries               1990 Female            1.41\n6 Injuries               2017 Female            1.42\n\n\nThese tables contain the exact same information, but in long (tidy) and wide formats, respectively.\nPivot values from rows into columns (wider)\nIf we want to take the long data and put some of the numbers next to each other for easier visualisation, then pivot_wider() from the tidyr package is the function to do it. It means we want to send a variable into columns, and it needs just two arguments: the variable we want to become the new columns, and the variable where the values currently are.\n\ngbd_long %&gt;% \n  pivot_wider(names_from = year, values_from = deaths_millions)\n\n# A tibble: 6 × 4\n  cause                     sex    `1990` `2017`\n  &lt;chr&gt;                     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Communicable diseases     Female   7.3    4.91\n2 Communicable diseases     Male     8.06   5.47\n3 Injuries                  Female   1.41   1.42\n4 Injuries                  Male     2.84   3.05\n5 Non-communicable diseases Female  12.8   19.2 \n6 Non-communicable diseases Male    13.9   21.7 \n\n\nThis means we can quickly eyeball how the number of deaths has changed from 1990 to 2017 for each cause category and sex. Whereas if we wanted to quickly look at the difference in the number of deaths for females and males, we can change the names_from = argument from = years to = sex. Furthermore, we can also add a mutate() to calculate the difference:\n\ngbd_long %&gt;% \n  pivot_wider(names_from = sex, values_from = deaths_millions) %&gt;% \n  mutate(Male - Female)\n\n# A tibble: 6 × 5\n  cause                      year Female  Male `Male - Female`\n  &lt;chr&gt;                     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1 Communicable diseases      1990   7.3   8.06           0.760\n2 Communicable diseases      2017   4.91  5.47           0.560\n3 Injuries                   1990   1.41  2.84           1.43 \n4 Injuries                   2017   1.42  3.05           1.63 \n5 Non-communicable diseases  1990  12.8  13.9            1.11 \n6 Non-communicable diseases  2017  19.2  21.7            2.59 \n\n\nAll of these differences are positive which means every year, more men die than women. Which make sense, as more boys are born than girls.\nAnd what if we want to look at both year and sex at the same time? No problem, pivot_wider() can deal with multiple variables at the same time, names_from = c(sex, year):\n\ngbd_long %&gt;% \n  pivot_wider(names_from = c(sex, year), values_from = deaths_millions)\n\n# A tibble: 3 × 5\n  cause                     Female_1990 Female_2017 Male_1990 Male_2017\n  &lt;chr&gt;                           &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Communicable diseases            7.3         4.91      8.06      5.47\n2 Injuries                         1.41        1.42      2.84      3.05\n3 Non-communicable diseases       12.8        19.2      13.9      21.7 \n\n\npivot_wider() has a few optional arguments that may be useful for you. For example, pivot_wider(..., values_fill = 0) can be used to fill empty cases (if you have any) with a value you specified. Or pivot_wider(..., names_sep = \": \") can be used to change the separator that gets put between the values (e.g., you may want “Female: 1990” instead of the default “Female_1990”). Remember that pressing F1 when your cursor is on a function opens it up in the Help tab where these extra options are listed.\nPivot values from columns to rows (longer)\nThe inverse of pivot_wider() is pivot_longer(). If you’re lucky enough, your data comes from a proper database and is already in the long and tidy format. But if not you’ll need to know how to wrangle the variables currently spread across different columns into the tidy format (where each column is a variable, each row is an observation).\npivot_longer() can be a little bit more difficult to use as you need to describe all the columns to be collected using a select_helper. Run `?select_helpers and click on the first result in the Help tab for a reminder.\nFor example, here we want to collect all the columns that include the words Female or Male, the select helper for it is matches(\"Female|Male\"):\n\ngbd_wide %&gt;% \n  pivot_longer(matches(\"Female|Male\"), \n               names_to = \"sex_year\", \n               values_to = \"deaths_millions\") %&gt;% \n  slice(1:6)\n\n# A tibble: 6 × 3\n  cause                 sex_year    deaths_millions\n  &lt;chr&gt;                 &lt;chr&gt;                 &lt;dbl&gt;\n1 Communicable diseases Female_1990            7.3 \n2 Communicable diseases Female_2017            4.91\n3 Communicable diseases Male_1990              8.06\n4 Communicable diseases Male_2017              5.47\n5 Injuries              Female_1990            1.41\n6 Injuries              Female_2017            1.42\n\n\nYou’re probably looking at the example above and thinking that’s all nice and simple on this miniature example dataset, but how on earth will I figure this out on a real-world example. And you’re right, we won’t deny that pivot_longer() is one of the most technically complicated functions in this book, and it can take a lot of trial and error to get it to work. How to get started with your own pivot_longer() transformation is to first play with the select() function to make sure you are telling R exactly which columns to pivot into the longer format. For example, before working out the pivot_longer() code for the above example, we would figure this out first:\n\ngbd_wide %&gt;% \n  select(matches(\"Female|Male\"))\n\n# A tibble: 3 × 4\n  Female_1990 Female_2017 Male_1990 Male_2017\n        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1        7.3         4.91      8.06      5.47\n2        1.41        1.42      2.84      3.05\n3       12.8        19.2      13.9      21.7 \n\n\nThen, knowing that matches(\"Female|Male\") works as expected inside our little select() test, we can copy-paste it into pivot_longer() and add the names_to and values_to arguments. Both of these arguments are new column names that you can make up (in the above example, we are using “sex_year” and “deaths_millions”).\n\nseparate() a column into multiple columns\nWhile pivot_longer() did a great job fetching the different observations that were spread across multiple columns into a single one, it’s still a combination of two variables - sex and year. We can use the separate() function to deal with that.\n\ngbd_wide %&gt;% \n  # same pivot_longer as before\n  pivot_longer(matches(\"Female|Male\"), \n               names_to = \"sex_year\", \n               values_to = \"deaths_millions\") %&gt;% \n  separate(sex_year, into = c(\"sex\", \"year\"), sep = \"_\", convert = TRUE)\n\n# A tibble: 12 × 4\n   cause                     sex     year deaths_millions\n   &lt;chr&gt;                     &lt;chr&gt;  &lt;int&gt;           &lt;dbl&gt;\n 1 Communicable diseases     Female  1990            7.3 \n 2 Communicable diseases     Female  2017            4.91\n 3 Communicable diseases     Male    1990            8.06\n 4 Communicable diseases     Male    2017            5.47\n 5 Injuries                  Female  1990            1.41\n 6 Injuries                  Female  2017            1.42\n 7 Injuries                  Male    1990            2.84\n 8 Injuries                  Male    2017            3.05\n 9 Non-communicable diseases Female  1990           12.8 \n10 Non-communicable diseases Female  2017           19.2 \n11 Non-communicable diseases Male    1990           13.9 \n12 Non-communicable diseases Male    2017           21.7 \n\n\nWe’ve also added convert = TRUE to separate() so year would get converted into a numeric variable. The combination of, e.g., “Female-1990” is a character variable, so after separating them both sex and year would still be classified as characters. But the convert = TRUE recognises that year is a number and will appropriately convert it into an integer.",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#arrange-rows",
    "href": "HealthyR_Summary.html#arrange-rows",
    "title": "",
    "section": "\narrange() rows",
    "text": "arrange() rows\nThe arrange() function sorts rows based on the column(s) you want. By default, it arranges the tibble in ascending order:\n\ngbd_long %&gt;% \n  arrange(deaths_millions) %&gt;% \n  # first 3 rows just for printing:\n  slice(1:3)\n\n# A tibble: 3 × 4\n  cause     year sex    deaths_millions\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 Injuries  1990 Female            1.41\n2 Injuries  2017 Female            1.42\n3 Injuries  1990 Male              2.84\n\n\nFor numeric variables, we can just use a - to sort in descending order:\n\ngbd_long %&gt;% \n  arrange(-deaths_millions) %&gt;% \n  slice(1:3)\n\n# A tibble: 3 × 4\n  cause                      year sex    deaths_millions\n  &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 Non-communicable diseases  2017 Male              21.7\n2 Non-communicable diseases  2017 Female            19.2\n3 Non-communicable diseases  1990 Male              13.9\n\n\nThe - doesn’t work for categorical variables; they need to be put in desc() for arranging in descending order:\n\ngbd_long %&gt;% \n  arrange(desc(sex)) %&gt;% \n  # printing rows 1, 2, 11, and 12\n  slice(1,2, 11, 12)\n\n# A tibble: 4 × 4\n  cause                      year sex    deaths_millions\n  &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 Communicable diseases      1990 Male              8.06\n2 Communicable diseases      2017 Male              5.47\n3 Non-communicable diseases  1990 Female           12.8 \n4 Non-communicable diseases  2017 Female           19.2 \n\n\nFactor levels\narrange() sorts characters alphabetically, whereas factors will be sorted by the order of their levels. Let’s make the cause column into a factor:\n\ngbd_factored &lt;- gbd_long %&gt;% \n  mutate(cause = factor(cause))\n\nWhen we first create a factor, its levels will be ordered alphabetically:\n\ngbd_factored$cause %&gt;% levels()\n\n[1] \"Communicable diseases\"     \"Injuries\"                 \n[3] \"Non-communicable diseases\"\n\n\nBut we can now use fct_relevel() inside mutate() to change the order of these levels:\n\ngbd_factored &lt;- gbd_factored %&gt;% \n  mutate(cause = cause %&gt;% \n           fct_relevel(\"Injuries\"))\n\ngbd_factored$cause %&gt;% levels()\n\n[1] \"Injuries\"                  \"Communicable diseases\"    \n[3] \"Non-communicable diseases\"\n\n\nfct_relevel() brings the level(s) listed in it to the front.\nSo if we use arrange() on gbd_factored, the cause column will be sorted based on the order of its levels, not alphabetically. This is especially useful in two places:\n\nplotting - categorical variables that are characters will be ordered alphabetically (e.g., think barplots), regardless of whether the rows are arranged or not;\nstatistical tests - the reference level of categorical variables that are characters is the alphabetically first (e.g., what the odds ratio is relative to).\n\nHowever, making a character column into a factor gives us power to give its levels a non-alphabetical order, giving us control over plotting order or defining our reference levels for use in statistical tests.",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HealthyR_Summary.html#exercises",
    "href": "HealthyR_Summary.html#exercises",
    "title": "",
    "section": "Exercises",
    "text": "Exercises\nExercise - pivot_wider()\n\nUsing the GBD dataset with variables cause, year (1990 and 2017 only), sex :\n\ngbd_long &lt;- read_csv(\"global_burden_disease_cause-year-sex.csv\")  #add your wd\n\nUse pivot_wider() to put the cause variable into columns using the deaths_millions as values:\n\n\n# A tibble: 4 × 5\n   year sex    `Communicable diseases` Injuries `Non-communicable diseases`\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;                       &lt;dbl&gt;\n1  1990 Female                    7.3      1.41                        12.8\n2  2017 Female                    4.91     1.42                        19.2\n3  1990 Male                      8.06     2.84                        13.9\n4  2017 Male                      5.47     3.05                        21.7\n\n\nSolution\n\ngbd_long &lt;- read_csv(\"global_burden_disease_cause-year-sex.csv\")\ngbd_long %&gt;% \n  pivot_wider(names_from = cause, values_from = deaths_millions)\n\nExercise - group_by(), summarise()\n\nRead in the full GBD dataset with variables cause, year, sex, income, deaths_millions.\n\ngbd_full &lt;- read_csv(\"global_burden_disease_cause-year-sex-income.csv\")\n\nglimpse(gbd_full)\n\nRows: 168\nColumns: 5\n$ cause           &lt;chr&gt; \"Communicable diseases\", \"Communicable diseases\", \"Com…\n$ year            &lt;dbl&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, …\n$ sex             &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male\"…\n$ income          &lt;chr&gt; \"High\", \"Upper-Middle\", \"Lower-Middle\", \"Low\", \"High\",…\n$ deaths_millions &lt;dbl&gt; 0.21, 1.15, 4.43, 1.51, 0.26, 1.35, 4.73, 1.72, 0.20, …\n\n\nYear 2017 of this dataset was shown before but we have multiple years avaiable: 1990, 1995, 2000, 2005, 2010, 2015, 2017.\nInvestigate these code examples:\n\nsummary_data1 &lt;- \n  gbd_full %&gt;% \n  group_by(year) %&gt;% \n  summarise(total_per_year = sum(deaths_millions))\n\nsummary_data1\n\n# A tibble: 7 × 2\n   year total_per_year\n  &lt;dbl&gt;          &lt;dbl&gt;\n1  1990           46.3\n2  1995           48.9\n3  2000           50.4\n4  2005           51.2\n5  2010           52.6\n6  2015           54.6\n7  2017           55.7\n\nsummary_data2 &lt;- \n  gbd_full %&gt;% \n  group_by(year, cause) %&gt;% \n  summarise(total_per_cause = sum(deaths_millions))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nsummary_data2\n\n# A tibble: 21 × 3\n# Groups:   year [7]\n    year cause                     total_per_cause\n   &lt;dbl&gt; &lt;chr&gt;                               &lt;dbl&gt;\n 1  1990 Communicable diseases               15.4 \n 2  1990 Injuries                             4.25\n 3  1990 Non-communicable diseases           26.7 \n 4  1995 Communicable diseases               15.1 \n 5  1995 Injuries                             4.53\n 6  1995 Non-communicable diseases           29.3 \n 7  2000 Communicable diseases               14.8 \n 8  2000 Injuries                             4.56\n 9  2000 Non-communicable diseases           31.0 \n10  2005 Communicable diseases               13.9 \n# ℹ 11 more rows\n\n\nYou should recognise that:\n\n\nsummary_data1 includes the total number of deaths per year.\n\nsummary_data2 includes the number of deaths per cause per year.\n\nsummary_data1 &lt;- means we are creating a new dataframe called summary_data1 and saving (&lt;-) results into it. If summary_data1 was a dataframe that already existed, it would get overwritten.\n\ngbd_full is the data being sent to the group_by() and then summarise() functions.\n\ngroup_by() tells summarise() that we want aggregated results for each year.\n\nsummarise() then creates a new variable called total_per_year that sums the deaths from each different observation (subcategory) together.\nCalling summary_data1 on a separate line gets it printed.\nWe then do something similar in summary_data2.\n\nCompare the number of rows (observations) and number of columns (variables) of gbd_full, summary_data1, and summary_data2.\nYou should notice that:\n\n\nsummary_data2 has exactly 3 times as many rows (observations) as summary_data1. Why?\n\ngbd_full has 5 variables, whereas the summarised tibbles have 2 and 3. Which variables got dropped? How?\n\nAnswers\n\n\ngbd_full has 168 observations (rows),\n\nsummary_data1 has 7,\n\nsummary_data2 has 21.\n\nsummary_data1 was grouped by year, therefore it includes a (summarised) value for each year in the original dataset. summary_data2 was grouped by year and cause (Communicable diseases, Injuries, Non-communicable diseases), so it has 3 values for each year.\nThe columns a summarise() function returns are: variables listed in group_by() + variables created inside summarise() (e.g., in this case deaths_peryear). All others get aggregated.\nExercise - full_join(), percent()\n\nFor each cause, calculate its percentage to total deaths in each year.\nHint: Use full_join() on summary_data1 and summary_data2, and then use mutate() to add a new column called percentage.\nExample result for a single year:\n\n\nJoining with `by = join_by(year)`\n\n\n# A tibble: 3 × 5\n   year total_per_year cause                     total_per_cause percentage\n  &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;                               &lt;dbl&gt; &lt;chr&gt;     \n1  1990           46.3 Communicable diseases               15.4  33.161%   \n2  1990           46.3 Injuries                             4.25 9.175%    \n3  1990           46.3 Non-communicable diseases           26.7  57.664%   \n\n\nSolution\n\nlibrary(scales)\nfull_join(summary_data1, summary_data2) %&gt;% \n  mutate(percentage = percent(total_per_cause/total_per_year)) \n\nExercise - mutate(), summarise()\n\nInstead of creating the two summarised tibbles and using a full_join(), achieve the same result as in the previous exercise with a single pipeline using summarise() and then mutate().\nHint: you have to do it the other way around, so group_by(year, cause) %&gt;% summarise(...) first, then group_by(year) %&gt;% mutate().\nBonus: select() columns year, cause, percentage, then pivot_wider() the cause variable using percentage as values.\nSolution\n\ngbd_full %&gt;% \n  # aggregate to deaths per cause per year using summarise()\n  group_by(year, cause) %&gt;% \n  summarise(total_per_cause = sum(deaths_millions)) %&gt;% \n  # then add a column of yearly totals using mutate()\n  group_by(year) %&gt;% \n  mutate(total_per_year = sum(total_per_cause)) %&gt;% \n  # add the percentage column\n  mutate(percentage = percent(total_per_cause/total_per_year)) %&gt;% \n  # select the final variables for better vieweing\n  select(year, cause, percentage) %&gt;% \n  pivot_wider(names_from = cause, values_from = percentage)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 7 × 4\n# Groups:   year [7]\n   year `Communicable diseases` Injuries `Non-communicable diseases`\n  &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;    &lt;chr&gt;                      \n1  1990 33%                     9%       58%                        \n2  1995 31%                     9%       60%                        \n3  2000 29%                     9%       62%                        \n4  2005 27%                     9%       64%                        \n5  2010 24%                     9%       67%                        \n6  2015 20%                     8%       72%                        \n7  2017 19%                     8%       73%                        \n\n\nNote that your pipelines shouldn’t be much longer than this, and we often save interim results into separate tibbles for checking (like we did with summary_data1 and summary_data2, making sure the number of rows are what we expect and spot checking that the calculation worked as expected).\n\nR doesn’t do what you want it to do, it does what you ask it to do. Testing and spot checking is essential as you will make mistakes. We sure do.\n\nDo not feel like you should be able to just bash out these clever pipelines without a lot of trial and error first.\nExercise - filter(), summarise(), pivot_wider()\n\nStill working with gbd_full:\n\nFilter for 1990.\nCalculate the total number of deaths in the different income groups (High, Upper-Middle, Lower-Middle, Low). Hint: use group_by(income) and summarise(new_column_name = sum(variable)).\nCalculate the total number of deaths within each income group for males and females. Hint: this is as easy as adding , sex to group_by(income).\npivot_wider() the income column.\n\nSolution\n\ngbd_full %&gt;% \n  filter(year == 1990) %&gt;% \n  group_by(income, sex) %&gt;% \n  summarise(total_deaths = sum(deaths_millions)) %&gt;% \n  pivot_wider(names_from = income, values_from = total_deaths)\n\n`summarise()` has grouped output by 'income'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 5\n  sex     High   Low `Lower-Middle` `Upper-Middle`\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 Female  4.14  2.22           8.47           6.68\n2 Male    4.46  2.57           9.83           7.95",
    "crumbs": [
      "EDA and Pre-Processing",
      "Data wrangling with R"
    ]
  },
  {
    "objectID": "HistogramBoxplot.html",
    "href": "HistogramBoxplot.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "EDA and Pre-Processing",
      "Histograms and Boxplots"
    ]
  },
  {
    "objectID": "HistogramBoxplot.html#histograms-and-boxplots",
    "href": "HistogramBoxplot.html#histograms-and-boxplots",
    "title": "",
    "section": "Histograms and Boxplots",
    "text": "Histograms and Boxplots\nHistograms show the overall shape and spread of the data distribution and box plots summarise key statistics (median, spread, and potential outliers) and are particularly helpful for comparing distributions across groups.\nSummary Statistics\n\nlibrary(ggplot2)\n\n\n# Example data\ndata &lt;- data.frame(age = c(2,  45, 56, 47, 67, 67, 68, 72, 75, 80, 85,86, 89,125))\nhead(data)\n\n  age\n1   2\n2  45\n3  56\n4  47\n5  67\n6  67\n\n\nWhat is the distribution of the age data? What are the mean, median and standard deviation?\n\nMean &lt;- mean(data$age)\nMedian &lt;- median(data$age)\nSd &lt;- sd(data$age)\n\nOther data distribution descriptors include quantiles, which divide data into equal-sized intervals:\n0% (Minimum) 25% (Q1, First Quartile) 50% (Median, Second Quartile) 75% (Q3, Third Quartile) *100% (Maximum)\n\nQuantile &lt;- quantile(data$age)\nQuantile\n\n    0%    25%    50%    75%   100% \n  2.00  58.75  70.00  83.75 125.00 \n\n\nThe IQR (Interquartile Range) measures the spread of the middle 50% of your data.\nIf you look at the Quantile results above, you can calculate it from \\(IQR=Q3−Q1\\), where Q3 is the value beyond which 75% of your data lies. What is then Q2? The value beyond which 25% of your data lies.\n\nIQR &lt;- IQR(data$age)\nIQR\n\n[1] 25\n\n\nCan do it in a single go:\n\nsummary(data$age) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00   58.75   70.00   68.86   83.75  125.00 \n\n\nAs we saw, lots of different datasets can end up with these summary statistics, so lets plot it to get real insight about what is happening:",
    "crumbs": [
      "EDA and Pre-Processing",
      "Histograms and Boxplots"
    ]
  },
  {
    "objectID": "HistogramBoxplot.html#histogram-in-r",
    "href": "HistogramBoxplot.html#histogram-in-r",
    "title": "",
    "section": "Histogram in R",
    "text": "Histogram in R\nA histogram is a way to visualise the frequency distribution of a dataset. It groups data into bins and shows how many data points fall into each bin\n\nggplot(data, aes(x = age)) + \n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nYou can modify the binwidth parameter, what is happening here?\n\nggplot(data, aes(x = age)) + \n  geom_histogram(binwidth = 30) \n\n\n\n\n\n\n\nNow lets go back t original and add the mean and median\n\nggplot(data, aes(x = age)) + \n  geom_histogram() +\n  geom_vline(xintercept = Mean, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = Median, color = \"blue\", linetype = \"dashed\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nIf do not remember how ggplot works , here is a cheatsheet: https://rstudio.github.io/cheatsheets/html/data-visualization.html\nIn summary, a histogram helps you understand the shape of your data distribution (e.g., normal, skewed) and peaks indicate the most frequent values.",
    "crumbs": [
      "EDA and Pre-Processing",
      "Histograms and Boxplots"
    ]
  },
  {
    "objectID": "HistogramBoxplot.html#box-plot-in-r",
    "href": "HistogramBoxplot.html#box-plot-in-r",
    "title": "",
    "section": "Box Plot in R",
    "text": "Box Plot in R\nA box plot (or box-and-whisker plot) summarises the distribution of a dataset. It highlights:\n\nMedian (middle line in the box).\nQuantiles (the box spans from the 25th percentile to the 75th percentile)\nIQR (Interquartile Range): The difference between the 75th and 25th percentiles - (IQR range calculated above)\nWhiskers: Extend to data points within 1.5*IQR (above or beyond = outliers)\n\n\nggplot(data, aes(x = age)) + \n  geom_boxplot() + \n  coord_flip() #have you checked what this does? Take it out and see\n\n\n\n\n\n\n\nCan you see summary statistics mapped here?\nLets plot the mean (as it is a value that is not reflected in a boxplot!)\n\nggplot(data, aes(x = age)) + \n  geom_boxplot() + \n  coord_flip() + \n   geom_vline(xintercept = Mean, color = \"blue\", linetype = \"dashed\", size = 1) \n\n\n\n\n\n\n\nBit more advanced! (add another geom layer, in this case geom_jitter. What does it do? How is it different from geom_point?)\n\n# Load ggplot2\nlibrary(ggplot2)\n\n# Sample data\nset.seed(123)\ndata &lt;- data.frame(\n  Group = rep(c(\"A\", \"B\", \"C\"), each = 200),\n  Value = c(rnorm(200, mean = 5, sd = 1), \n            rnorm(200, mean = 6, sd = 1.2), \n            rnorm(200, mean = 7, sd = 1.5))\n)\n\n# Combined Plot\np1 &lt;- ggplot(data, aes(x = Group, y = Value, fill = Group)) +\n  # Violin + Boxplot + Jitter\n  geom_boxplot(width = 0.2, outlier.color = \"red\", outlier.size = 2, colour = \"black\") +\n  geom_jitter(position = position_jitter(0.2), color = \"black\", alpha = 0.4) + #scatter plot but in random positions, check geom_point() and see\n  labs(title = \"Boxplot and Violin Plot with Jitter\", x = \"Groups\", y = \"Values\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = Value, fill = Group)) +\n  # Histogram\n  geom_histogram(alpha = 0.6, position = \"identity\", binwidth = 0.3, colour = \"black\") +\n  labs(title = \"Histogram of Values by Group\", x = \"Values\", y = \"Count\") +\n  theme_minimal()\n\n\n# Display both plots side by side\nlibrary(patchwork)\np1 + p2\n\n\n\n\n\n\n\n\n# Display both plots up down\nlibrary(patchwork)\np1 / p2\n\n\n\n\n\n\n\nAlso, maybe we want to understand how the distribution varies by more groups. We can use facets:\n\nlibrary(gapminder)\n\n# Box plot with facets by year\nggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +  # Create facets for each year\n  labs(title = \"Life Expectancy Across Continents by Year\",\n       x = \"Continent\",\n       y = \"Life Expectancy\") +\n  theme_minimal() + #just to make it nice\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels - just to make it nice\n\n\n\n\n\n\n\nWhat is happening here? View (gapminder) dataset. If we focus on 1952 year, extract the necessary values to create the boxplot for Africa.\n\n#for example \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ purrr     1.2.0     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nSubgroup &lt;- gapminder %&gt;% \n  filter(year == \"1952\") %&gt;%\n  filter(continent == \"Africa\")\n#data wrangling - can learn more from Introduction exercise on data wrangling\n\nMedian &lt;- median(Subgroup$lifeExp)\n\nMedian\n\n[1] 38.833\n\n\nDoes it match the plot? Continue with the rest of parameters (IQR, mean, quantile)",
    "crumbs": [
      "EDA and Pre-Processing",
      "Histograms and Boxplots"
    ]
  },
  {
    "objectID": "Imputation_R.html",
    "href": "Imputation_R.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "Imputation_R.html#imputation-in-r",
    "href": "Imputation_R.html#imputation-in-r",
    "title": "",
    "section": "Imputation in R",
    "text": "Imputation in R\nVisualizing imputation (NEW!!)\n\n# Load necessary libraries\n#install.packages(\"VIM\")\n\nlibrary(VIM)\nlibrary(ggplot2)\n\n\n#  Create example dataset with missing values\ndata &lt;- data.frame(\n  Age = c(25, 30, NA, 35, 40),\n  Gender = c(\"Male\", NA, \"Female\", \"Male\", \"Female\"),\n  Income = c(50000, 60000, NA, 55000, 58000),\n  Married = c(NA, \"Yes\", \"No\", \"Yes\", NA)\n)\n\ncat(\"Original Data:\\n\")\n\nOriginal Data:\n\nprint(data)\n\n  Age Gender Income Married\n1  25   Male  50000    &lt;NA&gt;\n2  30   &lt;NA&gt;  60000     Yes\n3  NA Female     NA      No\n4  35   Male  55000     Yes\n5  40 Female  58000    &lt;NA&gt;\n\n\n\n#  Visualising Missingness in Original Data\n\naggr(data, col = c(\"navyblue\", \"red\"), numbers = TRUE, sortVars = TRUE, \n     labels = names(data), cex.axis = 0.7, gap = 3, ylab = c(\"Missing Data\", \"Pattern\"))\n\n\n\n\n\n Variables sorted by number of missings: \n Variable Count\n  Married   0.4\n      Age   0.2\n   Gender   0.2\n   Income   0.2\n\n\n\n#  Mean Imputation\ndata_mean &lt;- data\ndata_mean$Age[is.na(data_mean$Age)] &lt;- mean(data_mean$Age, na.rm = TRUE)\ndata_mean$Income[is.na(data_mean$Income)] &lt;- mean(data_mean$Income, na.rm = TRUE)\n\n\n#  Median Imputation\ndata_median &lt;- data\ndata_median$Age[is.na(data_median$Age)] &lt;- median(data_median$Age, na.rm = TRUE)\ndata_median$Income[is.na(data_median$Income)] &lt;- median(data_median$Income, na.rm = TRUE)\n\n\nSmaller k Imputation is influenced by fewer, closer neighbours. The imputed values may closely match the most similar existing values. Can introduce variability or bias if the nearest neighbours are not representative of the overall distribution.\nLarger k Imputation is influenced by more neighbours, smoothing the imputed values. Reduces variability but may dilute local patterns, making the imputed values less specific.\n\n\n#  KNN Imputation (k=3)\ndata_knn &lt;- kNN(data, variable = c(\"Age\", \"Income\"), k = 3)\ndata_knn &lt;- data_knn[, colnames(data)]  # Remove '_imp' columns added by KNN\n\ncat(\"\\nAfter KNN Imputation (k=3):\\n\")\n\n\nAfter KNN Imputation (k=3):\n\nprint(data_knn)\n\n  Age Gender Income Married\n1  25   Male  50000    &lt;NA&gt;\n2  30   &lt;NA&gt;  60000     Yes\n3  35 Female  58000      No\n4  35   Male  55000     Yes\n5  40 Female  58000    &lt;NA&gt;\n\n\n\n#  Combine Data for ggplot2 Visualisation\n# Combine original, mean, median, and KNN imputation for Age\nimputed_age &lt;- data.frame(\n  Method = rep(c(\"Original\", \"Mean\", \"Median\", \"KNN\"), each = nrow(data)),\n  Age = c(data$Age, data_mean$Age, data_median$Age, data_knn$Age)\n)\n\n\n# Combine original, mean, median, and KNN imputation for Income\nimputed_income &lt;- data.frame(\n  Method = rep(c(\"Original\", \"Mean\", \"Median\", \"KNN\"), each = nrow(data)),\n  Income = c(data$Income, data_mean$Income, data_median$Income, data_knn$Income)\n)\n\n\n#  Visualise Imputed Results for Age\nggplot(imputed_age, aes(x = Method, y = Age, fill = Method)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Comparison of Age Imputation Methods\", x = \"Imputation Method\", y = \"Age\") \n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n#  Visualise Imputed Results for Income\nggplot(imputed_income, aes(x = Method, y = Income, fill = Method)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Comparison of Income Imputation Methods\", x = \"Imputation Method\", y = \"Income\") +\n  scale_fill_brewer(palette = \"Set3\")\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nWant to understand KNN better? Check this tutorial out (PYTHON) https://www.analyticsvidhya.com/blog/2020/07/knnimputer-a-robust-way-to-impute-missing-values-using-scikit-learn/"
  },
  {
    "objectID": "Imputation_R.html#one-hot-encoding",
    "href": "Imputation_R.html#one-hot-encoding",
    "title": "",
    "section": "One hot encoding",
    "text": "One hot encoding\n\ndata$Gender_Male &lt;- ifelse(data$Gender == \"Male\", 1, 0)\ndata$Gender_Female &lt;- ifelse(data$Gender == \"Female\", 1, 0)\n\n\nprint(data)\n\n  Age Gender Income Married Gender_Male Gender_Female\n1  25   Male  50000    &lt;NA&gt;           1             0\n2  30   &lt;NA&gt;  60000     Yes          NA            NA\n3  NA Female     NA      No           0             1\n4  35   Male  55000     Yes           1             0\n5  40 Female  58000    &lt;NA&gt;           0             1\n\n\nBut what if you have more categories?\n\nlibrary(caret)\n\nLoading required package: lattice\n\ndata &lt;- data.frame(\n  ID = 1:6,\n  Category = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"D\"),\n  Value = c(10, 20, 30, 40, 50, 60)\n)\n\ncat(\"Original Dataset:\\n\")\n\nOriginal Dataset:\n\nprint(data)\n\n  ID Category Value\n1  1        A    10\n2  2        B    20\n3  3        C    30\n4  4        A    40\n5  5        B    50\n6  6        D    60\n\n\nThere are other ways to do it! Other functions (e.g step_dummy() in tidymodels)\n\n#  Create Dummy Variables (All Categories)\n# Create a dummyVars object\ndummy_model &lt;- dummyVars(~ Category, data = data)\n\n\n# Apply the model to the dataset\ndummy_encoded &lt;- predict(dummy_model, newdata = data)\n\nprint(dummy_encoded)\n\n  CategoryA CategoryB CategoryC CategoryD\n1         1         0         0         0\n2         0         1         0         0\n3         0         0         1         0\n4         1         0         0         0\n5         0         1         0         0\n6         0         0         0         1\n\n\n\n# Combine Encoded Variables with Original Data\n# Combine dummy variables with the original dataset\ndata_combined &lt;- cbind(data, dummy_encoded)\nprint(data_combined)\n\n  ID Category Value CategoryA CategoryB CategoryC CategoryD\n1  1        A    10         1         0         0         0\n2  2        B    20         0         1         0         0\n3  3        C    30         0         0         1         0\n4  4        A    40         1         0         0         0\n5  5        B    50         0         1         0         0\n6  6        D    60         0         0         0         1\n\n\nWhat is changing here?\n\n#  Create Dummy Variables with Full Rank (Exclude Reference Category)\n# Create a dummyVars object with fullRank = TRUE\ndummy_model_reduced &lt;- dummyVars(~ Category, data = data, fullRank = TRUE)\n\n# Apply the model to the dataset\ndummy_encoded_reduced &lt;- predict(dummy_model_reduced, newdata = data)\n\nprint(dummy_encoded_reduced)\n\n  CategoryB CategoryC CategoryD\n1         0         0         0\n2         1         0         0\n3         0         1         0\n4         0         0         0\n5         1         0         0\n6         0         0         1\n\n\n\n# Combine reduced-rank dummy variables with the original dataset\ndata_combined_reduced &lt;- cbind(data, dummy_encoded_reduced)\n\n\nprint(data_combined_reduced)\n\n  ID Category Value CategoryB CategoryC CategoryD\n1  1        A    10         0         0         0\n2  2        B    20         1         0         0\n3  3        C    30         0         1         0\n4  4        A    40         0         0         0\n5  5        B    50         1         0         0\n6  6        D    60         0         0         1"
  },
  {
    "objectID": "MissingPackages.html",
    "href": "MissingPackages.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "EDA and Pre-Processing",
      "Missing Pipelines in R"
    ]
  },
  {
    "objectID": "MissingPackages.html#missing-pipelines-in-r",
    "href": "MissingPackages.html#missing-pipelines-in-r",
    "title": "",
    "section": "Missing Pipelines in R",
    "text": "Missing Pipelines in R\nAs with the correlation matrices before, there are really good packages to help you understand the missingness structure of your data. This is my recommendation:\n\nlibrary(naniar)\nlibrary(finalfit)\nlibrary(ggplot2)\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\nHere I will be using the airquality dataset (available as default in all R) but everything would be ready for you to introduce your own dataset in a future.\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nWe can already see with a simple summary, the ammount of missings we have per column. But what is their underlyng structure?\n\nYourDataset &lt;- airquality\n\nPlot 1\n\nll &lt;- data.frame(is.na(YourDataset))\n\nOpen up this ll object. What are you seeing here? is.na() detects all values coded as NA and gives a logical output, TRUE if truly a missing value and FALSE if not\n\nView(ll)\n\n\ncols &lt;- sapply(ll, is.logical)\nll[, cols] &lt;- lapply(ll[, cols], as.numeric)\n\nMiss1 &lt;- UpSetR::upset(ll,\n                       nsets = 20, number.angles = 10, point.size = 3.5, line.size = 2,\n                       mainbar.y.label = \"Missing Values\", sets.x.label = \"Total Number Missing Values\",\n                       text.scale = c(2.3, 2.3, 2, 2, 2, 1.75), order.by = \"freq\", sets.bar.color = \"red3\"\n)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the UpSetR package.\n  Please report the issue to the authors.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the UpSetR package.\n  Please report the issue to the authors.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the UpSetR package.\n  Please report the issue to the authors.\n\nMiss1\n\n\n\n\n\n\n\nHere we can see common patterns! If missing values were to happen at the same time in different variables, we would see it clearly. Lets store this figure into our own directories so we can then use it for our final research output!\n\npdf(\"MissingVal1.pdf\", 10, 20)\nprint(Miss1)\ndev.off()\n\nquartz_off_screen \n                2",
    "crumbs": [
      "EDA and Pre-Processing",
      "Missing Pipelines in R"
    ]
  },
  {
    "objectID": "LossFunction_Final.html",
    "href": "LossFunction_Final.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "LossFunction_Final.html#define-best-fit-minimizing-error",
    "href": "LossFunction_Final.html#define-best-fit-minimizing-error",
    "title": "",
    "section": "Define “Best Fit” – Minimizing Error",
    "text": "Define “Best Fit” – Minimizing Error\nThe “best fit” line minimizes the average distance (error) between the predicted values and the actual data points. This error (or residual) for a single data point is calculated as:\n\\[\nResidual = ActualValue - Predicted Value\n\\]"
  },
  {
    "objectID": "LossFunction_Final.html#example-with-a-single-data-point",
    "href": "LossFunction_Final.html#example-with-a-single-data-point",
    "title": "",
    "section": "Example with a Single Data Point",
    "text": "Example with a Single Data Point\nLet’s calculate the residual for a single data point in the dataset.\n\nDataSmaller &lt;- Data[1:80,]\n# Calcualte error of line (y = 112 + 0.5x)\nDataSmaller$line1 &lt;- 120 + 0.5 * seq(0, 60, length.out = 80)\nDataSmaller$linePredicted &lt;- 120 + 0.5 *DataSmaller$age\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(size = 3, color = \"black\", alpha = 0.7) +  # Larger, colored points with some transparency\n  labs(\n    x = \"Age (years)\",\n    y = \"Glucose Level (mmol/L)\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +  # Horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n  theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n    plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n    axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n    panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n    panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n  ) +\n  geom_line(aes(y = line1, x = seq(0, 60, length.out = 80)), color = \"darkorange\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nDataSmaller$linePredicted &lt;- 120 + 0.5 *DataSmaller$age\n# Get the predicted value for the first data point\npredicted_value &lt;- DataSmaller$linePredicted[63]\n\n# Calculate the residual\nactual_value &lt;- DataSmaller$glucose[63]\nresidual &lt;- actual_value - predicted_value\n\n# Print results\ncat(\"Actual Value:\", actual_value, \"\\n\")\n\nActual Value: 44 \n\ncat(\"Predicted Value:\", predicted_value, \"\\n\")\n\nPredicted Value: 138 \n\ncat(\"Residual (Error):\", residual, \"\\n\")\n\nResidual (Error): -94 \n\n\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(size = 3, color = \"black\", alpha = 0.7) +  # Larger, colored points with some transparency\n   geom_point(aes(x = DataSmaller$age[63], y = actual_value), size = 3, color = \"darkred\", alpha = 0.7) +\n   geom_point(aes(x = DataSmaller$age[63], y = predicted_value), size = 3, color = \"orange\", alpha = 0.7) +\n  geom_segment(aes(x = DataSmaller$age[63], y = actual_value, xend = DataSmaller$age[63], yend = predicted_value),\n               color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Age (years)\",\n    y = \"Glucose Level (mmol/L)\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +  # Horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n  theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n    plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n    axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n    panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n    panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n  ) +\n  geom_line(aes(y = line1, x = seq(0, 60, length.out = 80)), color = \"darkorange\", size = 1)"
  },
  {
    "objectID": "LossFunction_Final.html#sum-of-errors-squares-or-absolute-value",
    "href": "LossFunction_Final.html#sum-of-errors-squares-or-absolute-value",
    "title": "",
    "section": "Sum of Errors – squares or absolute value?",
    "text": "Sum of Errors – squares or absolute value?\nErrors can be both positive and negative, so simply summing them would lead to cancellation, which isn’t meaningful. Two common methods are:\n\n1.Sum of Squared Residuals: This squares each error, avoiding cancellation and penalizing larger errors more.\n2.Absolute Values of Residuals: Taking absolute values also prevents cancellation, but doesn’t penalize large errors as heavily as squaring."
  },
  {
    "objectID": "LossFunction_Final.html#different-error-metrics",
    "href": "LossFunction_Final.html#different-error-metrics",
    "title": "",
    "section": "Different Error Metrics",
    "text": "Different Error Metrics\nLet’s calculate different error metrics\n\n# Calculate Residuals\nresiduals &lt;- DataSmaller$glucose - DataSmaller$linePredicted\n\n# Define a function to calculate error metrics\ncalculate_errors &lt;- function(residuals) {\n  \n  # Sum of Squared Residuals (SSR)\n  SSR &lt;- sum(residuals^2)\n  \n  # Mean Squared Error (MSE) – equivalent to L2 loss\n  MSE &lt;- (SSR)/dim(DataSmaller)[1]\n  \n  # Root Mean Squared Error (RMSE)\n  RMSE &lt;- sqrt(MSE)\n  \n  # Mean Absolute Error (MAE) – equivalent to L1 loss\n  MAE &lt;- mean(abs(residuals))\n  \n  # Print the results\n  cat(\"Sum of Squared Residuals (SSR):\", SSR, \"\\n\")\n  cat(\"Mean Squared Error (MSE):\", MSE, \"\\n\")\n  cat(\"Mean Absolute Error (MAE):\", MAE, \"\\n\")\n  \n  # Return a list of the error metrics\n  return(list(SSR = SSR, MSE = MSE, RMSE = RMSE, MAE = MAE))\n}\n\nerror_metrics &lt;- calculate_errors(residuals)\n\nSum of Squared Residuals (SSR): 111807 \nMean Squared Error (MSE): 1397.588 \nMean Absolute Error (MAE): 30.575"
  },
  {
    "objectID": "LossFunction_Final.html#choosing-a-loss",
    "href": "LossFunction_Final.html#choosing-a-loss",
    "title": "",
    "section": "Choosing a loss",
    "text": "Choosing a loss\nDeciding whether to use MAE or MSE can depend on the dataset and the way you want to handle certain predictions. Most feature values in a dataset typically fall within a distinct range.Values outside the typical range and would be considered an outlier.\nWhen choosing the best loss function, consider how you want the model to treat outliers. For instance, MSE moves the model more toward the outliers, while MAE doesn’t. L2 loss incurs a much higher penalty for an outlier than L1 loss.\n\n\nMSE vs MAE\n\n\nRegardless, the functions that we will use that implement linear regression algorithms (e.g lm()) take into account MSE error, so this will not be part of any decision we have to take. The reason for this is that MSE has benefits that MAE has not in terms of optimizimg it! We will learn about this later.\nOutliers\nIn data pre-processing we discussed outliers, and here we will try and visually understand their influence when modeling.\n\n# Fit a linear model\nmodel_MSE &lt;- lm(glucose ~ age, data = DataSmaller)\nDataSmaller$predictions_MSE &lt;- predict(model_MSE, DataSmaller)\n\nprint(model_MSE$coefficients)\n\n(Intercept)         age \n  73.677460    1.330072 \n\n\nCalculate residuals\n\nresiduals_MSE &lt;- DataSmaller$glucose - DataSmaller$predictions_MSE\nresiduals_MSE &lt;- model_MSE$residuals # can also extract them directly from the model!\n\nCalculate all losses\n\nerror_metrics_MSE &lt;- calculate_errors(residuals_MSE)\n\nSum of Squared Residuals (SSR): 81632.69 \nMean Squared Error (MSE): 1020.409 \nMean Absolute Error (MAE): 24.23672 \n\n\nPlot linear regression model\n\nMSE &lt;- ggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(model_MSE)[1], slope = coef(model_MSE)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) +\n  labs(title = \"MSE without Outliers\", x = \"Age\", y = \"Glucose\")\n\nAdd Outliers\n\n# Introduce outliers \n\nDataOutliers &lt;- DataSmaller\nDataOutliers$glucose[c(1, 3, 5)] &lt;- DataOutliers$glucose[c(1, 3, 5)] * 3 # Changing 3 readings into 3 times their value! \n\nmodel_MSE_out &lt;- lm(glucose ~ age, data = DataOutliers)\nDataOutliers$predictions_MSE_out &lt;- predict(model_MSE_out, DataOutliers)\n\n#Calculate residuals\n\nresiduals_MSE_out &lt;- DataOutliers$glucose - DataOutliers$predictions_MSE_out\nresiduals_MSE_out &lt;- model_MSE_out$residuals # can also extract them directly from the model!\n\n#Calculate loss \n\nerror_metrics_MSE_out &lt;- calculate_errors(residuals_MSE_out)\n\nSum of Squared Residuals (SSR): 430818.6 \nMean Squared Error (MSE): 5385.232 \nMean Absolute Error (MAE): 38.6216 \n\nMSE_Out &lt;- ggplot(DataOutliers, aes(x = age, y = glucose)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(model_MSE_out)[1], slope = coef(model_MSE_out)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) +\n  labs(title = \"MSE Outliers\", x = \"Age\", y = \"Glucose\")\n\n\nlibrary(patchwork)\n\nMSE + MSE_Out \n\n\n\n\n\nAs key learning points: MAE vs MSE?\n\n\nLook at the scales! - MAE is more interpretable as it gives a more straightforward interpretation of the “average error,” as it represents the median prediction error.\nRobustness to Outliers: MAE is less sensitive to outliers than MSE because it doesn’t square the errors. But the functions used to implement linear regression use mostly MSE, because MAE is not differentiable, so it requires specialised optimisation algorithms, which are less computationally efficient than least-squares (MSE) for large datasets as MSE is differentiable and so easier to optimize. (we will learn about what this means later!)"
  },
  {
    "objectID": "SupervisedLearning.html",
    "href": "SupervisedLearning.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "SupervisedLearning.html#a-supervised-learning-algorithm-linear-regression",
    "href": "SupervisedLearning.html#a-supervised-learning-algorithm-linear-regression",
    "title": "",
    "section": "A Supervised Learning Algorithm: Linear Regression",
    "text": "A Supervised Learning Algorithm: Linear Regression\nSuppose we want to predict our glucose level based on our age with this dataset. We could fit a learning algorithm to our dataset and create a predictive model. In this case the learning algorithm is going to be linear regression. ggplot already contains the functionality of fitting and visualizing this linear regression model through geom_smooth as seen below:\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_smooth(method='lm', se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThis amounts to using the lm function to fit the data as you have seen in the Statistics Module statistics recap\nTo recap, the linear regression algorithm is as follows:\nIn ML, we write the equation as\n\\[\ny = \\beta_0 + \\beta_1\\, x_1\n\\]\nwhere\n\ny = outcome label ( column to predict )\n_0 = sometimes known as bias, it is the intercept of the line and is a parameter of the model\n_1 = weight of the feature/column/x 1 - same as slope in equation of a line (if we only had 2 dimensions) and is a parameter of the model\nx_1 = feature/column 1/ input data\n\nAnd how do we find out the two paramters? They are the result of fitting the learning algorithm (linear regression) to the data. Once we have them defined, we can then say we have the predictive algorithm where, if we were to have a new sample (\\(x_i\\)) with age information, we could find out the predicted glucose (\\(y_i'\\)).\nAs you will see later, te way this fitting works, is by finding the parameters _0 and _1 which make the best fit line.And this is calculated through optimization of the loss/cost function through either ordinary least squares (OLS)  or gradient descent (do not worry about this just now!)\nMore specifically, when you use the function lm below or any other function that fits a linear model in R or Python, _0 and _1 or _n (depending on the amount of features you are including in your model) are being calculated using OLS\n\n#ls_fit &lt;- lm(y ~ x, data = data)`\nls_fit &lt;- lm(glucose ~ age, data = DataSmaller)\n\nDisplay a summary of fit\n\nsummary(ls_fit)\n\n\nCall:\nlm(formula = glucose ~ age, data = DataSmaller)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-102.94  -16.49   -3.56   19.69   73.07 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  73.6775    11.9684   6.156 3.02e-08 ***\nage           1.3301     0.3237   4.110 9.71e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.35 on 78 degrees of freedom\nMultiple R-squared:  0.178, Adjusted R-squared:  0.1674 \nF-statistic: 16.89 on 1 and 78 DF,  p-value: 9.715e-05\n\n\n\ncoef(ls_fit)\n\n(Intercept)         age \n  73.677460    1.330072 \n\n\nIf we include this coefficients and draw the line ourselves, we will obtain the same thing as the geom_smooth above.\n\n#| fig-align: 'center'\n#| fig-width: 6\n#| fig-height: 4\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThe predictive model then (with fiited parameters) is:\n\\[\nglucose = 73.68 + 1.33*age\n\\] If we now have a new sample coming with \\(age = 45\\), what is the predicted glucose? Can you include it in the plot?\n\nage_newsample&lt;- 45\npredicted_glucose &lt;-  73.68 + 1.33*age_newsample\nprint(predicted_glucose)\n\n[1] 133.53\n\n\nThis is the same as doing:\n\npredict(ls_fit, newdata = data.frame(age = 45)) # we had to create a new dataframe with one value of age for this to work, as it normally expects more than 1 smple, and more features!\n\n       1 \n133.5307 \n\n\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \ngeom_point(x = age_newsample,y = predicted_glucose, colour = \"grey\", size = 5 ) +\nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5)\n\n\n\n\nQuestion!! If my new sample is \\(age = 10\\), can I still predict glucose, even though none of my data used to create the model had a sample with age = 10?\n\nage_newsample&lt;- 10\npredicted_glucose &lt;-  73.68 + 1.33*age_newsample\nprint(predicted_glucose)\n\n[1] 86.98\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \ngeom_point(x = age_newsample,y = predicted_glucose, colour = \"grey\", size = 5 ) +\nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) + \n  xlim(0, 100)\n\n\n\n\nYes we can! Another thing is how confident we are about the predicted value, but we will discuss that later on.The model is an infinite line, mapping age to glucose (so we could even map negative values, or very high age values (e.g 1000) and obtain a predicted glucose - but it would make no sense).\n\n\n\n\n# EXERCISE: Understand the role of the dataset in predictive modelling\n\n\nLets fit the decision tree learning algorithm in different number of training points (you will learn more about how decision trees work next Monday, but as you have seen in the previous classes, learning algorithms are already coded for you as functions, so you can apply them already)\n\n\n::: {.cell}\n\n\n```{.r .cell-code} library(rpart) #has functions for decision tree algorithm library(rpart.plot) library(ggplot2)\n\n\n#Decision tree algorithm tree_model &lt;- rpart(glucose ~ age, data = DataSmaller, method = “anova”)\n\n\n# Plot the decision tree rpart.plot(tree_model, type = 3, fallen.leaves = TRUE, box.palette = “Blues”, shadow.col = “gray”) ```\n\n\n::: {.cell-output-display}  ::: :::\n\n\n::: {.cell}\n\n\n{.r .cell-code} # Print the model for further details print(tree_model)\n\n\n::: {.cell-output .cell-output-stdout} ``` n= 80\n\n\nnode), split, n, deviance, yval * denotes terminal node\n\n\n1) root 80 99307.69 120.5625 2) age&lt; 22.5 10 8122.90 78.9000  3) age&gt;=22.5 70 71347.49 126.5143 6) age&lt; 38.5 42 34265.90 117.9524 12) age&gt;=26.5 31 22403.87 114.9355  13) age&lt; 26.5 11 10784.73 126.4545  7) age&gt;=38.5 28 29384.43 139.3571 14) age&lt; 50.5 16 16919.75 132.6250  15) age&gt;=50.5 12 10772.67 148.3333 * ``` ::: :::\n\n\n::: {.cell}\n\n\n\n{.r .cell-code} #Get the predicted glucose values DataSmaller$predicted_glucose &lt;- predict(tree_model, newdata = DataSmaller) :::\n\n\nThis we are now introducing the dataset that we fitted our learning algorithm on, as if they were new samples we want to have glucose predicted on. Do you see how the real value and the predicted one differ? The closer we can get them to be the better model we will have!\n\n\n::: {.cell}\n\n\n\n{.r .cell-code} View(DataSmaller) :::\n\n\nSTOP HERE AND MAKE SURE YOU HAVE UNDERSTOOD THIS CONCEPT\n\n\nDo these plots help understand?\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(DataSmaller, aes(x = age, y = glucose)) + geom_point(size = 3, color = \"blue\", alpha = 0.7) +  # Larger, colored points with some transparency geom_step( aes(y = predicted_glucose), colour = \"darkred\", size = 2) +  # Step function for #geom_step(aes(y = predicted_glucose), colour = \"darkred\",  size = 2) +  # labs( x = \"Age (years)\", y = \"Glucose Level (mmol/L)\") + theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size theme( plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title plot.subtitle = element_text(hjust = 0.5),              # Center subtitle axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity )\n\n\n::: {.cell-output-display}  ::: :::\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(DataSmaller, aes(x = age, y = glucose)) + geom_point(size = 3, color = \"blue\", alpha = 0.7) +  # Larger, colored points with some transparency geom_step( aes(y = predicted_glucose), colour = \"darkred\", size = 2) +  # Step function for geom_point(aes(x = age, y = predicted_glucose), size = 2, color = \"grey\", alpha = 0.8) + labs( x = \"Age (years)\", y = \"Glucose Level (mmol/L)\") + theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size theme( plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title plot.subtitle = element_text(hjust = 0.5),              # Center subtitle axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity )\n\n\n::: {.cell-output-display}  ::: :::\n\n\nDISCUSS THIS WITH YOUR PARTNER AND EXPLAIN IT TO EACH OTHER\n\n\n\nNow lets fit the decision tree algorithm in an even smaller set of points:\n\nDataEvenSmaller &lt;- DataSmaller[1:30, ]\ndim(DataEvenSmaller)\n\n[1] 30  3\n\n\n\n\nWhat is the model created?\n\n\nHow does it differ from the previous?\n\n\n\n\nDo the same with the linear regression model. What changes?"
  },
  {
    "objectID": "MSE_Change_FINAL.html",
    "href": "MSE_Change_FINAL.html",
    "title": "",
    "section": "",
    "text": "Code(3) Best Fit Line\n\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(ggplot2)\nlibrary(plotly)\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n\nLoad diabetes dataset (already available by installing package mlbench). This is a toy dataset that has been extensively used in many machine learning examples\n\ndata(\"PimaIndiansDiabetes\")\n\nIn the environment now you should see PimaIndiansDiabetes dataframe loaded\nLets now select only two of this columns age and glucose and store it as a new dataframe\n\nData &lt;- PimaIndiansDiabetes %&gt;%\n        select(age, glucose)\n\nLets implement this loss function in R and test it with our diabetes data.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plot3D)\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(patchwork)\n\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n\nLoad diabetes dataset, lets make the dataset even smaller so we can properly understand\n\ndata(\"PimaIndiansDiabetes\")\n\nData &lt;- PimaIndiansDiabetes %&gt;%\n  select(age, glucose, mass) %&gt;%\n  add_rownames(var = \"Patient ID\")\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\nDataSmaller &lt;- Data[1:3,]\n\nx &lt;- DataSmaller$age\ny &lt;- DataSmaller$glucose\n\nSo we have narrowed down that we are continuing exploring MSE loss function to identify the line of best fit. MSE in the end can also be written as:\n\\[\n\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\right) \\right)^2\n\\] Because the predicted y comes out of this model! So they mean the same thing.\nPAUSE for understanding!\nThis can be coded like this:\n\nmse_practical &lt;- function(beta0, beta1, x, y, m) {\n  (1 / m) * sum((y - (beta0 + beta1 * x))^2)\n}\n\nPut it to test with beta values B0 = 3 and B1 = 5 , then try with B0 = 4 and B1 = 7. Do it in a piece of paper by hand too. Remember, we already have the y (outcome label - glucose), x (glucose) and m (number of samples) defined.\n\nage &lt;- DataSmaller$age\nglucose &lt;- DataSmaller$glucose\nDataPoints &lt;- length(x)\n\nmse_practical(3, 5, age, glucose, DataPoints)\n\n[1] 5584.667\n\n\nRemember this is the same as:\n\nmse_practical(beta0 = 3, beta1 = 5, x = age, y= glucose, m =DataPoints)\n\n[1] 5584.667\n\n\nNow calculate it by hand! Use R as help (big numbers we are dealing with, but make sure you get the concept)\n Repeat process with B0 = 4 and B1 = 7. Figure continued here:\n((148 - (253))^2 + (85 - (158))^2 + (183 - (163))^2)/3\n\n((148 - (253))^2 + (85 - (158))^2 + (183 - (163))^2)/3\n\n[1] 5584.667\n\n\nSame as above!\nNow do the same for B0 = 4 and B1 = 7.\n\nClick here to reveal the solution\n\nmse_practical(beta0 = 4, beta1 = 7, x = age, y= glucose, m =DataPoints)\n\n[1] 20985.67\n\n\nMSE is lower with parameters beta0 = 3, beta1 = 5, but is it the lowest it can go? Lets plot!\nAs you can see in this plot we have two lines with the slopes previously selected. The one with the lower loss fits the data better.\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point() +\n  geom_abline(slope = 3, intercept = 5, color = \"blue\", size = 1) +\n  geom_abline(slope = 4, intercept = 7, color = \"red\", size = 1) +\n  xlim(0, 80) +\n  ylim(0, 300) +\n  labs(title = \"Different betas\",\n       x = \"Age\", y = \"Glucose\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nWe could try this out for a range of different parameters. But, very slow.\nFor example lets go back to more data points:\n\nDataSmaller &lt;- Data[1:80,]\n\nB0_values &lt;- seq(60, 80, by = 10)\nB1_values &lt;- seq(0.5, 2, by = 0.5)\n\nx_range &lt;- seq(min(DataSmaller$age), max(DataSmaller$age), length.out = 100)\n\n\nline_data &lt;- expand.grid(B0 = B0_values, B1 = B1_values) %&gt;%\n  group_by(B0, B1) %&gt;%\n  do(data.frame(x = x_range, y = .$B0 + .$B1 * x_range)) %&gt;%\n  ungroup() %&gt;%\n  mutate(label = paste(\"B0 =\", B0, \", B1 =\", B1))\n# Plot the original data points with the different regression lines\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(color = \"black\", size = 3) +  # Original data points\n  geom_line(data = line_data, aes(x = x, y = y, color = label, linetype = label), size = 1) +\n  labs(title = \"Effect of Different B0 and B1 Values on the Regression Line\",\n       x = \"Age (Predictor)\", y = \"Glucose (Response)\",\n       color = \"Parameter Combination\", linetype = \"Parameter Combination\") +\n  theme_minimal()\n\n\n\n\n\n\n\nWe cannot just keep trying randomly! How does the lm() function find out the parameters corresponding to the lowest MSE?\n\nSo we need a way in which to find the combination of parameters that yields the minimum MSE value (i.e the chosen loss function) and so the linear regression model that best fits the data points. But what is the minimum of the loss function?\nLets plot the function for us to understand it better! As we have three parameters that vary, to study the function, we need a 3D plot.\n\\[\nMSE(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\right) \\right)^2\n\\]\nUsing same betas as above but more spaced out:\n\n#B0_values &lt;- seq(10, 60, by = 20)\n#B1_values &lt;- seq(0.5, 8, by = 2)\n\nB0_values &lt;- seq(20, 120, by = 0.5)\nB1_values &lt;- seq(-5, 10, by = 0.1)\n\nWe can calculate the MSE for each combination (same way as we did by hand before) and store it in a matrix\n\ndf &lt;- expand_grid(B0_values, B1_values ) %&gt;% \n  rowwise() %&gt;%\n  mutate(MSE = mse_practical(beta0 = B0_values, beta1 = B1_values, x = DataSmaller$age, y = DataSmaller$glucose, m=length(DataSmaller$glucose)))\n\nhead(df)\n\n# A tibble: 6 × 3\n# Rowwise: \n  B0_values B1_values    MSE\n      &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1        20      -5   82650.\n2        20      -4.9 80554.\n3        20      -4.8 78485.\n4        20      -4.7 76444.\n5        20      -4.6 74430.\n6        20      -4.5 72443.\n\n\n\nmse_practical(beta0 = 40, beta1 = 0.5, x = DataSmaller$age, y= DataSmaller$glucose, m =length(DataSmaller$glucose))\n\n[1] 5067.588\n\n\nNow lets plot this function!\n\nwide_df &lt;- df %&gt;%\n  pivot_wider(\n    names_from = B1_values,  # Column names from B1_values\n    values_from = MSE        # Fill these columns with MSE values\n  )\n\n# Convert wide data to matrix for Plotly\nMSE_matrix &lt;- as.matrix(wide_df[, -1])  # Exclude the first column (B0_values)\n\n# Extract B0 and B1 values\nB0_values &lt;- sort(wide_df$B0_values)\nB1_values &lt;- as.numeric(colnames(MSE_matrix))  # Column names become B1 values\n\n\n# If the rows of MSE_matrix need to be flipped, reorder them\n#MSE_matrix &lt;- MSE_matrix[order(B0_values), ]\n\n plot_ly(\n   x = B1_values, \n   y = B0_values, \n   z = MSE_matrix,  # Use the reshaped matrix\n   type = \"surface\"\n ) %&gt;%\n   layout(\n     title = \"3D Surface Plot of MSE\",\n     scene = list(\n       xaxis = list(title = \"B1_values\"),\n       yaxis = list(title = \"B0_values\"),\n       zaxis = list(title = \"MSE\")\n     )\n   )\n\n\n\n\n\nWhat is the minimum MSE? This one! Roughly we can see that it corresponds to around x. What we also find from here is the idea that the minim point is the one at the bottom of the 3D curved surface.\n\nlibrary(plotly)\n\nmin(df$MSE)\n\n[1] 1020.578\n\n# Find the minimum point\nfilter(df, MSE == min(df$MSE))\n\n# A tibble: 1 × 3\n# Rowwise: \n  B0_values B1_values   MSE\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      74.5       1.3 1021.\n\n\n\nfig &lt;- plot_ly(\n  x = B1_values, \n  y = B0_values, \n  z = MSE_matrix,  # Use the reshaped matrix\n  type = \"surface\"\n) %&gt;%\n  layout(\n    title = \"3D Surface Plot of MSE\",\n    scene = list(\n      xaxis = list(title = \"B1_values\"),\n      yaxis = list(title = \"B0_values\"),\n      zaxis = list(title = \"MSE\"), \n       camera = list(\n        eye = list(x = 2, y = 2, z = 1) # Adjust these values for the angle\n      )\n    )\n  )\n\n# Add the minimum point\nfig &lt;- fig %&gt;%\n  add_trace(\n    x = filter(df, MSE == min(df$MSE))$B1_values,\n    y = filter(df, MSE == min(df$MSE))$B0_values,\n    z = filter(df, MSE == min(df$MSE))$MSE,\n    mode = \"markers\",\n    type = \"scatter3d\",\n    marker = list(size = 10, color = \"red\"),\n    name = \"Minimum MSE\"\n  )\n\n# Render the plot\nfig\n\n\n\n\n\nThese parameters are very similar to what we obtained from the lm() function (and would be the same if we had sampled the MSE at that accurate decimal point)\n\nlm(glucose ~ age, data = DataSmaller)\n\n\nCall:\nlm(formula = glucose ~ age, data = DataSmaller)\n\nCoefficients:\n(Intercept)          age  \n      73.68         1.33  \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Pima_Challenge_Explore.html",
    "href": "Pima_Challenge_Explore.html",
    "title": "Pima Machine Learning Challenge – Exploratory Preprocessing (R & Python)",
    "section": "",
    "text": "Pima ML Challenge\n\nExplore preprocessing choices\n\n\n\n\n1. Load and inspect the data\nUse the Pima Indians Diabetes dataset and work with:\n\npredictors: age, glucose, insulin\noutcome: pedigree\n\nCreate a train/test split and reuse it for all experiments.\nTools you will need:\n\nR: initial_split(), training(), testing()\nPython: train_test_split() from scikit‑learn\n\n\n\n\n2. Baseline model (no preprocessing)\nFit a simple regression model:\n\nOutcome: pedigree\nPredictors: age + glucose + insulin + diabetes\n\nGoal:\nGet a baseline MAE against which all your later models will be compared.\nTools:\n\nR:\n\nmodel: linear_reg(engine = \"lm\")\n\nevaluation: mae()\n\n\nPython:\n\nmodel: LinearRegression()\n\nevaluation: mean_absolute_error()\n\n\nResources: tidymodels.org / scikit-learn.org\n\n\n\n3. Mean imputation (leak-free)\nSome insulin and glucose values are missing.\nTry mean imputation, but do it correctly:\n\nmean should be computed from the training data only\napply that mean to both training and test sets\n\nTools:\n\nR (recipes):\n\nimputation step to explore: step_impute_mean()\n\n\nPython:\n\nSimpleImputer(strategy=\"mean\")\n\n\nTask:\nCompute MAE and compare with the baseline.\n\n\n\n4. Try alternative imputation methods\nExperiment with different imputation choices:\n\nmedian\n\nconstant value\n\noptional: bagged tree imputation (R)\n\nTools:\n\nR:\n\nstep_impute_median()\n\nstep_impute_bagged()\n\n\nPython:\n\nSimpleImputer(strategy=\"median\")\n\nSimpleImputer(strategy=\"constant\")\n\n\nTask:\nWhich imputer gives the best MAE?\nWhich one behaves unexpectedly?\n\n\n\n5. Log-transform insulin (after imputation)\nCreate a new variable:\n[ = () ]\nDo this after imputation to avoid problems with missing values or zeros.\nTools:\n\nR: step_log()\n\nPython: FunctionTransformer or manual np.log1p()\n\nTask:\nCompare MAE between using insulin vs. insulin_log.\n\n\n\n6. Categorical age + one-hot encoding\nTurn age into three categories:\n\n&lt; 30\n30−60\n&gt; 60\n\nThen convert that categorical variable into dummy variables.\nTools:\n\nR:\n\ncut age into categories (mutate() or step_cut())\n\nstep_dummy() for one-hot encoding\n\n\nPython:\n\npd.cut()\n\nOneHotEncoder()\n\n\nTask:\nCheck out coefficients of the model created, how do they differ? Are you identifying any issues when splitting the data?\n\n\n\n7. Build your own full pipeline\nCombine any of the following:\n\nyour preferred imputation method\n\nlog-transform\n\nscaling\n\nage category & one‑hot encoding\n\nadditional engineered features if you want\n\nTools:\n\nR:\n\nrecipe() + steps of your choice\n\nbuild model with a workflow\n\nevaluate with last_fit() if you want — or evaluate manually\n\n\nPython:\n\nColumnTransformer()\n\nPipeline()\n\nevaluate with MAE\n\n\nTask:\nCreate your best-performing MAE model.\n\n\n\n8. Compare all your models\nCreate a small results table summarising:\n\nbaseline MAE\n\neach imputation MAE\n\nlog-transform MAE\n\nage category MAE\n\nyour final best model MAE\n\nAdd a short reflection:\n\nWhich transformation improved performance?\nWhich added complexity for no benefit?\nWhat does this suggest about the dataset?\n\n\n\n\nResources You May Want to Explore\n\nR\n\nhttps://www.tidymodels.org/start/recipes/\n\n\n\nPython\n\nhttps://scikit-learn.org/stable/modules/impute.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n\nhttps://scikit-learn.org/stable/modules/compose.ColumnTransformer.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n\nExtra: This dataset has been widely studied, check out this python pipeline (until Pipeline factory in chunk 60) for a good preprocessing and EDA example\n\nhttps://github.com/tarekmasryo/pima-diabetes-pipeline/blob/main/diabetes-prediction-from-eda-to-production.ipynb\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima Machine Learning Challenge – Exploratory Preprocessing (R & Python)"
    ]
  },
  {
    "objectID": "pima_preprocessing_sklearn_python.html",
    "href": "pima_preprocessing_sklearn_python.html",
    "title": "Pima – Imputation & Scaling with scikit-learn (Python)",
    "section": "",
    "text": "This document shows how to perform mean imputation and standardisation correctly in Python using scikit-learn, first manually and then using a Pipeline.\nWe load the same Pima Indians database and select the following columns:\nCodeimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima – Imputation & Scaling with scikit-learn (Python)"
    ]
  },
  {
    "objectID": "pima_preprocessing_sklearn_python.html#example-data-placeholder",
    "href": "pima_preprocessing_sklearn_python.html#example-data-placeholder",
    "title": "Pima – Imputation & Scaling with scikit-learn (Python)",
    "section": "1. Example data (placeholder)",
    "text": "1. Example data (placeholder)\nReplace this with your real data load. Here we just create a tiny artificial example so the code runs.\n\nCode\n\nurl=\"https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\"\ndf = pd.read_csv(url)\n\ndf.columns = df.columns.str.lower()\ndf = df.rename(columns={\"diabetespedigreefunction\": \"pedigree\"})\n\n\nfeatures = [\"age\", \"glucose\", \"insulin\"]\ndf[features] = df[features].replace(0, np.nan)\nX = df[features]\ny = df[\"pedigree\"]",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima – Imputation & Scaling with scikit-learn (Python)"
    ]
  },
  {
    "objectID": "pima_preprocessing_sklearn_python.html#manual-preprocessing-impute-scale-no-pipeline",
    "href": "pima_preprocessing_sklearn_python.html#manual-preprocessing-impute-scale-no-pipeline",
    "title": "Pima – Imputation & Scaling with scikit-learn (Python)",
    "section": "2. Manual preprocessing: impute + scale (no Pipeline)",
    "text": "2. Manual preprocessing: impute + scale (no Pipeline)\nHere we explicitly:\n\nSplit into train and test.\nFit SimpleImputer on the training set and transform train & test.\nFit StandardScaler on the training set and transform train & test.\nFit a LinearRegression model and compute MAE on the test set.\n\n\nCode\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=123\n)\n\n# 1. Mean imputation (TRAIN only)\nimputer = SimpleImputer(strategy=\"mean\")\nX_train_imp = imputer.fit_transform(X_train)   # learns means from TRAIN\nX_test_imp  = imputer.transform(X_test)        # applies TRAIN means\n\n# 2. Standardisation (TRAIN only)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_imp)  # learns μ, σ from TRAIN\nX_test_scaled  = scaler.transform(X_test_imp)       # uses TRAIN μ, σ\n\n\n\nCode# 3. Linear regression on preprocessed data\nmodel = LinearRegression()\nmodel.fit(X_train_scaled, y_train)\n\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLinearRegression\n\n?Documentation for LinearRegressioniFitted\n\n        \n            Parameters\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n    \n\n\n\n\nCodey_pred = model.predict(X_test_scaled)\nmae_manual = mean_absolute_error(y_test, y_pred)\nmae_manual\n\n0.24771496683579874",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima – Imputation & Scaling with scikit-learn (Python)"
    ]
  },
  {
    "objectID": "pima_preprocessing_sklearn_python.html#using-a-scikit-learn-pipeline-cleanest-version",
    "href": "pima_preprocessing_sklearn_python.html#using-a-scikit-learn-pipeline-cleanest-version",
    "title": "Pima – Imputation & Scaling with scikit-learn (Python)",
    "section": "3. Using a scikit-learn Pipeline (cleanest version)",
    "text": "3. Using a scikit-learn Pipeline (cleanest version)\nPipelines package preprocessing + model into a single object:\n\nImputation and scaling are still fit on the training data only.\nThe same transformations are automatically applied to any new data.\n\n\nCodenumeric_features = [\"age\", \"glucose\", \"insulin\"]\n\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n        (\"scaler\", StandardScaler()),\n    ]\n)\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features)\n    ]\n)\n\nmodel = LinearRegression()\n\npipe = Pipeline(\n    steps=[\n        (\"preprocess\", preprocess),\n        (\"model\", model),\n    ]\n)\n\nX = df[numeric_features]\ny = df[\"pedigree\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=123\n)\n\npipe.fit(X_train, y_train)\n\n\n\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'glucose',\n                                                   'insulin'])])),\n                ('model', LinearRegression())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nPipeline\n\n?Documentation for PipelineiFitted\n\n        \n            Parameters\n\n\nsteps \n[('preprocess', ...), ('model', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n    \n\n\n\n\npreprocess: ColumnTransformer\n?Documentation for preprocess: ColumnTransformer\n        \n            Parameters\n\n\ntransformers \n[('num', ...)]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n    \n\n\n\nnum['age', 'glucose', 'insulin']\n\n\n\nSimpleImputer\n?Documentation for SimpleImputer\n        \n            Parameters\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'mean'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n    \n\n\nStandardScaler\n?Documentation for StandardScaler\n        \n            Parameters\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n    \n\n\n\n\n\nLinearRegression\n?Documentation for LinearRegression\n        \n            Parameters\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n    \n\n\n\n\n\n\nCodey_pred_pipe = pipe.predict(X_test)\n\nmae_pipe = mean_absolute_error(y_test, y_pred_pipe)\nmae_pipe\n\n0.24771496683579874\n\n\n\nCodeprint(\"MAE (manual preprocessing):\", mae_manual)\n\nMAE (manual preprocessing): 0.24771496683579874\n\nCodeprint(\"MAE (Pipeline):           \", mae_pipe)\n\nMAE (Pipeline):            0.24771496683579874\n\n\nIn practice, you should prefer the Pipeline approach:\n\nlower risk of mistakes,\neasy extension to cross-validation and more complex models.",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima – Imputation & Scaling with scikit-learn (Python)"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html",
    "href": "pima_leakage_pipeline.html",
    "title": "Pima Preprocessing → Leak-free ML Pipeline (R)",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(janitor)\nlibrary(tidymodels)\n\ntheme_set(theme_minimal())\nset.seed(123)",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima Preprocessing → Leak-free ML Pipeline (R)"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html#data-and-goal",
    "href": "pima_leakage_pipeline.html#data-and-goal",
    "title": "Pima Preprocessing → Leak-free ML Pipeline (R)",
    "section": "1. Data and goal",
    "text": "1. Data and goal\nWe will use the PimaIndiansDiabetes2 dataset and focus on a simple regression problem:\n\n\nOutcome: pedigree (diabetes pedigree function)\n\n\nPredictors: age, glucose\n\n\nOur goal is to compare two pipelines:\n\nA wrong one, where we scale before splitting (data leakage).\nA correct one, where we scale using training data only.\n\nWe’ll use MAE (Mean Absolute Error) to compare test performance.\n\nCodedata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\npima_small &lt;- PimaIndiansDiabetes2 %&gt;%\n  clean_names() %&gt;%\n  select(age, glucose, pedigree) %&gt;%\n  filter(!is.na(pedigree))  # outcome must be present\n\nhead(pima_small)\n\n  age glucose pedigree\n1  50     148    0.627\n2  31      85    0.351\n3  32     183    0.672\n4  21      89    0.167\n5  33     137    2.288\n6  30     116    0.201\n\nCodesummary(pima_small)\n\n      age           glucose         pedigree     \n Min.   :21.00   Min.   : 44.0   Min.   :0.0780  \n 1st Qu.:24.00   1st Qu.: 99.0   1st Qu.:0.2437  \n Median :29.00   Median :117.0   Median :0.3725  \n Mean   :33.24   Mean   :121.7   Mean   :0.4719  \n 3rd Qu.:41.00   3rd Qu.:141.0   3rd Qu.:0.6262  \n Max.   :81.00   Max.   :199.0   Max.   :2.4200  \n                 NA's   :5",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima Preprocessing → Leak-free ML Pipeline (R)"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html#wrong-pipeline-scaling-before-traintest-split-leakage",
    "href": "pima_leakage_pipeline.html#wrong-pipeline-scaling-before-traintest-split-leakage",
    "title": "Pima Preprocessing → Leak-free ML Pipeline (R)",
    "section": "2. WRONG pipeline: scaling before train–test split (leakage)",
    "text": "2. WRONG pipeline: scaling before train–test split (leakage)\n2.1. Compute global mean and sd (leaky)\n\nCodemean_all_glucose &lt;- mean(pima_small$glucose, na.rm = TRUE)\nsd_all_glucose   &lt;- sd(pima_small$glucose, na.rm = TRUE)\n\nmean_all_age &lt;- mean(pima_small$age, na.rm = TRUE)\nsd_all_age   &lt;- sd(pima_small$age, na.rm = TRUE)\n\nmean_all_glucose; sd_all_glucose\n\n[1] 121.6868\n\n\n[1] 30.53564\n\nCodemean_all_age; sd_all_age\n\n[1] 33.24089\n\n\n[1] 11.76023\n\n\n2.2. Scale the full dataset using global statistics\n\nCodepima_small_scaled &lt;- pima_small %&gt;%\n  mutate(\n    glucose_scaled = (glucose - mean_all_glucose) / sd_all_glucose,\n    age_scaled     = (age - mean_all_age) / sd_all_age\n  )\n\nhead(pima_small_scaled)\n\n  age glucose pedigree glucose_scaled  age_scaled\n1  50     148    0.627      0.8617221  1.42506672\n2  31      85    0.351     -1.2014407 -0.19054773\n3  32     183    0.672      2.0079237 -0.10551539\n4  21      89    0.167     -1.0704463 -1.04087112\n5  33     137    2.288      0.5014873 -0.02048305\n6  30     116    0.201     -0.1862336 -0.27558007\n\n\n2.3. Train–test split on already scaled data\n\nCodeset.seed(123)\ndata_split &lt;- initial_split(pima_small_scaled, prop = 0.7)\n\ntrain &lt;- training(data_split)\ntest  &lt;- testing(data_split)\n\nnrow(train); nrow(test)\n\n[1] 537\n\n\n[1] 231\n\n\n2.4. Fit linear regression model and compute MAE (WRONG)\n\nCodeped_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nfit_wrong &lt;- ped_model %&gt;%\n  fit(pedigree ~ age_scaled + glucose_scaled, data = train)\n\nresults_wrong &lt;- predict(fit_wrong, new_data = test) %&gt;%\n  bind_cols(test)\n\nmae_wrong &lt;- mae(\n  data    = results_wrong,\n  truth   = pedigree,\n  estimate = .pred\n)\n\nmae_wrong\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.234",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima Preprocessing → Leak-free ML Pipeline (R)"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html#right-pipeline-scaling-using-training-data-only-no-leakage",
    "href": "pima_leakage_pipeline.html#right-pipeline-scaling-using-training-data-only-no-leakage",
    "title": "Pima Preprocessing → Leak-free ML Pipeline (R)",
    "section": "3. RIGHT pipeline: scaling using training data only (no leakage)",
    "text": "3. RIGHT pipeline: scaling using training data only (no leakage)\n3.1. Train–test split on the original data\n\nCodeset.seed(123)\ndata_split_right &lt;- initial_split(pima_small, prop = 0.8)\n\ntrain &lt;- training(data_split_right)\ntest  &lt;- testing(data_split_right)\n\nnrow(train); nrow(test)\n\n[1] 614\n\n\n[1] 154\n\n\n3.2. Compute training-only mean and sd\n\nCodemu_tr_glucose &lt;- mean(train$glucose, na.rm = TRUE)\nsd_tr_glucose &lt;- sd(train$glucose, na.rm = TRUE)\n\nmu_tr_age &lt;- mean(train$age, na.rm = TRUE)\nsd_tr_age &lt;- sd(train$age, na.rm = TRUE)\n\nmu_tr_glucose; sd_tr_glucose\n\n[1] 122.2902\n\n\n[1] 30.25447\n\nCodemu_tr_age; sd_tr_age\n\n[1] 33.1759\n\n\n[1] 11.81636\n\n\n3.3. Scale train and test using training statistics\n\nCodetrain_scaled &lt;- train %&gt;%\n  mutate(\n    glucose_scaled = (glucose - mu_tr_glucose) / sd_tr_glucose,\n    age_scaled     = (age - mu_tr_age)     / sd_tr_age\n  )\n\ntest_scaled &lt;- test %&gt;%\n  mutate(\n    glucose_scaled = (glucose - mu_tr_glucose) / sd_tr_glucose,\n    age_scaled     = (age - mu_tr_age)     / sd_tr_age\n  )\n\nhead(train_scaled)\n\n    age glucose pedigree glucose_scaled age_scaled\n415  21     138    0.534      0.5192568 -1.0304267\n463  39      74    0.705     -1.5961334  0.4928847\n179  47     143    0.190      0.6845216  1.1699120\n526  21      87    0.444     -1.1664448 -1.0304267\n195  42      85    0.136     -1.2325507  0.7467699\n118  25      78    0.654     -1.4639215 -0.6919131\n\nCodehead(test_scaled)\n\n   age glucose pedigree glucose_scaled  age_scaled\n1   50     148    0.627      0.8497865  1.42379718\n3   32     183    0.672      2.0066404 -0.09951419\n9   53     197    0.158      2.4693820  1.67768241\n17  31     118    0.551     -0.1418027 -0.18414260\n22  50      99    0.388     -0.7698091  1.42379718\n27  43     147    0.257      0.8167335  0.83139832\n\n\n3.4. Fit linear regression model and compute MAE (RIGHT)\n\nCodeped_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nfit_right &lt;- ped_model %&gt;%\n  fit(pedigree ~ age_scaled + glucose_scaled, data = train_scaled)\n\nresults_right &lt;- predict(fit_right, new_data = test_scaled) %&gt;%\n  bind_cols(test_scaled)\n\nmae_right &lt;- mae(\n  data    = results_right,\n  truth   = pedigree,\n  estimate = .pred\n)\n\nmae_right\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.234",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima Preprocessing → Leak-free ML Pipeline (R)"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html#comparing-mae-leaky-vs-leak-free",
    "href": "pima_leakage_pipeline.html#comparing-mae-leaky-vs-leak-free",
    "title": "Pima Preprocessing → Leak-free ML Pipeline (R)",
    "section": "4. Comparing MAE: leaky vs leak-free",
    "text": "4. Comparing MAE: leaky vs leak-free\n\nCodetibble(\n  pipeline = c(\"WRONG: scaled before split (leakage)\",\n               \"RIGHT: scaled using training only\"),\n  MAE = c(mae_wrong$.estimate,\n          mae_right$.estimate)\n)\n\n# A tibble: 2 × 2\n  pipeline                               MAE\n  &lt;chr&gt;                                &lt;dbl&gt;\n1 WRONG: scaled before split (leakage) 0.234\n2 RIGHT: scaled using training only    0.234",
    "crumbs": [
      "EDA and Pre-Processing",
      "Pima Preprocessing → Leak-free ML Pipeline (R)"
    ]
  }
]