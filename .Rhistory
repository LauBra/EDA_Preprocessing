final_tree <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = best_params$maxdepth,
minsplit = best_params$minsplit))
rpart.plot(final_tree)  # Plot the final model
# Evaluate the final model
final_pred <- predict(final_tree, testData, type = "class")
# Evaluate the final model
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, testData$Outcome)
# Evaluate the final model
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, test_data$diabetes)
print(final_conf_matrix)
final_pred <- predict(final_tree, train_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, train_data$diabetes)
print(final_conf_matrix)# Plot the final model
# Evaluate the final model
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, test_data$diabetes)
print(final_conf_matrix)
# Load libraries
library(ranger) #random forest learning algorithm package! Although many others too
library(tidymodels)
library(mlbench)
library(tidyverse)
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Split into training and testing datasets
set.seed(123)
data_split <- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)
train_data <- training(data_split)
test_data <- testing(data_split)
basic_tree <- ranger(diabetes ~ ., data = train_data, seed = 123) # Need this because random bootstrapping plus random feature splitting!
basic_tree
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "class")
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "response")
head(pred)
conf_matrix <- confusionMatrix(pred, train_data$diabetes)
pred
pred
train_data$diabetes
pred
predict(basic_tree, train_data, type = "response")
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "response")
head(pred)
conf_matrix <- confusionMatrix(pred$predictions, train_data$diabetes)
print(conf_matrix)
# Evaluate the basic model
pred <- predict(basic_tree, test_data, type = "response")
conf_matrix <- confusionMatrix(pred$predictions, test_data$diabetes)
print(conf_matrix)
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(mtry = 3))
rpart.plot(tuned_tree_depth)  # Plot the tuned tree
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(mtry = 3))
rpart.plot(tuned_tree_depth)  # Plot the tuned tree
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(mtry = 3))
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "class")
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "response")
conf_matrix_depth <- confusionMatrix(pred_depth_train$, train_data$diabetes)
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "response")
conf_matrix_depth <- confusionMatrix(pred_depth_train$predictions, train_data$diabetes)
print(conf_matrix_depth)
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(mtry = 3,min.node.size = 10     ))
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data, method = "class",
mtry = 3,min.node.size = 10 )
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data,
mtry = 3,min.node.size = 10 )
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "response")
conf_matrix_depth <- confusionMatrix(pred_depth_train$predictions, train_data$diabetes)
print(conf_matrix_depth)
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data,
mtry = 3 )
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "response")
conf_matrix_depth <- confusionMatrix(pred_depth_train$predictions, train_data$diabetes)
print(conf_matrix_depth)
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data,
mtry = 2,min.node.size = 20 )
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "response")
conf_matrix_depth <- confusionMatrix(pred_depth_train$predictions, train_data$diabetes)
print(conf_matrix_depth)
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data,
mtry = 2,min.node.size = 50 )
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "response")
conf_matrix_depth <- confusionMatrix(pred_depth_train$predictions, train_data$diabetes)
print(conf_matrix_depth)
pred_depth_test <- predict(tuned_tree_depth, test_data, type = "response")
conf_matrix_depth <- confusionMatrix(pred_depth_test$predictions, test_data$diabetes)
print(conf_matrix_depth)
# Manually tune hyperparameters (example with mtry)
tuned_tree_depth <- ranger(diabetes ~ ., data = train_data,
mtry = 2,min_n = 50, mode = "classification" )
# Create a grid of hyperparameters
rf_grid <- grid_regular(
mtry(range = c(2, 7)),
min_n(range = c(2, 10)),
levels = 5
)
head(rf_grid)
# Define the Random Forest model with tunable parameters
rf_model <- rand_forest(
mtry = tune(),
trees = 500,
min_n = tune()
) %>%
set_engine("ranger") %>%
set_mode("classification")
# Define the workflow
rf_workflow <- workflow() %>%
add_model(rf_model) %>%
add_formula(diabetes ~ .)
# Create a grid of hyperparameters
rf_grid <- grid_regular(
mtry(range = c(2, 7)),
min_n(range = c(2, 10)),
levels = 5
)
head(rf_grid)
tuning_results <- rf_workflow %>%
tune_grid(
#resamples = bootstraps(train_data, times = 1),  # Single resample (no CV)
grid = rf_grid,
metrics = metric_set(accuracy)
)
tuning_results <- rf_workflow %>%
tune_grid(
resamples = bootstraps(train_data, times = 1),  # Single resample (no CV)
grid = rf_grid,
metrics = metric_set(accuracy)
)
# Extract the best parameters based on accuracy
best_params <- select_best(tuning_results, "accuracy")
# Finalise the workflow with the best parameters
final_rf_workflow <- finalize_workflow(rf_workflow, best_params)
# Fit the final model on the training data
final_rf_model <- final_rf_workflow %>%
fit(data = train_data)
# Evaluate the final model on the test data
final_predictions <- predict(final_rf_model, test_data) %>%
bind_cols(test_data)
final_metrics <- final_predictions %>%
metrics(truth = diabetes, estimate = .pred_class)
# Display metrics
print(final_metrics)
# Evaluate the final model on the test data
final_predictions <- predict(final_rf_model, train_data) %>%
bind_cols(train_data)
final_metrics <- final_predictions %>%
metrics(truth = diabetes, estimate = .pred_class)
# Display metrics
print(final_metrics)
# Load libraries
library(rpart)
library(rpart.plot)
library(tidymodels)
library(mlbench)
library(tidyverse)
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Split into training and testing datasets
set.seed(123)
data_split <- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)
train_data <- training(data_split)
test_data <- testing(data_split)
basic_tree <- rpart(diabetes ~ ., data = train_data, method = "class")
basic_tree
rpart.plot(basic_tree, extra = 101)  # Plot the tree
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "prob")
head(pred)
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "class")
head(pred)
conf_matrix <- confusionMatrix(pred, train_data$diabetes)
print(conf_matrix)
# Evaluate the basic model
pred <- predict(basic_tree, test_data, type = "class")
conf_matrix <- confusionMatrix(pred, test_data$diabetes)
print(conf_matrix)
# Manually tune hyperparameters (example with maxdepth)
tuned_tree_depth <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = 3))
rpart.plot(tuned_tree_depth)  # Plot the tuned tree
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "class")
conf_matrix_depth <- confusionMatrix(pred_depth_train, train_data$diabetes)
print(conf_matrix_depth)
pred_depth_test <- predict(tuned_tree_depth, test_data, type = "class")
conf_matrix_depth <- confusionMatrix(pred_depth_test, test_data$diabetes)
print(conf_matrix_depth)
# Grid search for hyperparameter tuning
grid <- expand.grid(maxdepth = 3:5,                   # Maximum depth
minsplit = c(10, 20))          # Minimum splits
View(grid)
results <- data.frame()  # To store results
for (i in 1:nrow(grid)) {
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = grid$maxdepth[i],
minsplit = grid$minsplit[i]))
pred <- predict(tree_model, train_data, type = "class")
accuracy <- sum(pred == train_data$diabetes) / nrow(train_data)
results <- rbind(results, cbind(grid[i, ], Accuracy = accuracy))
}
# Display sorted results
results <- results[order(-results$Accuracy), ]
print(results)
# Train the final model with the best hyperparameters
best_params <- results[1, ]
final_tree <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = best_params$maxdepth,
minsplit = best_params$minsplit))
rpart.plot(final_tree)
final_pred <- predict(final_tree, train_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, train_data$diabetes)
print(final_conf_matrix)# Plot the final model
# Evaluate the final model
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, test_data$diabetes)
print(final_conf_matrix)
# Load libraries
library(rpart)
library(rpart.plot)
library(tidymodels)
library(mlbench)
library(tidyverse)
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Split into training and testing datasets
set.seed(123)
data_split <- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)
train_data <- training(data_split)
test_data <- testing(data_split)
basic_tree <- rpart(diabetes ~ ., data = train_data, method = "class")
basic_tree
rpart.plot(basic_tree, extra = 101)  # Plot the tree
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "prob")
head(pred)
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "class")
head(pred)
conf_matrix <- confusionMatrix(pred, train_data$diabetes)
print(conf_matrix)
# Evaluate the basic model
pred <- predict(basic_tree, test_data, type = "class")
conf_matrix <- confusionMatrix(pred, test_data$diabetes)
print(conf_matrix)
# Manually tune hyperparameters (example with maxdepth)
tuned_tree_depth <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = 3))
rpart.plot(tuned_tree_depth)  # Plot the tuned tree
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "class")
conf_matrix_depth <- confusionMatrix(pred_depth_train, train_data$diabetes)
print(conf_matrix_depth)
pred_depth_test <- predict(tuned_tree_depth, test_data, type = "class")
conf_matrix_depth <- confusionMatrix(pred_depth_test, test_data$diabetes)
print(conf_matrix_depth)
# Grid search for hyperparameter tuning
grid <- expand.grid(maxdepth = 3:5,                   # Maximum depth
minsplit = c(10, 20))          # Minimum splits
View(grid)
results <- data.frame()  # To store results
for (i in 1:nrow(grid)) {
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = grid$maxdepth[i],
minsplit = grid$minsplit[i]))
pred <- predict(tree_model, train_data, type = "class")
accuracy <- sum(pred == train_data$diabetes) / nrow(train_data)
results <- rbind(results, cbind(grid[i, ], Accuracy = accuracy))
}
# Display sorted results
results <- results[order(-results$Accuracy), ]
print(results)
# Train the final model with the best hyperparameters
best_params <- results[1, ]
final_tree <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = best_params$maxdepth,
minsplit = best_params$minsplit))
rpart.plot(final_tree)
final_pred <- predict(final_tree, train_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, train_data$diabetes)
print(final_conf_matrix)# Plot the final model
# Evaluate the final model
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, test_data$diabetes)
print(final_conf_matrix)
# Load libraries
library(rpart)
library(rpart.plot)
library(tidymodels)
library(mlbench)
library(tidyverse)
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Split into training and testing datasets
set.seed(123)
data_split <- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)
train_data <- training(data_split)
test_data <- testing(data_split)
basic_tree <- rpart(diabetes ~ ., data = train_data, method = "class")
basic_tree
rpart.plot(basic_tree, extra = 101)  # Plot the tree
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "prob")
head(pred)
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "class")
head(pred)
conf_matrix <- confusionMatrix(pred, train_data$diabetes)
print(conf_matrix)
# Evaluate the basic model
pred <- predict(basic_tree, test_data, type = "class")
conf_matrix <- confusionMatrix(pred, test_data$diabetes)
print(conf_matrix)
# Manually tune hyperparameters (example with maxdepth)
tuned_tree_depth <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = 3))
rpart.plot(tuned_tree_depth)  # Plot the tuned tree
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "class")
conf_matrix_depth <- confusionMatrix(pred_depth_train, train_data$diabetes)
print(conf_matrix_depth)
pred_depth_test <- predict(tuned_tree_depth, test_data, type = "class")
conf_matrix_depth <- confusionMatrix(pred_depth_test, test_data$diabetes)
print(conf_matrix_depth)
# Grid search for hyperparameter tuning
grid <- expand.grid(maxdepth = 3:5,                   # Maximum depth
minsplit = c(10, 20))          # Minimum splits
results <- data.frame()  # To store results
for (i in 1:nrow(grid)) {
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = grid$maxdepth[i],
minsplit = grid$minsplit[i]))
pred <- predict(tree_model, train_data, type = "class")
accuracy <- sum(pred == train_data$diabetes) / nrow(train_data)
results <- rbind(results, cbind(grid[i, ], Accuracy = accuracy))
}
# Display sorted results
results <- results[order(-results$Accuracy), ]
print(results)
# Train the final model with the best hyperparameters
best_params <- results[1, ]
final_tree <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = best_params$maxdepth,
minsplit = best_params$minsplit))
rpart.plot(final_tree)
final_pred <- predict(final_tree, train_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, train_data$diabetes)
print(final_conf_matrix)# Plot the final model
# Evaluate the final model
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, test_data$diabetes)
print(final_conf_matrix)
1/1 + exp(-(1+ 2+1))
1/(1 + exp(-(1+ 2+1)))
# Generate data for the logistic function
x_values <- seq(-10, 10, by = 0.1)
logistic_curve <- data.frame(
x = x_values,
y = 1 / (1 + exp(-(1 + 2 + 1)))
)
# Plot using ggplot2
ggplot(logistic_curve, aes(x = x, y = y)) +
geom_line(size = 1.2) +
labs(
title = "Logistic Function",
x = "x",
y = "f(x)"
) +
theme_minimal()
View(logistic_curve)
x1 <- 1
x2 <- 2
1/(1 + exp(-(1+ 2*x1+1*x2)))
1/(1 + exp(-(1+ 2*x1+0.5*x2)))
x1 <- -3
x2 <- -5
1/(1 + exp(-(1+ 2*x1+0.5*x2)))
x1 <- -1
x2 <- -1
1/(1 + exp(-(1+ 2*x1+0.5*x2)))
x1 <- -1
x2 <- -1
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 0.3
x2 <- 0.5
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 0.3
x2 <- 0.005
1/(1 + exp(-(3+ 2*x1+5*x2)))
3/-5
2/-5
x1 <- 1
x2 <- 5
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 1
x2 <- 5
x1 <- 1
x2 <- 4
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 1
x2 <- -4
1/(1 + exp(-(3+ 2*x1+5*x2)))
log_class <- function(x1, x2){
1/(1 + exp(-(3+ 2*x1+5*x2)))
}
x_values1 <- seq(-10, 10, by = 0.1)
x_values2 <- seq(-10, 10, by = 0.1)
y_values <- log_class(x_values1, x_values2)
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 10
x2 <- 2
y_values <- log_class(x_values1, x_values2)
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 2
x2 <- 2
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 1
x2 <- 2
1/(1 + exp(-(3+ 2*x1+5*x2)))
# Load libraries
library(rpart)
library(rpart.plot)
library(tidymodels)
library(mlbench)
library(tidyverse)
library(caret) # for confusionMatrix function!
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Train the final model with the best hyperparameters
best_params <- results[1, ]
# Load libraries
library(rpart)
library(rpart.plot)
library(tidymodels)
library(mlbench)
library(tidyverse)
library(caret) # for confusionMatrix function!
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Split into training and testing datasets
set.seed(123)
data_split <- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)
train_data <- training(data_split)
test_data <- testing(data_split)
reticulate::install_miniconda()
reticulate::repl_python()
library(reticulate)
# one-off setup (if you haven't done it yet)
# install_miniconda()
##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)
use_condaenv("hds-python", required = TRUE)
#py_config()
#conda_install("hds-python", c("jupyter", "plotly"))
reticulate::repl_python()
getwd()
reticulate::repl_python()
library(reticulate)
# one-off setup (if you haven't done it yet)
# install_miniconda()
##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)
use_condaenv("hds-python", required = TRUE)
#py_config()
#conda_install("hds-python", c("jupyter", "plotly"))
reticulate::repl_python()
reticulate::repl_python()
setwd("~/Library/CloudStorage/OneDrive-UniversityofBirmingham/Desktop/ML_Classes2025/Day2/Practical")
library(reticulate)
# one-off setup (if you haven't done it yet)
# install_miniconda()
##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)
use_condaenv("hds-python", required = TRUE)
#py_config()
#conda_install("hds-python", c("jupyter", "plotly"))
reticulate::repl_python()
