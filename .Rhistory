# Grid search for hyperparameter tuning
grid <- expand.grid(maxdepth = 3:5,                   # Maximum depth
minsplit = c(10, 20))          # Minimum splits
View(grid)
results <- data.frame()  # To store results
for (i in 1:nrow(grid)) {
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = grid$maxdepth[i],
minsplit = grid$minsplit[i]))
pred <- predict(tree_model, train_data, type = "class")
accuracy <- sum(pred == train_data$diabetes) / nrow(train_data)
results <- rbind(results, cbind(grid[i, ], Accuracy = accuracy))
}
# Display sorted results
results <- results[order(-results$Accuracy), ]
print(results)
# Train the final model with the best hyperparameters
best_params <- results[1, ]
final_tree <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = best_params$maxdepth,
minsplit = best_params$minsplit))
rpart.plot(final_tree)
final_pred <- predict(final_tree, train_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, train_data$diabetes)
print(final_conf_matrix)# Plot the final model
# Evaluate the final model
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, test_data$diabetes)
print(final_conf_matrix)
# Load libraries
library(rpart)
library(rpart.plot)
library(tidymodels)
library(mlbench)
library(tidyverse)
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Split into training and testing datasets
set.seed(123)
data_split <- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)
train_data <- training(data_split)
test_data <- testing(data_split)
basic_tree <- rpart(diabetes ~ ., data = train_data, method = "class")
basic_tree
rpart.plot(basic_tree, extra = 101)  # Plot the tree
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "prob")
head(pred)
# Evaluate the basic model
pred <- predict(basic_tree, train_data, type = "class")
head(pred)
conf_matrix <- confusionMatrix(pred, train_data$diabetes)
print(conf_matrix)
# Evaluate the basic model
pred <- predict(basic_tree, test_data, type = "class")
conf_matrix <- confusionMatrix(pred, test_data$diabetes)
print(conf_matrix)
# Manually tune hyperparameters (example with maxdepth)
tuned_tree_depth <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = 3))
rpart.plot(tuned_tree_depth)  # Plot the tuned tree
pred_depth_train <- predict(tuned_tree_depth, train_data, type = "class")
conf_matrix_depth <- confusionMatrix(pred_depth_train, train_data$diabetes)
print(conf_matrix_depth)
pred_depth_test <- predict(tuned_tree_depth, test_data, type = "class")
conf_matrix_depth <- confusionMatrix(pred_depth_test, test_data$diabetes)
print(conf_matrix_depth)
# Grid search for hyperparameter tuning
grid <- expand.grid(maxdepth = 3:5,                   # Maximum depth
minsplit = c(10, 20))          # Minimum splits
results <- data.frame()  # To store results
for (i in 1:nrow(grid)) {
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = grid$maxdepth[i],
minsplit = grid$minsplit[i]))
pred <- predict(tree_model, train_data, type = "class")
accuracy <- sum(pred == train_data$diabetes) / nrow(train_data)
results <- rbind(results, cbind(grid[i, ], Accuracy = accuracy))
}
# Display sorted results
results <- results[order(-results$Accuracy), ]
print(results)
# Train the final model with the best hyperparameters
best_params <- results[1, ]
final_tree <- rpart(diabetes ~ ., data = train_data, method = "class",
control = rpart.control(maxdepth = best_params$maxdepth,
minsplit = best_params$minsplit))
rpart.plot(final_tree)
final_pred <- predict(final_tree, train_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, train_data$diabetes)
print(final_conf_matrix)# Plot the final model
# Evaluate the final model
final_pred <- predict(final_tree, test_data, type = "class")
final_conf_matrix <- confusionMatrix(final_pred, test_data$diabetes)
print(final_conf_matrix)
1/1 + exp(-(1+ 2+1))
1/(1 + exp(-(1+ 2+1)))
# Generate data for the logistic function
x_values <- seq(-10, 10, by = 0.1)
logistic_curve <- data.frame(
x = x_values,
y = 1 / (1 + exp(-(1 + 2 + 1)))
)
# Plot using ggplot2
ggplot(logistic_curve, aes(x = x, y = y)) +
geom_line(size = 1.2) +
labs(
title = "Logistic Function",
x = "x",
y = "f(x)"
) +
theme_minimal()
View(logistic_curve)
x1 <- 1
x2 <- 2
1/(1 + exp(-(1+ 2*x1+1*x2)))
1/(1 + exp(-(1+ 2*x1+0.5*x2)))
x1 <- -3
x2 <- -5
1/(1 + exp(-(1+ 2*x1+0.5*x2)))
x1 <- -1
x2 <- -1
1/(1 + exp(-(1+ 2*x1+0.5*x2)))
x1 <- -1
x2 <- -1
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 0.3
x2 <- 0.5
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 0.3
x2 <- 0.005
1/(1 + exp(-(3+ 2*x1+5*x2)))
3/-5
2/-5
x1 <- 1
x2 <- 5
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 1
x2 <- 5
x1 <- 1
x2 <- 4
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 1
x2 <- -4
1/(1 + exp(-(3+ 2*x1+5*x2)))
log_class <- function(x1, x2){
1/(1 + exp(-(3+ 2*x1+5*x2)))
}
x_values1 <- seq(-10, 10, by = 0.1)
x_values2 <- seq(-10, 10, by = 0.1)
y_values <- log_class(x_values1, x_values2)
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 10
x2 <- 2
y_values <- log_class(x_values1, x_values2)
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 2
x2 <- 2
1/(1 + exp(-(3+ 2*x1+5*x2)))
x1 <- 1
x2 <- 2
1/(1 + exp(-(3+ 2*x1+5*x2)))
# Load libraries
library(rpart)
library(rpart.plot)
library(tidymodels)
library(mlbench)
library(tidyverse)
library(caret) # for confusionMatrix function!
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Train the final model with the best hyperparameters
best_params <- results[1, ]
# Load libraries
library(rpart)
library(rpart.plot)
library(tidymodels)
library(mlbench)
library(tidyverse)
library(caret) # for confusionMatrix function!
# Load the dataset and view
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
PimaIndiansDiabetes$diabetes <- factor(PimaIndiansDiabetes$diabetes, levels = c("pos", "neg"))  # Convert Outcome to a factor
# Split into training and testing datasets
set.seed(123)
data_split <- initial_split(PimaIndiansDiabetes, prop = 0.7, strata = diabetes)
train_data <- training(data_split)
test_data <- testing(data_split)
reticulate::install_miniconda()
reticulate::repl_python()
library(reticulate)
# one-off setup (if you haven't done it yet)
# install_miniconda()
##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)
use_condaenv("hds-python", required = TRUE)
#py_config()
#conda_install("hds-python", c("jupyter", "plotly"))
reticulate::repl_python()
getwd()
reticulate::repl_python()
library(reticulate)
# one-off setup (if you haven't done it yet)
# install_miniconda()
##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)
use_condaenv("hds-python", required = TRUE)
#py_config()
#conda_install("hds-python", c("jupyter", "plotly"))
reticulate::repl_python()
reticulate::repl_python()
setwd("~/Library/CloudStorage/OneDrive-UniversityofBirmingham/Desktop/ML_Classes2025/Day2/Practical")
library(reticulate)
# one-off setup (if you haven't done it yet)
# install_miniconda()
##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)
use_condaenv("hds-python", required = TRUE)
#py_config()
#conda_install("hds-python", c("jupyter", "plotly"))
reticulate::repl_python()
#Remember, if you do not have these packages installed, install them first with (install.packages())
library(ggplot2)
library(patchwork)
library(gapminder)
library(tidyverse)
# Example data
data <- data.frame(age = c(2,  45, 56, 47, 67, 67, 68, 72, 75, 80, 85,86, 89,125))
head(data)
Mean <- mean(data$age)
Median <- median(data$age)
Sd <- sd(data$age)
Quantile <- quantile(data$age)
Quantile
IQR <- IQR(data$age)
IQR
summary(data$age)
ggplot(data, aes(x = age)) +
geom_histogram()
ggplot(data, aes(x = age)) +
geom_histogram(binwidth = 30)
ggplot(data, aes(x = age)) +
geom_histogram() +
geom_vline(xintercept = Mean, color = "red", linetype = "dashed", size = 1) +
geom_vline(xintercept = Median, color = "blue", linetype = "dashed", size = 1)
ggplot(data, aes(x = age)) +
geom_boxplot() +
coord_flip() #have you checked what this does? Take it out and see
ggplot(data, aes(x = age)) +
geom_boxplot() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
# Sample data
set.seed(123)
data <- data.frame(
Group = rep(c("A", "B", "C"), each = 200),
Value = c(rnorm(200, mean = 5, sd = 1),
rnorm(200, mean = 6, sd = 1.2),
rnorm(200, mean = 7, sd = 1.5))
)
# Combined Plot
p1 <- ggplot(data, aes(x = Group, y = Value, fill = Group)) +
# Violin + Boxplot + Jitter
geom_boxplot(width = 0.2, outlier.color = "red", outlier.size = 2, colour = "black") +
geom_jitter(position = position_jitter(0.2), color = "black", alpha = 0.4) + #scatter plot but in random positions, check geom_point() and see
labs(title = "Boxplot and Violin Plot with Jitter", x = "Groups", y = "Values") +
theme_minimal()
p2 <- ggplot(data, aes(x = Value, fill = Group)) +
# Histogram
geom_histogram(alpha = 0.6, position = "identity", binwidth = 0.3, colour = "black") +
labs(title = "Histogram of Values by Group", x = "Values", y = "Count") +
theme_minimal()
# Display both plots side by side (thanks to having patchwork installed)
p1 + p2
# Display both plots up down
p1 / p2
# Box plot with facets by year
ggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +
geom_boxplot() +
facet_wrap(~ year) +  # Create facets for each year
labs(title = "Life Expectancy Across Continents by Year",
x = "Continent",
y = "Life Expectancy") +
theme_minimal() + #just to make it nice
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels - just to make it nice
Subgroup <- gapminder %>%
filter(year == "1952") %>%
filter(continent == "Africa")
Median <- median(Subgroup$lifeExp)
Median
ggplot(data, aes(x = age, y = age)) +
geom_boxplot() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
ggplot(data, aes(x = age, y = age)) +
geom_boxplot() +
geom_point() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
ggplot(data, aes(x = age, y = 1)) +
geom_boxplot() +
geom_point() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
ggplot(data, aes(x = age, y = 1 )) +
geom_boxplot() +
geom_point() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
# Example data
data <- data.frame(age = c(2,  45, 56, 47, 67, 67, 68, 72, 75, 80, 85,86, 89,125))
head(data)
ggplot(data, aes(x = age, y = 1 )) +
geom_boxplot() +
geom_point() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
ggplot(data, aes(x = age, y = "age" )) +
geom_boxplot() +
geom_point() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
ggplot(data, aes(x = age, y = "age" )) +
geom_boxplot() +
geom_jitter() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
View(data)
?ggplot2
ggplot(data, aes(x = age)) +
geom_boxplot() +
coord_flip() +
geom_point() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
ggplot(data, aes(x = age, y = "age" )) +
geom_boxplot() +
geom_point() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
ggplot(data, aes(x = age, y = "age" )) +
geom_boxplot() +
geom_jitter() +
coord_flip() +
geom_vline(xintercept = Mean, color = "blue", linetype = "dashed", size = 1)
#Remember, if you do not have these packages installed, install them first with (install.packages())
library(ggplot2)
library(patchwork)
library(gapminder)
library(tidyverse)
# Box plot with facets by year
ggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +
geom_boxplot() +
facet_wrap(~ year) +  # Create facets for each year
labs(title = "Life Expectancy Across Continents by Year",
x = "Continent",
y = "Life Expectancy") +
theme_minimal() + #just to make it nice
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels - just to make it nice
gapminder <- gapminder
View(gapminder)
# Install required libraries (if not already installed)
#install.packages(c("ggplot2", "corrplot", "ggcorrplot",
#"PerformanceAnalytics", "GGally", "psych", "corrr"))
# Load necessary libraries
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(PerformanceAnalytics)
library(GGally)
library(psych)
library(corrr)
set.seed(123)
x <- rnorm(100)
y <- x + rnorm(100, sd = 0.5)
ggplot(data = data.frame(x, y), aes(x = x, y = y)) +
geom_point(color = "blue", size = 2) +
geom_smooth(method = "lm", color = "red", se = FALSE) +
labs(title = "Correlation Plot with ggplot2",
x = "X values", y = "Y values") +
theme_minimal()
data <- mtcars[, c("mpg", "disp", "hp", "wt")]
cor_matrix <- cor(data)
corrplot(cor_matrix, method = "circle", type = "upper",
title = "Correlation Matrix", addCoef.col = "black")
ggcorrplot(cor_matrix,
method = "square",
type = "lower",
lab = TRUE,
title = "mtcars",
lab_size = 3) +
theme_minimal()
chart.Correlation(data, histogram = TRUE, pch = 19)
ggpairs(data,
title = "Pairwise Correlation Plot with GGally",
lower = list(continuous = wrap("smooth", method = "lm")),
upper = list(continuous = wrap("cor", size = 3)))
pairs.panels(data,
method = "pearson",
hist.col = "lightblue",
density = TRUE,
ellipses = TRUE)
# ----- 8. Base R Heatmap -----
heatmap(cor_matrix, symm = TRUE,
main = "Correlation Heatmap",
col = colorRampPalette(c("red", "white", "blue"))(20))
# ----- 10. corrr Tidy Correlation -----
cor_matrix_tidy <- correlate(data)
cor_matrix_tidy %>%
fashion() %>%
print()
# Network-style correlation plot
cor_matrix_tidy %>%
network_plot(min_cor = 0.3)
#Remember, if you do not have these packages installed, install them first with (install.packages())
library(naniar)
library(finalfit)
library(ggplot2)
library(mice)
summary(airquality)
YourDataset <- airquality
ll <- data.frame(is.na(YourDataset))
View(ll)
cols <- sapply(ll, is.logical)
ll[, cols] <- lapply(ll[, cols], as.numeric)
Miss1 <- UpSetR::upset(ll,
nsets = 20, number.angles = 10, point.size = 3.5, line.size = 2,
mainbar.y.label = "Missing Values", sets.x.label = "Total Number Missing Values",
text.scale = c(2.3, 2.3, 2, 2, 2, 1.75), order.by = "freq", sets.bar.color = "red3"
)
Miss1
pdf("MissingVal1.pdf", 10, 20)
print(Miss1)
dev.off()
print(vis_miss(YourDataset, warn_large_data = FALSE) +
theme(axis.text.x =  element_text(angle = 90)))
# uncomment to have your pdf created
#pdf("Missing2.pdf", 20, 10)
#print(vis_miss(YourDataset, warn_large_data = FALSE) +
#        theme(axis.text.x =  element_text(angle = 90)))
#dev.off()
missing_pattern(YourDataset)
# uncomment to have your pdf created
#pdf("PlotMissing3.pdf", 4,11)
#
#print(missing_pattern(YourDataset))
#
#dev.off()
#
#Remember, if you do not have these packages installed, install them first with (install.packages())
library(naniar)
library(finalfit)
library(ggplot2)
library(mice)
summary(airquality)
YourDataset <- airquality
ll <- data.frame(is.na(YourDataset))
View(ll)
cols <- sapply(ll, is.logical)
ll[, cols] <- lapply(ll[, cols], as.numeric)
Miss1 <- UpSetR::upset(ll,
nsets = 20, number.angles = 10, point.size = 3.5, line.size = 2,
mainbar.y.label = "Missing Values", sets.x.label = "Total Number Missing Values",
text.scale = c(2.3, 2.3, 2, 2, 2, 1.75), order.by = "freq", sets.bar.color = "red3"
)
Miss1
pdf("MissingVal1.pdf", 10, 20)
print(Miss1)
dev.off()
print(vis_miss(YourDataset, warn_large_data = FALSE) +
theme(axis.text.x =  element_text(angle = 90)))
# uncomment to have your pdf created
#pdf("Missing2.pdf", 20, 10)
#print(vis_miss(YourDataset, warn_large_data = FALSE) +
#        theme(axis.text.x =  element_text(angle = 90)))
#dev.off()
missing_pattern(YourDataset)
# uncomment to have your pdf created
#pdf("PlotMissing3.pdf", 4,11)
#
#print(missing_pattern(YourDataset))
#
#dev.off()
#
View(YourDataset)
View(YourDataset)
getwd()
library(reticulate)
# one-off setup (if you haven't done it yet)
# install_miniconda()
##conda_create(
##  envname = "hds-python",
##  python_version = "3.11",
##  packages = c("numpy", "pandas", "matplotlib", "seaborn", "scikit-learn")
##)
use_condaenv("hds-python", required = TRUE)
#py_config()
#conda_install("hds-python", c("jupyter", "plotly"))
reticulate::repl_python()
