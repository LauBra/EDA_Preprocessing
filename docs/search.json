[
  {
    "objectID": "Pima_Challenge_Explore.html",
    "href": "Pima_Challenge_Explore.html",
    "title": "Extra challenge",
    "section": "",
    "text": "Pima ML Challenge\n\nExplore preprocessing choices\n\n\n\n\n1. Load and inspect the data\nUse the Pima Indians Diabetes dataset and work with:\n\npredictors: age, glucose, insulin\noutcome: pedigree\n\nCreate a train/test split and reuse it for all experiments.\nTools you will need:\n\nR: initial_split(), training(), testing()\nPython: train_test_split() from scikit‑learn\n\n\n\n\n2. Baseline model (no preprocessing)\nFit a simple regression model:\n\nOutcome: pedigree\nPredictors: age + glucose + insulin + diabetes\n\nGoal:\nGet a baseline MAE against which all your later models will be compared.\nTools:\n\nR:\n\nmodel: linear_reg(engine = \"lm\")\n\nevaluation: mae()\n\n\nPython:\n\nmodel: LinearRegression()\n\nevaluation: mean_absolute_error()\n\n\nResources: tidymodels.org / scikit-learn.org\n\n\n\n3. Mean imputation (leak-free)\nSome insulin and glucose values are missing.\nTry mean imputation, but do it correctly:\n\nmean should be computed from the training data only\napply that mean to both training and test sets\n\nTools:\n\nR (recipes):\n\nimputation step to explore: step_impute_mean()\n\n\nPython:\n\nSimpleImputer(strategy=\"mean\")\n\n\nTask:\nCompute MAE and compare with the baseline.\n\n\n\n4. Try alternative imputation methods\nExperiment with different imputation choices:\n\nmedian\n\nconstant value\n\noptional: bagged tree imputation (R)\n\nTools:\n\nR:\n\nstep_impute_median()\n\nstep_impute_bagged()\n\n\nPython:\n\nSimpleImputer(strategy=\"median\")\n\nSimpleImputer(strategy=\"constant\")\n\n\nTask:\nWhich imputer gives the best MAE?\nWhich one behaves unexpectedly?\n\n\n\n5. Log-transform insulin (after imputation)\nCreate a new variable:log(insulin)\nDo this after imputation to avoid problems with missing values or zeros.\nTools:\n\nR: step_log()\n\nPython: FunctionTransformer or manual np.log1p()\n\nTask:\nCompare MAE between using insulin vs. insulin_log.\n\n\n\n6. Categorical age + one-hot encoding\nTurn age into three categories:\n\n&lt; 30\n30−60\n&gt; 60\n\nThen convert that categorical variable into dummy variables.\nTools:\n\nR:\n\ncut age into categories (mutate() or step_cut())\n\nstep_dummy() for one-hot encoding\n\n\nPython:\n\npd.cut()\n\nOneHotEncoder()\n\n\nTask:\nCheck out coefficients of the model created, how do they differ? Are you identifying any issues when splitting the data?\n\n\n\n7. Build your own full pipeline\nCombine any of the following:\n\nyour preferred imputation method\n\nlog-transform\n\nscaling\n\nage category & one‑hot encoding\n\nadditional engineered features if you want\n\nTools:\n\nR:\n\nrecipe() + steps of your choice\n\nbuild model with a workflow\n\nevaluate with last_fit() if you want — or evaluate manually\n\n\nPython:\n\nColumnTransformer()\n\nPipeline()\n\nevaluate with MAE\n\n\nTask:\nCreate your best-performing MAE model.\n\n\n\n8. Compare all your models\nCreate a small results table summarising:\n\nbaseline MAE\n\neach imputation MAE\n\nlog-transform MAE\n\nage category MAE\n\nyour final best model MAE\n\nAdd a short reflection:\n\nWhich transformation improved performance?\nWhich added complexity for no benefit?\nWhat does this suggest about the dataset?\n\n\n\n\nResources to Explore\n\nR\n\nhttps://www.tidymodels.org/start/recipes/\n\n\n\nPython\n\nhttps://scikit-learn.org/stable/modules/impute.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n\nhttps://scikit-learn.org/stable/modules/compose.ColumnTransformer.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n\nExtra: This dataset has been widely studied, check out this python pipeline (until Pipeline factory in chunk 60) for a good preprocessing and EDA example\n\nhttps://github.com/tarekmasryo/pima-diabetes-pipeline/blob/main/diabetes-prediction-from-eda-to-production.ipynb\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "EDA and Pre-Processing",
      "Extra challenge"
    ]
  },
  {
    "objectID": "pima_leakage_tidymodels_R.html",
    "href": "pima_leakage_tidymodels_R.html",
    "title": "(8) Complete pipeline leakage - R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\nlibrary(tidymodels)\n\ntheme_set(theme_minimal())\nset.seed(123)\nWe will use the PimaIndiansDiabetes2 dataset and focus on simple regression problems where:\nWe will:\nShow the correct, leak-free approach using recipes + workflows in tidymodels. Please be sure to check out anything new for you here: https://www.tidymodels.org/start/recipes/\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\npima_base &lt;- PimaIndiansDiabetes2 %&gt;%\n  clean_names() %&gt;%\n  select(age, glucose, insulin, pedigree) %&gt;%\n  filter(!is.na(pedigree))  # outcome must be present\n\nhead(pima_base)\n\n  age glucose insulin pedigree\n1  50     148      NA    0.627\n2  31      85      NA    0.351\n3  32     183      NA    0.672\n4  21      89      94    0.167\n5  33     137     168    2.288\n6  30     116      NA    0.201\n\nsummary(pima_base)\n\n      age           glucose         insulin          pedigree     \n Min.   :21.00   Min.   : 44.0   Min.   : 14.00   Min.   :0.0780  \n 1st Qu.:24.00   1st Qu.: 99.0   1st Qu.: 76.25   1st Qu.:0.2437  \n Median :29.00   Median :117.0   Median :125.00   Median :0.3725  \n Mean   :33.24   Mean   :121.7   Mean   :155.55   Mean   :0.4719  \n 3rd Qu.:41.00   3rd Qu.:141.0   3rd Qu.:190.00   3rd Qu.:0.6262  \n Max.   :81.00   Max.   :199.0   Max.   :846.00   Max.   :2.4200  \n                 NA's   :5       NA's   :374",
    "crumbs": [
      "EDA and Pre-Processing",
      "(8) Complete pipeline leakage - R"
    ]
  },
  {
    "objectID": "pima_leakage_tidymodels_R.html#doing-it-properly-with-tidymodels-recipes-impute-scale",
    "href": "pima_leakage_tidymodels_R.html#doing-it-properly-with-tidymodels-recipes-impute-scale",
    "title": "(8) Complete pipeline leakage - R",
    "section": "Doing it properly with tidymodels recipes (impute + scale)",
    "text": "Doing it properly with tidymodels recipes (impute + scale)\nWe let recipes handle both:\n\n\nstep_impute_mean() → mean imputation\n\n\nstep_normalize() → standardisation\n\nThe recipe is fitted on the training data only and then applied to the test data via last_fit().\n\npima_rec &lt;- pima_base %&gt;%\n  select(age, glucose, insulin, pedigree)\n\nset.seed(123)\nrec_split &lt;- initial_split(pima_rec, prop = 0.7)\n\npima_train &lt;- training(rec_split)\npima_test  &lt;- testing(rec_split)\n\npima_train %&gt;% head()\n\n    age glucose insulin pedigree\n415  21     138     167    0.534\n463  39      74      49    0.705\n179  47     143      NA    0.190\n526  21      87      NA    0.444\n195  42      85      NA    0.136\n118  25      78      NA    0.654\n\n\n\n# Recipe: impute + scale all predictors\nped_recipe &lt;- recipe(pedigree ~ age + glucose + insulin, data = pima_train) %&gt;%\n  step_impute_mean(all_predictors()) %&gt;%\n  step_normalize(all_predictors())\n\nped_recipe\n\n\nped_model_rec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nped_wf &lt;- workflow() %&gt;%\n  add_model(ped_model_rec) %&gt;%\n  add_recipe(ped_recipe)\n\nped_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n# Fit on TRAIN, evaluate on TEST\nfinal_fit &lt;- last_fit(\n  ped_wf,\n  split   = rec_split,\n  metrics = metric_set(mae)\n)\n\ncollect_metrics(final_fit)\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mae     standard       0.233 pre0_mod0_post0\n\n\n\n# Optional: inspect the baked design matrix\nprepped_recipe &lt;- prep(ped_recipe)\n\nbaked_train &lt;- bake(prepped_recipe, new_data = pima_train)\nbaked_test  &lt;- bake(prepped_recipe, new_data = pima_test)\n\nhead(baked_train)\n\n# A tibble: 6 × 4\n     age glucose   insulin pedigree\n   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 -1.03    0.512  1.34e- 1    0.534\n2  0.495  -1.61  -1.22e+ 0    0.705\n3  1.17    0.678  3.59e-15    0.19 \n4 -1.03   -1.18   3.59e-15    0.444\n5  0.748  -1.25   3.59e-15    0.136\n6 -0.688  -1.48   3.59e-15    0.654",
    "crumbs": [
      "EDA and Pre-Processing",
      "(8) Complete pipeline leakage - R"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html",
    "href": "pima_leakage_pipeline.html",
    "title": "(6) Simple pipeline leakage (scaling) -R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\nlibrary(tidymodels)\n\ntheme_set(theme_minimal())\nset.seed(123)",
    "crumbs": [
      "EDA and Pre-Processing",
      "(6) Simple pipeline leakage (scaling) -R"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html#data-and-goal",
    "href": "pima_leakage_pipeline.html#data-and-goal",
    "title": "(6) Simple pipeline leakage (scaling) -R",
    "section": "1. Data and goal",
    "text": "1. Data and goal\nWe will use the PimaIndiansDiabetes2 dataset and focus on a simple regression problem:\n\n\nOutcome: pedigree (diabetes pedigree function - family members with diabetes)\n\n\nPredictors: age, glucose\n\n\nOur goal is to compare two pipelines:\n\nA wrong one, where we scale before splitting (data leakage).\nA correct one, where we scale using training data only.\n\nWe’ll use MAE (Mean Absolute Error) to compare test performance.\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\npima_small &lt;- PimaIndiansDiabetes2 %&gt;%\n  clean_names() %&gt;%\n  select(age, glucose, pedigree) %&gt;%\n  filter(!is.na(pedigree))  # outcome must be present\n\nhead(pima_small)\n\n  age glucose pedigree\n1  50     148    0.627\n2  31      85    0.351\n3  32     183    0.672\n4  21      89    0.167\n5  33     137    2.288\n6  30     116    0.201\n\nsummary(pima_small)\n\n      age           glucose         pedigree     \n Min.   :21.00   Min.   : 44.0   Min.   :0.0780  \n 1st Qu.:24.00   1st Qu.: 99.0   1st Qu.:0.2437  \n Median :29.00   Median :117.0   Median :0.3725  \n Mean   :33.24   Mean   :121.7   Mean   :0.4719  \n 3rd Qu.:41.00   3rd Qu.:141.0   3rd Qu.:0.6262  \n Max.   :81.00   Max.   :199.0   Max.   :2.4200  \n                 NA's   :5",
    "crumbs": [
      "EDA and Pre-Processing",
      "(6) Simple pipeline leakage (scaling) -R"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html#wrong-pipeline-scaling-before-traintest-split-leakage",
    "href": "pima_leakage_pipeline.html#wrong-pipeline-scaling-before-traintest-split-leakage",
    "title": "(6) Simple pipeline leakage (scaling) -R",
    "section": "2. WRONG pipeline: scaling before train–test split (leakage)",
    "text": "2. WRONG pipeline: scaling before train–test split (leakage)\n2.1. Compute global mean and sd (leaky)\n\nmean_all_glucose &lt;- mean(pima_small$glucose, na.rm = TRUE)\nsd_all_glucose   &lt;- sd(pima_small$glucose, na.rm = TRUE)\n\nmean_all_age &lt;- mean(pima_small$age, na.rm = TRUE)\nsd_all_age   &lt;- sd(pima_small$age, na.rm = TRUE)\n\nmean_all_glucose; sd_all_glucose\n\n[1] 121.6868\n\n\n[1] 30.53564\n\nmean_all_age; sd_all_age\n\n[1] 33.24089\n\n\n[1] 11.76023\n\n\n2.2. Scale the full dataset using global statistics\n\npima_small_scaled &lt;- pima_small %&gt;%\n  mutate(\n    glucose_scaled = (glucose - mean_all_glucose) / sd_all_glucose,\n    age_scaled     = (age - mean_all_age) / sd_all_age\n  )\n\nhead(pima_small_scaled)\n\n  age glucose pedigree glucose_scaled  age_scaled\n1  50     148    0.627      0.8617221  1.42506672\n2  31      85    0.351     -1.2014407 -0.19054773\n3  32     183    0.672      2.0079237 -0.10551539\n4  21      89    0.167     -1.0704463 -1.04087112\n5  33     137    2.288      0.5014873 -0.02048305\n6  30     116    0.201     -0.1862336 -0.27558007\n\n\n2.3. Train–test split on already scaled data\n\nset.seed(123)\ndata_split &lt;- initial_split(pima_small_scaled, prop = 0.7)\n\ntrain &lt;- training(data_split)\ntest  &lt;- testing(data_split)\n\nnrow(train); nrow(test)\n\n[1] 537\n\n\n[1] 231\n\n\n2.4. Fit linear regression model and compute MAE\n\nped_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nfit_wrong &lt;- ped_model %&gt;%\n  fit(pedigree ~ age_scaled + glucose_scaled, data = train)\n\nresults_wrong &lt;- predict(fit_wrong, new_data = test) %&gt;%\n  bind_cols(test)\n\nmae_wrong &lt;- mae(\n  data    = results_wrong,\n  truth   = pedigree,\n  estimate = .pred\n)\n\nmae_wrong\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.234",
    "crumbs": [
      "EDA and Pre-Processing",
      "(6) Simple pipeline leakage (scaling) -R"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html#right-pipeline-scaling-using-training-data-only-no-leakage",
    "href": "pima_leakage_pipeline.html#right-pipeline-scaling-using-training-data-only-no-leakage",
    "title": "(6) Simple pipeline leakage (scaling) -R",
    "section": "3. RIGHT pipeline: scaling using training data only (no leakage)",
    "text": "3. RIGHT pipeline: scaling using training data only (no leakage)\n3.1. Train–test split on the original data\n\nset.seed(123)\ndata_split_right &lt;- initial_split(pima_small, prop = 0.8)\n\ntrain &lt;- training(data_split_right)\ntest  &lt;- testing(data_split_right)\n\nnrow(train); nrow(test)\n\n[1] 614\n\n\n[1] 154\n\n\n3.2. Compute training-only mean and sd\n\nmu_tr_glucose &lt;- mean(train$glucose, na.rm = TRUE)\nsd_tr_glucose &lt;- sd(train$glucose, na.rm = TRUE)\n\nmu_tr_age &lt;- mean(train$age, na.rm = TRUE)\nsd_tr_age &lt;- sd(train$age, na.rm = TRUE)\n\nmu_tr_glucose; sd_tr_glucose\n\n[1] 122.2902\n\n\n[1] 30.25447\n\nmu_tr_age; sd_tr_age\n\n[1] 33.1759\n\n\n[1] 11.81636\n\n\n3.3. Scale train and test using training statistics\n\ntrain_scaled &lt;- train %&gt;%\n  mutate(\n    glucose_scaled = (glucose - mu_tr_glucose) / sd_tr_glucose,\n    age_scaled     = (age - mu_tr_age)     / sd_tr_age\n  )\n\ntest_scaled &lt;- test %&gt;%\n  mutate(\n    glucose_scaled = (glucose - mu_tr_glucose) / sd_tr_glucose,\n    age_scaled     = (age - mu_tr_age)     / sd_tr_age\n  )\n\nhead(train_scaled)\n\n    age glucose pedigree glucose_scaled age_scaled\n415  21     138    0.534      0.5192568 -1.0304267\n463  39      74    0.705     -1.5961334  0.4928847\n179  47     143    0.190      0.6845216  1.1699120\n526  21      87    0.444     -1.1664448 -1.0304267\n195  42      85    0.136     -1.2325507  0.7467699\n118  25      78    0.654     -1.4639215 -0.6919131\n\nhead(test_scaled)\n\n   age glucose pedigree glucose_scaled  age_scaled\n1   50     148    0.627      0.8497865  1.42379718\n3   32     183    0.672      2.0066404 -0.09951419\n9   53     197    0.158      2.4693820  1.67768241\n17  31     118    0.551     -0.1418027 -0.18414260\n22  50      99    0.388     -0.7698091  1.42379718\n27  43     147    0.257      0.8167335  0.83139832\n\n\n3.4. Fit linear regression model and compute MAE\n\nped_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nfit_right &lt;- ped_model %&gt;%\n  fit(pedigree ~ age_scaled + glucose_scaled, data = train_scaled)\n\nresults_right &lt;- predict(fit_right, new_data = test_scaled) %&gt;%\n  bind_cols(test_scaled)\n\nmae_right &lt;- mae(\n  data    = results_right,\n  truth   = pedigree,\n  estimate = .pred\n)\n\nmae_right\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.234",
    "crumbs": [
      "EDA and Pre-Processing",
      "(6) Simple pipeline leakage (scaling) -R"
    ]
  },
  {
    "objectID": "pima_leakage_pipeline.html#comparing-mae-leaky-vs-leak-free",
    "href": "pima_leakage_pipeline.html#comparing-mae-leaky-vs-leak-free",
    "title": "(6) Simple pipeline leakage (scaling) -R",
    "section": "4. Comparing MAE: leaky vs leak-free",
    "text": "4. Comparing MAE: leaky vs leak-free\n\ntibble(\n  pipeline = c(\"WRONG: scaled before split (leakage)\",\n               \"RIGHT: scaled using training only\"),\n  MAE = c(mae_wrong$.estimate,\n          mae_right$.estimate)\n)\n\n# A tibble: 2 × 2\n  pipeline                               MAE\n  &lt;chr&gt;                                &lt;dbl&gt;\n1 WRONG: scaled before split (leakage) 0.234\n2 RIGHT: scaled using training only    0.234\n\n\nEven though same results, what you can find is that if data from the test is seen by data in the training, we can end up with an inflated performance. In this case, data distribution is the same, so mean and sd in train and general/all data did not vary much (but they are definitely not the same, can go back now and compare values) so this is good practice.",
    "crumbs": [
      "EDA and Pre-Processing",
      "(6) Simple pipeline leakage (scaling) -R"
    ]
  },
  {
    "objectID": "EDA_Practical1_Final.html",
    "href": "EDA_Practical1_Final.html",
    "title": "(4) Missing Data, Imputation and Dummy Variables - Python",
    "section": "",
    "text": "We are now moving to Python. You will be able to explore missingness here too, as well as explore the concept of imputation and dummy variables.\nThis example illustrates how to apply different preprocessing and feature imputation functions to different subsets of features, using SimpleImputer and KNNImputer. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to impute the numeric as well as categorical features.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn import set_config\nset_config(display='diagram')\n\nYou can download the dataset we will be using through the link:\n\nLongIsland_Heart_Data.csv\n\n\nnp.random.seed(0)\n\n## Load the LongIsland_Heart_Data Set\nheart_df = pd.read_csv('/Users/l.bravo@bham.ac.uk/Library/CloudStorage/OneDrive-UniversityofBirmingham/Desktop/Practical/EDA/LongIsland_Heart_Data.csv') #change to your own directory\n\nheart_df.describe() #similar to summary() in R\n\n              age         sex  ...        thal  diagnosis of heart disease\ncount  180.000000  180.000000  ...  180.000000                  200.000000\nmean    59.472222    0.966667  ...    1.411111                    2.520000\nstd      7.718823    0.180006  ...    0.967568                    1.219441\nmin     35.000000    0.000000  ...    1.000000                    1.000000\n25%     55.000000    1.000000  ...    1.000000                    1.000000\n50%     60.000000    1.000000  ...    1.000000                    2.000000\n75%     64.000000    1.000000  ...    1.000000                    4.000000\nmax     77.000000    1.000000  ...    4.000000                    5.000000\n\n[8 rows x 14 columns]\n\nprint(heart_df)\n\n      age  sex   cp  trestbps  ...  slope   ca  thal  diagnosis of heart disease\n0    63.0  1.0  4.0      10.0  ...    3.0  1.0   1.0                           3\n1    44.0  1.0  4.0       3.0  ...    1.0  NaN   1.0                           1\n2    60.0  1.0  4.0       5.0  ...    4.0  1.0   NaN                           3\n3    55.0  1.0  4.0      11.0  ...    2.0  1.0   1.0                           2\n4    66.0  1.0  3.0      33.0  ...    3.0  1.0   1.0                           1\n..    ...  ...  ...       ...  ...    ...  ...   ...                         ...\n195  54.0  0.0  4.0      41.0  ...    1.0  1.0   1.0                           2\n196  62.0  1.0  1.0       1.0  ...    1.0  1.0   1.0                           1\n197  55.0  1.0  4.0      37.0  ...    1.0  NaN   3.0                           3\n198  58.0  1.0  NaN       1.0  ...    1.0  1.0   1.0                           1\n199  62.0  1.0  2.0      34.0  ...    1.0  1.0   NaN                           2\n\n[200 rows x 14 columns]\n\n\nNow let’s make a smaller dataset with just 4 features:\nNumeric Features:\n\n\nchol: numeric; – serum cholestoral in mg/dl\n\nthalach: numeric – maximum heart rate achieved\n\nCategorical Features:\n\n\nsex: categories encoded as numeric {'1 = male', '2=female'};\n\ncp: ordinal integers {1, 2, 3, 4}. – Value 1: typical angina – Value 2: atypical angina – Value 3: non-anginal pain – Value 4: asymptomatic\n\n\nX_reduced = heart_df.loc[:, ['chol', 'thalach','sex','cp']]\nprint(X_reduced)\n\n     chol  thalach  sex   cp\n0    60.0     12.0  1.0  4.0\n1     NaN      8.0  1.0  4.0\n2    27.0     19.0  1.0  4.0\n3    39.0     25.0  1.0  4.0\n4    22.0     53.0  1.0  3.0\n..    ...      ...  ...  ...\n195  95.0      NaN  0.0  4.0\n196  30.0      1.0  1.0  1.0\n197  33.0      4.0  1.0  4.0\n198   3.0      1.0  1.0  NaN\n199   NaN     47.0  1.0  2.0\n\n[200 rows x 4 columns]",
    "crumbs": [
      "EDA and Pre-Processing",
      "(4) Missing Data, Imputation and Dummy Variables - Python"
    ]
  },
  {
    "objectID": "EDA_Practical1_Final.html#dummy-variables",
    "href": "EDA_Practical1_Final.html#dummy-variables",
    "title": "(4) Missing Data, Imputation and Dummy Variables - Python",
    "section": "Dummy variables",
    "text": "Dummy variables\nAs well as imputation we are interested in other transformations. For example, hot encoding categorical data. For extra insight https://www.kaggle.com/code/marcinrutecki/one-hot-encoding-everything-you-need-to-know\n\nX_reduced_Encoder = heart_df.loc[:, ['chol', 'thalach','sex','cp']]\nX_reduced_Encoder[categorical_features] = X_reduced_Encoder[categorical_features].astype('category')\n\n# Apply OneHotEncoder\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nencoded_categorical = encoder.fit_transform(X_reduced_Encoder[categorical_features])\n\n\nencoded_df = pd.DataFrame(\n    encoded_categorical,\n    columns=encoder.get_feature_names_out(categorical_features)\n)\n\n\n# Combine the encoded features with the original dataset (drop original categorical columns)\nvisualised_df = pd.concat([X_reduced_Encoder.drop(columns=categorical_features), encoded_df], axis=1)\nprint(visualised_df)\n\n     chol  thalach  sex_0.0  sex_1.0  ...  cp_2.0  cp_3.0  cp_4.0  cp_nan\n0    60.0     12.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n1     NaN      8.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n2    27.0     19.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n3    39.0     25.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n4    22.0     53.0      0.0      1.0  ...     0.0     1.0     0.0     0.0\n..    ...      ...      ...      ...  ...     ...     ...     ...     ...\n195  95.0      NaN      1.0      0.0  ...     0.0     0.0     1.0     0.0\n196  30.0      1.0      0.0      1.0  ...     0.0     0.0     0.0     0.0\n197  33.0      4.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n198   3.0      1.0      0.0      1.0  ...     0.0     0.0     0.0     1.0\n199   NaN     47.0      0.0      1.0  ...     1.0     0.0     0.0     0.0\n\n[200 rows x 10 columns]\n\n\nAs you have seen we have gone back to original data so one of the new columns created is nan! To circumvent this apply this to a dataframe AFTER imputation!!!\n\n\n#Impute dataset and then do hot encoding:",
    "crumbs": [
      "EDA and Pre-Processing",
      "(4) Missing Data, Imputation and Dummy Variables - Python"
    ]
  },
  {
    "objectID": "Correlation.html",
    "href": "Correlation.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "EDA and Pre-Processing",
      "(2) Correlation - R"
    ]
  },
  {
    "objectID": "Correlation.html#correlation---r",
    "href": "Correlation.html#correlation---r",
    "title": "",
    "section": "(2) Correlation - R",
    "text": "(2) Correlation - R\nAgain, this practical is also in R. This is an example of the great amount of resources available in open-source programming languages (will also be able to find similar things in Python), where new packages are developed and made available constantly. As key insight, explore, probably what you need is already out there for you to use!\n\nFor you to see the breadth of possibilities out there, here is an example of different ways in which to describe and learn about the correlation matrix of your dataset.\n\n# Install required libraries (if not already installed)\n\n#install.packages(c(\"ggplot2\", \"corrplot\", \"ggcorrplot\", \n#\"PerformanceAnalytics\", \"GGally\", \"psych\", \"corrr\"))\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(PerformanceAnalytics)\nlibrary(GGally)\nlibrary(psych)\nlibrary(corrr)\n\nTo assess the association between two variables, you can produce a scatter plot like below:\n\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- x + rnorm(100, sd = 0.5)\n\nggplot(data = data.frame(x, y), aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Correlation Plot with ggplot2\",\n       x = \"X values\", y = \"Y values\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow we are going to produce the correlation matrix, where in a go, you get insight of all the association happening in your dataset.For this we are going to use the mtcars data (A built-in dataset that contains measurements on 11 different attributes for 32 different cars). More specifically, the chosen variables are:\n\nmpg (Miles per Gallon): Represents the fuel efficiency of the car. Higher values, better efficiency.\ndisp (Displacement): Represents the engine displacement, which is the total volume of all the cylinders in the engine.\nhp (Horsepower): Represents the power output of the car’s engine.\nwt (Weight): Represents the weight of the car.\n\n\ndata &lt;- mtcars[, c(\"mpg\", \"disp\", \"hp\", \"wt\")]\n\nCan you calculate the correlation between them? Try out at least a pair.Remember there are different ways in which to calculate correlation https://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r and you can specify the type in your code cor(x, y, method = c(\"pearson\", \"kendall\", \"spearman\"))\nNow that you know what to expect, lets proceed with the correlation matrices (same thing as above, but all in a go!):\n—–corrplot Correlation Matrix —–\n\ncor_matrix &lt;- cor(data)\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         title = \"Correlation Matrix\", addCoef.col = \"black\")\n\n\n\n\n\n\n\n—–corrplot Correlation Matrix —–\n\nggcorrplot(cor_matrix, \n           method = \"square\", \n           type = \"lower\", \n           lab = TRUE, \n           title = \"mtcars\", \n           lab_size = 3) +\n  theme_minimal()\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggcorrplot package.\n  Please report the issue at &lt;https://github.com/kassambara/ggcorrplot/issues&gt;.\n\n\n\n\n\n\n\n\n—–PerformanceAnalytics Pairwise Correlation Plot —–\n\nchart.Correlation(data, histogram = TRUE, pch = 19)\n\n\n\n\n\n\n\n###—–GGally Pairwise Correlation Plot —–\n\nggpairs(data, \n        title = \"Pairwise Correlation Plot with GGally\",\n        lower = list(continuous = wrap(\"smooth\", method = \"lm\")),\n        upper = list(continuous = wrap(\"cor\", size = 3)))\n\n\n\n\n\n\n\n###—– psych Enhanced Pairwise Correlation Plot —–\n\npairs.panels(data, \n             method = \"pearson\", \n             hist.col = \"lightblue\", \n             density = TRUE, \n             ellipses = TRUE)\n\n\n\n\n\n\n\n###—– Base R Heatmap —–\n\n# ----- 8. Base R Heatmap -----\nheatmap(cor_matrix, symm = TRUE, \n        main = \"Correlation Heatmap\", \n        col = colorRampPalette(c(\"red\", \"white\", \"blue\"))(20))\n\n\n\n\n\n\n\n\n# ----- 10. corrr Tidy Correlation -----\ncor_matrix_tidy &lt;- correlate(data)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\ncor_matrix_tidy %&gt;% \n  fashion() %&gt;% \n  print()\n\n  term  mpg disp   hp   wt\n1  mpg      -.85 -.78 -.87\n2 disp -.85       .79  .89\n3   hp -.78  .79       .66\n4   wt -.87  .89  .66     \n\n# Network-style correlation plot\ncor_matrix_tidy %&gt;% \n  network_plot(min_cor = 0.3)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the corrr package.\n  Please report the issue at &lt;https://github.com/tidymodels/corrr/issues&gt;.\n\n\n\n\n\n\n\n\nWhat are your insights? Do they make sense? Remember:\n\nmpg (Miles per Gallon): Represents the fuel efficiency of the car. Higher values, better efficiency.\ndisp (Displacement): Represents the engine displacement, which is the total volume of all the cylinders in the engine.\nhp (Horsepower): Represents the power output of the car’s engine.\nwt (Weight): Represents the weight of the car.",
    "crumbs": [
      "EDA and Pre-Processing",
      "(2) Correlation - R"
    ]
  },
  {
    "objectID": "HistogramBoxplot.html",
    "href": "HistogramBoxplot.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "EDA and Pre-Processing",
      "(1) Histograms and Boxplots - R"
    ]
  },
  {
    "objectID": "HistogramBoxplot.html#histograms-and-boxplots---r",
    "href": "HistogramBoxplot.html#histograms-and-boxplots---r",
    "title": "",
    "section": "(1) Histograms and Boxplots - R",
    "text": "(1) Histograms and Boxplots - R\nIn this first practical, we will be using R. This is just a training to improve our plotting skills and basic understanding on data distribution.\n\nHistograms show the overall shape and spread of the data distribution and box plots summarise key statistics (median, spread, and potential outliers) and are particularly helpful for comparing distributions across groups.\nSummary Statistics\n\n#Remember, if you do not have these packages installed, install them first with (install.packages())\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(gapminder)\nlibrary(tidyverse)\n\n\n# Example data\ndata &lt;- data.frame(age = c(2,  45, 56, 47, 67, 67, 68, 72, 75, 80, 85,86, 89,125))\nhead(data)\n\n  age\n1   2\n2  45\n3  56\n4  47\n5  67\n6  67\n\n\nWhat is the distribution of the age data? What are the mean, median and standard deviation?\n\nMean &lt;- mean(data$age)\nMedian &lt;- median(data$age)\nSd &lt;- sd(data$age)\n\nOther data distribution descriptors include quantiles, which divide data into equal-sized intervals:\n0% (Minimum) 25% (Q1, First Quartile) 50% (Median, Second Quartile) 75% (Q3, Third Quartile) *100% (Maximum)\n\nQuantile &lt;- quantile(data$age)\nQuantile\n\n    0%    25%    50%    75%   100% \n  2.00  58.75  70.00  83.75 125.00 \n\n\nThe IQR (Interquartile Range) measures the spread of the middle 50% of your data.\nIf you look at the Quantile results above, you can calculate it from \\(IQR=Q3−Q1\\), where Q3 is the value beyond which 75% of your data lies. What is then Q2? The value beyond which 25% of your data lies.\n\nIQR &lt;- IQR(data$age)\nIQR\n\n[1] 25\n\n\nCan do it in a single go:\n\nsummary(data$age) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00   58.75   70.00   68.86   83.75  125.00 \n\n\nAs we saw, lots of different datasets can end up with these summary statistics, so lets plot it to get real insight about what is happening:",
    "crumbs": [
      "EDA and Pre-Processing",
      "(1) Histograms and Boxplots - R"
    ]
  },
  {
    "objectID": "HistogramBoxplot.html#histogram-in-r",
    "href": "HistogramBoxplot.html#histogram-in-r",
    "title": "",
    "section": "Histogram in R",
    "text": "Histogram in R\nA histogram is a way to visualise the frequency distribution of a dataset. It groups data into bins and shows how many data points fall into each bin\n\nggplot(data, aes(x = age)) + \n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nYou can modify the binwidth parameter, what is happening here?\n\nggplot(data, aes(x = age)) + \n  geom_histogram(binwidth = 30) \n\n\n\n\n\n\n\nNow lets go back t original and add the mean and median\n\nggplot(data, aes(x = age)) + \n  geom_histogram() +\n  geom_vline(xintercept = Mean, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = Median, color = \"blue\", linetype = \"dashed\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nIf do not remember how ggplot works , here is a cheatsheet: https://rstudio.github.io/cheatsheets/html/data-visualization.html\nIn summary, a histogram helps you understand the shape of your data distribution (e.g., normal, skewed) and peaks indicate the most frequent values.",
    "crumbs": [
      "EDA and Pre-Processing",
      "(1) Histograms and Boxplots - R"
    ]
  },
  {
    "objectID": "HistogramBoxplot.html#box-plot-in-r",
    "href": "HistogramBoxplot.html#box-plot-in-r",
    "title": "",
    "section": "Box Plot in R",
    "text": "Box Plot in R\nA box plot (or box-and-whisker plot) summarises the distribution of a dataset. It highlights:\n\nMedian (middle line in the box).\nQuantiles (the box spans from the 25th percentile to the 75th percentile)\nIQR (Interquartile Range): The difference between the 75th and 25th percentiles - (IQR range calculated above)\nWhiskers: Extend to data points within 1.5*IQR (above or beyond = outliers)\n\n\nggplot(data, aes(x = age)) + \n  geom_boxplot() + \n  coord_flip() #have you checked what this does? Take it out and see\n\n\n\n\n\n\n\nCan you see summary statistics mapped here?\nLets plot the mean (as it is a value that is not reflected in a boxplot!)\n\nggplot(data, aes(x = age)) + \n  geom_boxplot() + \n  coord_flip() + \n   geom_vline(xintercept = Mean, color = \"blue\", linetype = \"dashed\", size = 1) \n\n\n\n\n\n\n\nFor an extra challenge, add another geom layer, i.e geom_jitter. What does it do? How is it different from geom_point?\nNow, lets go a step further. If we want to understand how the data varies by groups, we can use facets:\n\nggplot(data, aes(x = age, y = \"age\" )) + \n  geom_boxplot() + \n  geom_jitter() +\n  coord_flip() + \n  geom_vline(xintercept = Mean, color = \"blue\", linetype = \"dashed\", size = 1) \n\n\n\n\n\n\n\n\n# Sample data\nset.seed(123)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"A\", \"B\", \"C\"), each = 200),\n  Value = c(rnorm(200, mean = 5, sd = 1), \n            rnorm(200, mean = 6, sd = 1.2), \n            rnorm(200, mean = 7, sd = 1.5))\n)\n\n\n# Combined Plot\np1 &lt;- ggplot(data, aes(x = Group, y = Value, fill = Group)) +\n  # Violin + Boxplot + Jitter\n  geom_boxplot(width = 0.2, outlier.color = \"red\", outlier.size = 2, colour = \"black\") +\n  geom_jitter(position = position_jitter(0.2), color = \"black\", alpha = 0.4) + #scatter plot but in random positions, check geom_point() and see\n  labs(title = \"Boxplot and Violin Plot with Jitter\", x = \"Groups\", y = \"Values\") +\n  theme_minimal()\n\n\np2 &lt;- ggplot(data, aes(x = Value, fill = Group)) +\n  # Histogram\n  geom_histogram(alpha = 0.6, position = \"identity\", binwidth = 0.3, colour = \"black\") +\n  labs(title = \"Histogram of Values by Group\", x = \"Values\", y = \"Count\") +\n  theme_minimal()\n\n\n# Display both plots side by side (thanks to having patchwork installed)\n\np1 + p2\n\n\n\n\n\n\n\n\n# Display both plots up down\n\np1 / p2\n\n\n\n\n\n\n\nLets use this in another dataset (gapminder dataset that we have access to, thanks to the laoded package gapminder)\n\n# Box plot with facets by year\nggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +  # Create facets for each year\n  labs(title = \"Life Expectancy Across Continents by Year\",\n       x = \"Continent\",\n       y = \"Life Expectancy\") +\n  theme_minimal() + #just to make it nice\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels - just to make it nice\n\n\n\n\n\n\n\nWhat is happening here? View (gapminder) dataset. If we focus on 1952 year, extract the necessary values to create the boxplot for Africa (use data wrangling skills through tidyverse package).\n\nSubgroup &lt;- gapminder %&gt;% \n  filter(year == \"1952\") %&gt;%\n  filter(continent == \"Africa\")\n\nMedian &lt;- median(Subgroup$lifeExp)\n\nMedian\n\n[1] 38.833\n\n\nDoes it match the plot?",
    "crumbs": [
      "EDA and Pre-Processing",
      "(1) Histograms and Boxplots - R"
    ]
  },
  {
    "objectID": "MissingPackages.html",
    "href": "MissingPackages.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "EDA and Pre-Processing",
      "(3) Missing Pipelines - R"
    ]
  },
  {
    "objectID": "MissingPackages.html#missing-pipelines---r",
    "href": "MissingPackages.html#missing-pipelines---r",
    "title": "",
    "section": "(3) Missing Pipelines - R",
    "text": "(3) Missing Pipelines - R\nAs with the correlation matrices before, there are really good packages to help you understand the missingness structure of your data. This is my recommendation:\n\n#Remember, if you do not have these packages installed, install them first with (install.packages())\n\nlibrary(naniar)\nlibrary(finalfit)\nlibrary(ggplot2)\nlibrary(mice)\n\nHere I will be using the airquality dataset (available as default in all R) but everything would be ready for you to introduce your own dataset in a future.\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nWe can already see with a simple summary, the ammount of missings we have per column. But what is their underlying structure?\n\nYourDataset &lt;- airquality\n\nPlot 1\n\nll &lt;- data.frame(is.na(YourDataset))\n\nOpen up this ll object. What are you seeing here? is.na() detects all values coded as NA and gives a logical output, TRUE if truly a missing value and FALSE if not\n\nView(ll)\n\n\ncols &lt;- sapply(ll, is.logical)\nll[, cols] &lt;- lapply(ll[, cols], as.numeric)\n\nMiss1 &lt;- UpSetR::upset(ll,\n                       nsets = 20, number.angles = 10, point.size = 3.5, line.size = 2,\n                       mainbar.y.label = \"Missing Values\", sets.x.label = \"Total Number Missing Values\",\n                       text.scale = c(2.3, 2.3, 2, 2, 2, 1.75), order.by = \"freq\", sets.bar.color = \"red3\"\n)\n\n\nMiss1\n\n\n\n\n\n\n\nHere we can see common patterns! If missing values were to happen at the same time in different variables, we would see it clearly. Lets store this figure into our own directories so we can then use it for our final research output!\n\npdf(\"MissingVal1.pdf\", 10, 20)\nprint(Miss1)\ndev.off()\n\nquartz_off_screen \n                2",
    "crumbs": [
      "EDA and Pre-Processing",
      "(3) Missing Pipelines - R"
    ]
  },
  {
    "objectID": "EDA_Practical3.html",
    "href": "EDA_Practical3.html",
    "title": "(5) Data Transformation - Python",
    "section": "",
    "text": "We are now continuing with Python and pre-processing functions. In this case, we will be exploring feature scaling and transformation.",
    "crumbs": [
      "EDA and Pre-Processing",
      "(5) Data Transformation - Python"
    ]
  },
  {
    "objectID": "EDA_Practical3.html#question-1",
    "href": "EDA_Practical3.html#question-1",
    "title": "(5) Data Transformation - Python",
    "section": "Question 1:",
    "text": "Question 1:\n1.1 Load the Bupa Data set and apply a new transformations from &lt;https://scikit-learn.org/stable/modules/preprocessing.html&gt; to a feature of your choice\n1.2 Remove the Label column (\"selector\") from the data set.\n1.3 Visualize the data before and after the feature transformation using boxplots. \nYou can download the dataset through the link:\n\nbupa.csv\n\n\n#df_bupa = pd.read_csv('bupa.csv')\n#df_bupa = df_bupa.dropna()\n\n\n#print(df_bupa)",
    "crumbs": [
      "EDA and Pre-Processing",
      "(5) Data Transformation - Python"
    ]
  },
  {
    "objectID": "pima_imputation_leakage.html",
    "href": "pima_imputation_leakage.html",
    "title": "(7) Simple pipeline leakage (imputation) -R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\nlibrary(tidymodels)\n\ntheme_set(theme_minimal())\nset.seed(123)",
    "crumbs": [
      "EDA and Pre-Processing",
      "(7) Simple pipeline leakage (imputation) -R"
    ]
  },
  {
    "objectID": "pima_imputation_leakage.html#imputation-leakage-with-insulin-no-scaling",
    "href": "pima_imputation_leakage.html#imputation-leakage-with-insulin-no-scaling",
    "title": "(7) Simple pipeline leakage (imputation) -R",
    "section": "Imputation leakage with insulin (no scaling)",
    "text": "Imputation leakage with insulin (no scaling)\nFollowing the scaling concept, if we are learning anything from the data, this should strictly happen in the training data. Therefore, when imputing we have to be aware of data leakage too. As an example, we now look at mean imputation of insulin:\n\n\nOutcome: pedigree\n\n\nPredictors: age, insulin_imp\n\nWe compare:\n\n❌ WRONG: mean for imputation computed using all data (leakage)\n✅ RIGHT: mean for imputation computed using training data only\n\n\n\n\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\npima_imp &lt;- PimaIndiansDiabetes2 %&gt;%\n  clean_names() %&gt;%\n  select(age, insulin, pedigree) %&gt;%\n  filter(!is.na(pedigree))  # outcome must be present\n\nsummary(pima_imp)\n\n      age           insulin          pedigree     \n Min.   :21.00   Min.   : 14.00   Min.   :0.0780  \n 1st Qu.:24.00   1st Qu.: 76.25   1st Qu.:0.2437  \n Median :29.00   Median :125.00   Median :0.3725  \n Mean   :33.24   Mean   :155.55   Mean   :0.4719  \n 3rd Qu.:41.00   3rd Qu.:190.00   3rd Qu.:0.6262  \n Max.   :81.00   Max.   :846.00   Max.   :2.4200  \n                 NA's   :374                      \n\n\n1. WRONG pipeline: impute using global mean (leakage)\nHere we:\n\nCompute the global mean insulin using all rows.\nImpute missing insulin values with this global mean.\nThen split into train/test.\nFit pedigree ~ age + insulin_imp and evaluate MAE.\n\n\n# 1. Global mean (leaky: uses all data)\nmean_all_insulin &lt;- mean(pima_imp$insulin, na.rm = TRUE)\nmean_all_insulin\n\n[1] 155.5482\n\n\n\n# 2. Impute WHOLE dataset with global mean (before splitting)\npima_imp_wrong &lt;- pima_imp %&gt;%\n  mutate(\n    insulin_imp = ifelse(is.na(insulin), mean_all_insulin, insulin)\n  )\n\nsummary(pima_imp_wrong$insulin_imp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   14.0   121.5   155.5   155.5   155.5   846.0 \n\n\n\n# 3. Train–test split AFTER imputation (still leaky)\nset.seed(123)\nsplit_wrong &lt;- initial_split(pima_imp_wrong, prop = 0.7)\n\ntrain_wrong &lt;- training(split_wrong)\ntest_wrong  &lt;- testing(split_wrong)\n\nnrow(train_wrong); nrow(test_wrong)\n\n[1] 537\n\n\n[1] 231\n\n\n\n# 4. Fit linear regression and compute MAE (WRONG)\nped_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nfit_wrong_imp &lt;- ped_model %&gt;%\n  fit(pedigree ~ age + insulin_imp, data = train_wrong)\n\ntidy(fit_wrong_imp)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.369     0.0487       7.57  1.60e-13\n2 age         0.00115   0.00126      0.912 3.62e- 1\n3 insulin_imp 0.000467  0.000172     2.72  6.75e- 3\n\n\n\nresults_wrong_imp &lt;- predict(fit_wrong_imp, new_data = test_wrong) %&gt;%\n  bind_cols(test_wrong)\n\nmae_wrong_imp &lt;- mae(\n  data     = results_wrong_imp,\n  truth    = pedigree,\n  estimate = .pred\n)\n\nmae_wrong_imp\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.235\n\n\n2. RIGHT pipeline: impute using training mean only (no leakage)\nNow we:\n\nSplit the original (unimputed) data into train/test.\nCompute mean insulin using training data only.\nImpute train and test using this training mean.\nFit the same model and compute MAE.\n\n\n# 1. Train–test split on RAW data\nset.seed(123)\nsplit_right &lt;- initial_split(pima_imp, prop = 0.7)\n\ntrain_right &lt;- training(split_right)\ntest_right  &lt;- testing(split_right)\n\nnrow(train_right); nrow(test_right)\n\n[1] 537\n\n\n[1] 231\n\n\n\n# 2. Training-only mean for insulin (correct)\nmean_tr_insulin &lt;- mean(train_right$insulin, na.rm = TRUE)\nmean_tr_insulin\n\n[1] 155.3705\n\n\n\n# 3. Impute train and test using TRAIN mean\ntrain_right_imp &lt;- train_right %&gt;%\n  mutate(\n    insulin_imp = ifelse(is.na(insulin), mean_tr_insulin, insulin)\n  )\n\ntest_right_imp &lt;- test_right %&gt;%\n  mutate(\n    insulin_imp = ifelse(is.na(insulin), mean_tr_insulin, insulin)\n  )\n\nsummary(train_right_imp$insulin_imp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   14.0   120.0   155.4   155.4   155.4   846.0 \n\nsummary(test_right_imp$insulin_imp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   15.0   125.5   155.4   155.7   155.4   600.0 \n\n\n\n# 4. Fit linear regression and compute MAE (RIGHT)\nfit_right_imp &lt;- ped_model %&gt;%\n  fit(pedigree ~ age + insulin_imp, data = train_right_imp)\n\ntidy(fit_right_imp)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.369     0.0487       7.57  1.60e-13\n2 age         0.00115   0.00126      0.912 3.62e- 1\n3 insulin_imp 0.000468  0.000172     2.72  6.68e- 3\n\n\n\nresults_right_imp &lt;- predict(fit_right_imp, new_data = test_right_imp) %&gt;%\n  bind_cols(test_right_imp)\n\nmae_right_imp &lt;- mae(\n  data     = results_right_imp,\n  truth    = pedigree,\n  estimate = .pred\n)\n\nmae_right_imp\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.235\n\n\n3. Compare MAE: imputation with vs without leakage\n\ntibble(\n  pipeline = c(\"WRONG: impute with global mean (leakage)\",\n               \"RIGHT: impute with training mean only\"),\n  MAE = c(mae_wrong_imp$.estimate,\n          mae_right_imp$.estimate)\n)\n\n# A tibble: 2 × 2\n  pipeline                                   MAE\n  &lt;chr&gt;                                    &lt;dbl&gt;\n1 WRONG: impute with global mean (leakage) 0.235\n2 RIGHT: impute with training mean only    0.235",
    "crumbs": [
      "EDA and Pre-Processing",
      "(7) Simple pipeline leakage (imputation) -R"
    ]
  },
  {
    "objectID": "pima_preprocessing_sklearn_python.html",
    "href": "pima_preprocessing_sklearn_python.html",
    "title": "(9) Complete pipeline leakage - Python",
    "section": "",
    "text": "This document shows how to perform mean imputation and standardisation correctly in Python using scikit-learn, first manually and then using a Pipeline. This is a new concept that we will explore in detail in the next classes, but please check this out for things you do not understand https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\nWe load the same Pima Indians database and select the following columns:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nurl=\"https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\"\ndf = pd.read_csv(url)\n\ndf.columns = df.columns.str.lower()\ndf = df.rename(columns={\"diabetespedigreefunction\": \"pedigree\"})\n\n\nfeatures = [\"age\", \"glucose\", \"insulin\"]\ndf[features] = df[features].replace(0, np.nan)\nX = df[features]\ny = df[\"pedigree\"]",
    "crumbs": [
      "EDA and Pre-Processing",
      "(9) Complete pipeline leakage - Python"
    ]
  },
  {
    "objectID": "pima_preprocessing_sklearn_python.html#manual-preprocessing-impute-scale-no-pipeline",
    "href": "pima_preprocessing_sklearn_python.html#manual-preprocessing-impute-scale-no-pipeline",
    "title": "(9) Complete pipeline leakage - Python",
    "section": "2. Manual preprocessing: impute + scale (no Pipeline)",
    "text": "2. Manual preprocessing: impute + scale (no Pipeline)\nHere we explicitly:\n\nSplit into train and test.\nFit SimpleImputer on the training set and transform train & test.\nFit StandardScaler on the training set and transform train & test.\nFit a LinearRegression model and compute MAE on the test set.\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=123\n)\n\n# 1. Mean imputation (TRAIN only)\nimputer = SimpleImputer(strategy=\"mean\")\nX_train_imp = imputer.fit_transform(X_train)   # learns means from TRAIN\nX_test_imp  = imputer.transform(X_test)        # applies TRAIN means\n\n# 2. Standardisation (TRAIN only)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_imp)  # learns μ, σ from TRAIN\nX_test_scaled  = scaler.transform(X_test_imp)       # uses TRAIN μ, σ\n\n\n# 3. Linear regression on preprocessed data\nmodel = LinearRegression()\nmodel.fit(X_train_scaled, y_train)\n\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLinearRegression\n\n?Documentation for LinearRegressioniFitted\n\n        \n            Parameters\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n    \n\n\n\n\ny_pred = model.predict(X_test_scaled)\nmae_manual = mean_absolute_error(y_test, y_pred)\nmae_manual\n\n0.24771496683579874",
    "crumbs": [
      "EDA and Pre-Processing",
      "(9) Complete pipeline leakage - Python"
    ]
  },
  {
    "objectID": "pima_preprocessing_sklearn_python.html#using-a-scikit-learn-pipeline-cleanest-version",
    "href": "pima_preprocessing_sklearn_python.html#using-a-scikit-learn-pipeline-cleanest-version",
    "title": "(9) Complete pipeline leakage - Python",
    "section": "3. Using a scikit-learn Pipeline (cleanest version)",
    "text": "3. Using a scikit-learn Pipeline (cleanest version)\nPipelines package preprocessing + model into a single object:\n\nImputation and scaling are still fit on the training data only.\nThe same transformations are automatically applied to any new data.\n\n\nnumeric_features = [\"age\", \"glucose\", \"insulin\"]\n\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n        (\"scaler\", StandardScaler()),\n    ]\n)\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features)\n    ]\n)\n\nmodel = LinearRegression()\n\npipe = Pipeline(\n    steps=[\n        (\"preprocess\", preprocess),\n        (\"model\", model),\n    ]\n)\n\nX = df[numeric_features]\ny = df[\"pedigree\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=123\n)\n\npipe.fit(X_train, y_train)\n\n\n\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'glucose',\n                                                   'insulin'])])),\n                ('model', LinearRegression())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nPipeline\n\n?Documentation for PipelineiFitted\n\n        \n            Parameters\n\n\nsteps \n[('preprocess', ...), ('model', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n    \n\n\n\n\npreprocess: ColumnTransformer\n?Documentation for preprocess: ColumnTransformer\n        \n            Parameters\n\n\ntransformers \n[('num', ...)]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n    \n\n\n\nnum['age', 'glucose', 'insulin']\n\n\n\nSimpleImputer\n?Documentation for SimpleImputer\n        \n            Parameters\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'mean'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n    \n\n\nStandardScaler\n?Documentation for StandardScaler\n        \n            Parameters\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n    \n\n\n\n\n\nLinearRegression\n?Documentation for LinearRegression\n        \n            Parameters\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n    \n\n\n\n\n\n\ny_pred_pipe = pipe.predict(X_test)\n\nmae_pipe = mean_absolute_error(y_test, y_pred_pipe)\nmae_pipe\n\n0.24771496683579874\n\n\n\nprint(\"MAE (manual preprocessing):\", mae_manual)\n\nMAE (manual preprocessing): 0.24771496683579874\n\nprint(\"MAE (Pipeline):           \", mae_pipe)\n\nMAE (Pipeline):            0.24771496683579874\n\n\nIn practice, you should prefer the Pipeline approach:\n\nlower risk of mistakes,\neasy extension to cross-validation and more complex models.",
    "crumbs": [
      "EDA and Pre-Processing",
      "(9) Complete pipeline leakage - Python"
    ]
  }
]