{
  "hash": "e97fd16a4645882e4ff5854571278391",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Data Transformation - Python'\nformat: html\nexecute:\n  engine: knitr\n---\n\nWe are now continuing with Python and pre-processing functions. In this case, we will be exploring feature scaling and transformation.\n\n### Data Set Used :\n\nBreast Cancer Wisconsin (Diagnostic) Data Set\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: \\[K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34\\].\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Load the dataset\ndata = datasets.load_breast_cancer() #already included in libraries downloaded\ndf = pd.DataFrame(data.data, columns=data.feature_names) #pick just 5/30 features\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Let's look at the first few rows of the dataframe\nprint(df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0        17.99         10.38  ...          0.4601                  0.11890\n1        20.57         17.77  ...          0.2750                  0.08902\n2        19.69         21.25  ...          0.3613                  0.08758\n3        11.42         20.38  ...          0.6638                  0.17300\n4        20.29         14.34  ...          0.2364                  0.07678\n\n[5 rows x 30 columns]\n```\n\n\n:::\n:::\n\n\nLet's plot the data before transformation\n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df)\nplt.title(\"Non-normalised data\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\nIt is very difficult to see anything right? This is because we have most data around 0 but then a very long tail to the right!\n\n#### Strategy 1 : Min-Max scaling\n\nMin-Max scaling: This transformation scales and translates each feature individually such that it is in the range \\[0, 1\\]. The transformed data is stored in df_minmax.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#### Strategy 1 : Min-Max scaling\nscaler = preprocessing.MinMaxScaler(feature_range=(0,100)) #what happens if you change the 100? \ndf_minmax = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)\n#print(\"\\nAfter Min-Max Scaling:\\n\", df_minmax.head())\n\n\n# Let's plot the data after min-max scaling\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_minmax)\nplt.title(\"After Min-Max Scaling\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-6-3.png){width=960}\n:::\n:::\n\n\n### Log transformation:\n\nThis transformation applies the natural logarithm to each value in the DataFrame. This is often used when the data is highly skewed, as it can help to make the data more \"normal\" (i.e., more closely approximate a normal distribution). The transformed data is stored in df_log.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Log Transformation\ndf_log = df.apply(np.log)\ndf_log = df_log.replace([np.inf, -np.inf], np.nan)\ndf_log = df_log.dropna()\nprint(\"\\nAfter Log Transformation:\\n\", df_log.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAfter Log Transformation:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0     2.889816      2.339881  ...       -0.776311                -2.129472\n1     3.023834      2.877512  ...       -1.290984                -2.418894\n2     2.980111      3.056357  ...       -1.018047                -2.435203\n3     2.435366      3.014554  ...       -0.409774                -1.754464\n4     3.010128      2.663053  ...       -1.442230                -2.566811\n\n[5 rows x 30 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_log)\nplt.title(\"After Log Transformation\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-7-5.png){width=960}\n:::\n:::\n\n\n### Z-score normalization:\n\nAlso known as standardization, this transformation scales and translates each feature so that it has a mean of 0 and a standard deviation of 1. The transformed data is stored in df_zscore.\n\nZ-score normalization is a type of data standardization where we convert all features in our dataset to have a mean (µ) of 0 and a standard deviation (σ) of 1. The purpose of this transformation is to remove the scale effect of measurements.\n\nThe formula for Z-score normalization is:\n\nZ = (X - µ) / σ\n\nwhere:\n\nZ is the standardized (Z-score normalized) value, X is the original value, µ is the mean of the feature, σ is the standard deviation of the feature. Why do we do this? In machine learning, many algorithms (like K-nearest neighbors, Neural Networks, and others) perform better when their input features are roughly on the same scale and centered around zero. If one feature has a range of -1 to 1, while another feature has a range of -1000 to 1000, the second feature will completely dominate when these features are combined, even though the first feature might be just as important.\n\nAfter Z-score normalization, every feature in the dataset will have a mean of 0 and a standard deviation of 1, putting them all on roughly the same scale. The data values in each column now represent how many standard deviations the original value was from the mean of that column. This makes it easier to compare different features, and helps many machine learning algorithms perform better.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Z-score normalization\nscaler = preprocessing.StandardScaler()\ndf_zscore = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)\nprint(\"\\nAfter Z-score Normalization:\\n\", df_zscore.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAfter Z-score Normalization:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0     1.097064     -2.073335  ...        2.750622                 1.937015\n1     1.829821     -0.353632  ...       -0.243890                 0.281190\n2     1.579888      0.456187  ...        1.152255                 0.201391\n3    -0.768909      0.253732  ...        6.046041                 4.935010\n4     1.750297     -1.151816  ...       -0.868353                -0.397100\n\n[5 rows x 30 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Let's plot the data after Z-score normalization\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_zscore)\nplt.title(\"After Z-score Normalization\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-8-7.png){width=960}\n:::\n:::\n\n\n\n#### Your Task\n\n## Question 1:\n\n```         \n1.1 Load the Bupa Data set and apply a new transformations from <https://scikit-learn.org/stable/modules/preprocessing.html> to a feature of your choice\n1.2 Remove the Label column (\"selector\") from the data set.\n1.3 Visualize the data before and after the feature transformation using boxplots. \n```\n\nYou can download the dataset through the link:\n\n-   [bupa.csv](bupa.csv)\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#df_bupa = pd.read_csv('bupa.csv')\n#df_bupa = df_bupa.dropna()\n\n\n#print(df_bupa)\n\n```\n:::\n\n\n",
    "supporting": [
      "EDA_Practical3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}