{
  "hash": "241be495fe99a8a9bc9b05b4c6443493",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Data Transformation - Python'\nformat: html\nexecute:\n  engine: knitr\n---\n\n### Welcome to the \"Data Transformation\" practical session.\n\nWe are using both **Pandas** (data loading, processing, transformation and manipulation) and **Scikit-learn** (example data source, ML and statistical analysis)\n\n## Data Scaling and Normalization\n\n-   Different Types of Normalization.\n\n### Data Set Used :\n\nBreast Cancer Wisconsin (Diagnostic) Data Set\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: \\[K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34\\].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n\n# one-off setup (if you haven't done it yet)\n# install_miniconda()\n\n##conda_create(\n##  envname = \"hds-python\",\n##  python_version = \"3.11\",\n##  packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\")\n##)\n\nuse_condaenv(\"hds-python\", required = TRUE)\n#py_config()\n\n#conda_install(\"hds-python\", c(\"jupyter\", \"plotly\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Load the dataset\ndata = datasets.load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Let's look at the first few rows of the dataframe\nprint(df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0        17.99         10.38  ...          0.4601                  0.11890\n1        20.57         17.77  ...          0.2750                  0.08902\n2        19.69         21.25  ...          0.3613                  0.08758\n3        11.42         20.38  ...          0.6638                  0.17300\n4        20.29         14.34  ...          0.2364                  0.07678\n\n[5 rows x 30 columns]\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Let's plot the data before transformation\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df)\nplt.title(\"Non-normalised data\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n#### Strategy 1 : Min-Max scaling\n\nMin-Max scaling: This transformation scales and translates each feature individually such that it is in the range \\[0, 1\\]. The transformed data is stored in df_minmax.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#### Strategy 1 : Min-Max scaling\nscaler = preprocessing.MinMaxScaler(feature_range=(0,100))\ndf_minmax = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)\n#print(\"\\nAfter Min-Max Scaling:\\n\", df_minmax.head())\n\n\n# Let's plot the data after min-max scaling\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_minmax)\nplt.title(\"After Min-Max Scaling\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-6-3.png){width=960}\n:::\n:::\n\n\n### Log transformation:\n\nThis transformation applies the natural logarithm to each value in the DataFrame. This is often used when the data is highly skewed, as it can help to make the data more \"normal\" (i.e., more closely approximate a normal distribution). The transformed data is stored in df_log.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Log Transformation\ndf_log = df.apply(np.log)\ndf_log = df_log.replace([np.inf, -np.inf], np.nan)\ndf_log = df_log.dropna()\nprint(\"\\nAfter Log Transformation:\\n\", df_log.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAfter Log Transformation:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0     2.889816      2.339881  ...       -0.776311                -2.129472\n1     3.023834      2.877512  ...       -1.290984                -2.418894\n2     2.980111      3.056357  ...       -1.018047                -2.435203\n3     2.435366      3.014554  ...       -0.409774                -1.754464\n4     3.010128      2.663053  ...       -1.442230                -2.566811\n\n[5 rows x 30 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Let's plot the data after min-max scaling\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_log)\nplt.title(\"After Log Transformation\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-7-5.png){width=960}\n:::\n:::\n\n\n### Z-score normalization:\n\nAlso known as standardization, this transformation scales and translates each feature so that it has a mean of 0 and a standard deviation of 1. The transformed data is stored in df_zscore.\n\nZ-score normalization is a type of data standardization where we convert all features in our dataset to have a mean (µ) of 0 and a standard deviation (σ) of 1. The purpose of this transformation is to remove the scale effect of measurements.\n\nThe formula for Z-score normalization is:\n\nZ = (X - µ) / σ\n\nwhere:\n\nZ is the standardized (Z-score normalized) value, X is the original value, µ is the mean of the feature, σ is the standard deviation of the feature. Why do we do this? In machine learning, many algorithms (like K-nearest neighbors, Neural Networks, and others) perform better when their input features are roughly on the same scale and centered around zero. If one feature has a range of -1 to 1, while another feature has a range of -1000 to 1000, the second feature will completely dominate when these features are combined, even though the first feature might be just as important.\n\nAfter Z-score normalization, every feature in the dataset will have a mean of 0 and a standard deviation of 1, putting them all on roughly the same scale. The data values in each column now represent how many standard deviations the original value was from the mean of that column. This makes it easier to compare different features, and helps many machine learning algorithms perform better.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Z-score normalization\nscaler = preprocessing.StandardScaler()\ndf_zscore = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)\nprint(\"\\nAfter Z-score Normalization:\\n\", df_zscore.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAfter Z-score Normalization:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0     1.097064     -2.073335  ...        2.750622                 1.937015\n1     1.829821     -0.353632  ...       -0.243890                 0.281190\n2     1.579888      0.456187  ...        1.152255                 0.201391\n3    -0.768909      0.253732  ...        6.046041                 4.935010\n4     1.750297     -1.151816  ...       -0.868353                -0.397100\n\n[5 rows x 30 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Let's plot the data after Z-score normalization\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_zscore)\nplt.title(\"After Z-score Normalization\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-8-7.png){width=960}\n:::\n:::\n\n\n### Power transformation (Cube):\n\nThis transformation raises each value in the DataFrame to the power of 3. It can be used to increase the skewness in the data.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Power transformation (cube)\ndf_power = df.apply(lambda x: x**3)\nprint(\"\\nAfter Power Transformation:\\n\", df_power.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAfter Power Transformation:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0  5822.285399   1118.386872  ...        0.097399                 0.001681\n1  8703.679193   5611.284433  ...        0.020797                 0.000705\n2  7633.736209   9595.703125  ...        0.047163                 0.000672\n3  1489.355288   8464.718872  ...        0.292490                 0.005178\n4  8353.070389   2948.814504  ...        0.013211                 0.000453\n\n[5 rows x 30 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Let's plot the data after power transformation\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_power)\nplt.title(\"After Power Transformation\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-9-9.png){width=960}\n:::\n:::\n\n\n### Quantile transformation:\n\nQuantile transformation, also known as quantile normalization, is a technique for making two distributions identical in statistical properties. To achieve this, it maps the values from the input distribution to a desired output distribution, such as a uniform or a normal distribution.\n\nThe process can be summarized as follows:\n\nFor each feature, sort the values in ascending order.\n\nThe smallest value is replaced with the smallest value from the desired distribution, the second smallest with the second smallest, and so on.\n\nThis way, the transformed data will have the same distribution as the desired output distribution, while preserving the rank of the original data.\n\nOne of the main uses of quantile transformation is to reduce the impact of outliers. Because it's based on the rank of the data, not the actual values, extreme values will be closer to the rest of the data after transformation.\n\nAnother use is to make non-linear data more suitable for linear models. If a feature has a distribution that's skewed or has a heavy tail, quantile transformation can help make it more \"normal\", so that linear models can make better predictions.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Quantile transformation\nscaler = preprocessing.QuantileTransformer(output_distribution='normal')\ndf_quantile = pd.DataFrame(scaler.fit_transform(df), columns=data.feature_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n/Users/l.bravo@bham.ac.uk/Library/r-miniconda-arm64/envs/hds-python/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2846: UserWarning: n_quantiles (1000) is greater than the total number of samples (569). n_quantiles is set to n_samples.\n  warnings.warn(\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nAfter Quantile Transformation:\\n\", df_quantile.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAfter Quantile Transformation:\n    mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n0     0.982803     -2.918152  ...        1.966025                 1.618134\n1     1.634697     -0.277117  ...       -0.137235                 0.526427\n2     1.352998      0.513796  ...        1.304975                 0.451806\n3    -0.820429      0.356013  ...        5.199338                 2.918152\n4     1.512992     -1.221674  ...       -1.004493                -0.263385\n\n[5 rows x 30 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Let's plot the data after quantile transformation\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=df_quantile)\nplt.title(\"After Quantile Transformation\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical3_files/figure-html/unnamed-chunk-10-11.png){width=960}\n:::\n:::\n\n\n#### Your Task\n\n## Question 1:\n\n```         \n1.1 Load the Bupa Data set and apply Quantile Normalization on each feature.\n1.2 Remove the Label column (\"selector\") from the data set.\n1.3 Visualize the data before and after normalization using boxplot. \n```\n\nYou can download the dataset through the link:\n\n-   [bupa.csv](bupa.csv)\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#df_bupa = pd.read_csv('bupa.csv')\n#df_bupa = df_bupa.dropna()\n\n\n#print(df_bupa)\n\n#print(\"\\Before Quantile Transformation:\\n\")\n# Let's plot the data after quantile transformation\n#plt.figure(figsize=(10, 6))\n#sns.boxplot(data=df_bupa)\n#plt.title(\"After Quantile Transformation\")\n#plt.show()\n\n## ------------------- Complete This ------------------------- ##\n## scaler = preprocessing. ... \n## df_bupa_quantile = pd.DataFrame(scaler.fit_transform( ... ), columns =  ... )\n\n#print(\"\\nAfter Quantile Transformation:\\n\")\n## Let's plot the data after quantile transformation\n#plt.figure(figsize=(10, 6))\n#sns.boxplot(data=df_bupa_quantile)\n#plt.title(\"After Quantile Transformation\")\n#plt.show()\n```\n:::\n\n",
    "supporting": [
      "EDA_Practical3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}