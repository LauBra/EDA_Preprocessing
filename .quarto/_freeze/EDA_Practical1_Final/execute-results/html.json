{
  "hash": "090aa609ad3fd3fbfaa8d9a223c1bdac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Missing Data, Imputation and Dummy Variables - Python'\nformat: html\nexecute:\n  engine: knitr\n---\n\n\n\n### Missing Data Imputation\n\nWe are now moving to Python. You will be able to explore missingness here too, as well as explore the concept of imputation and dummy variables. \n\nThis example illustrates how to apply different preprocessing and feature imputation functions to different subsets of features, using `SimpleImputer` and `KNNImputer`. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to impute the numeric as well as categorical features.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn import set_config\nset_config(display='diagram')\n```\n:::\n\n\nYou can download the dataset we will be using through the link:\n\n-   [LongIsland_Heart_Data.csv](/Users/bravol/Desktop/ML%20Class/Practical/EDA/LongIsland_Heart_Data.csv)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(0)\n\n## Load the LongIsland_Heart_Data Set\nheart_df = pd.read_csv('/Users/l.bravo@bham.ac.uk/Library/CloudStorage/OneDrive-UniversityofBirmingham/Desktop/Practical/EDA/LongIsland_Heart_Data.csv') #change to your own directory\n\nheart_df.describe() #similar to summary() in R\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              age         sex  ...        thal  diagnosis of heart disease\ncount  180.000000  180.000000  ...  180.000000                  200.000000\nmean    59.472222    0.966667  ...    1.411111                    2.520000\nstd      7.718823    0.180006  ...    0.967568                    1.219441\nmin     35.000000    0.000000  ...    1.000000                    1.000000\n25%     55.000000    1.000000  ...    1.000000                    1.000000\n50%     60.000000    1.000000  ...    1.000000                    2.000000\n75%     64.000000    1.000000  ...    1.000000                    4.000000\nmax     77.000000    1.000000  ...    4.000000                    5.000000\n\n[8 rows x 14 columns]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(heart_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      age  sex   cp  trestbps  ...  slope   ca  thal  diagnosis of heart disease\n0    63.0  1.0  4.0      10.0  ...    3.0  1.0   1.0                           3\n1    44.0  1.0  4.0       3.0  ...    1.0  NaN   1.0                           1\n2    60.0  1.0  4.0       5.0  ...    4.0  1.0   NaN                           3\n3    55.0  1.0  4.0      11.0  ...    2.0  1.0   1.0                           2\n4    66.0  1.0  3.0      33.0  ...    3.0  1.0   1.0                           1\n..    ...  ...  ...       ...  ...    ...  ...   ...                         ...\n195  54.0  0.0  4.0      41.0  ...    1.0  1.0   1.0                           2\n196  62.0  1.0  1.0       1.0  ...    1.0  1.0   1.0                           1\n197  55.0  1.0  4.0      37.0  ...    1.0  NaN   3.0                           3\n198  58.0  1.0  NaN       1.0  ...    1.0  1.0   1.0                           1\n199  62.0  1.0  2.0      34.0  ...    1.0  1.0   NaN                           2\n\n[200 rows x 14 columns]\n```\n\n\n:::\n:::\n\n\nNow let's make a smaller dataset with just 4 features:\n\nNumeric Features:\n\n-   `chol`: numeric; -- serum cholestoral in mg/dl\n-   `thalach`: numeric -- maximum heart rate achieved\n\nCategorical Features:\n\n-   `sex`: categories encoded as numeric `{'1 = male', '2=female'}`;\n-   `cp`: ordinal integers `{1, 2, 3, 4}`. -- Value 1: typical angina -- Value 2: atypical angina -- Value 3: non-anginal pain -- Value 4: asymptomatic\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_reduced = heart_df.loc[:, ['chol', 'thalach','sex','cp']]\nprint(X_reduced)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     chol  thalach  sex   cp\n0    60.0     12.0  1.0  4.0\n1     NaN      8.0  1.0  4.0\n2    27.0     19.0  1.0  4.0\n3    39.0     25.0  1.0  4.0\n4    22.0     53.0  1.0  3.0\n..    ...      ...  ...  ...\n195  95.0      NaN  0.0  4.0\n196  30.0      1.0  1.0  1.0\n197  33.0      4.0  1.0  4.0\n198   3.0      1.0  1.0  NaN\n199   NaN     47.0  1.0  2.0\n\n[200 rows x 4 columns]\n```\n\n\n:::\n:::\n\n\n### Visualize Missingness\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsns.heatmap(X_reduced.isnull(), cbar=False, cmap=\"viridis\")\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical1_Final_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Impute Missing Values with SimpleImputer\n\nFor more information on imputation in python go to <https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Define numeric and categorical features\nnumeric_features = ['chol', 'thalach']\ncategorical_features = ['sex','cp']\n```\n:::\n\n\nMake sure categorical values are interpreted as categorical\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_reduced.dtypes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nchol       float64\nthalach    float64\nsex        float64\ncp         float64\ndtype: object\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_reduced[categorical_features] = X_reduced[categorical_features].astype('category')\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_reduced.dtypes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nchol        float64\nthalach     float64\nsex        category\ncp         category\ndtype: object\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Impute numeric features with the mean\nnumeric_imputer = SimpleImputer(strategy='median')\nX_reduced[numeric_features] = numeric_imputer.fit_transform(X_reduced[numeric_features])\n\nprint(X_reduced)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     chol  thalach  sex   cp\n0    60.0     12.0  1.0  4.0\n1    29.0      8.0  1.0  4.0\n2    27.0     19.0  1.0  4.0\n3    39.0     25.0  1.0  4.0\n4    22.0     53.0  1.0  3.0\n..    ...      ...  ...  ...\n195  95.0     19.0  0.0  4.0\n196  30.0      1.0  1.0  1.0\n197  33.0      4.0  1.0  4.0\n198   3.0      1.0  1.0  NaN\n199  29.0     47.0  1.0  2.0\n\n[200 rows x 4 columns]\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Impute categorical features with a constant value ('Unknown')\ncategorical_imputer = SimpleImputer(strategy='constant', fill_value=-1)\nX_reduced[categorical_features] = categorical_imputer.fit_transform(X_reduced[categorical_features])\n\nprint(X_reduced)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     chol  thalach  sex   cp\n0    60.0     12.0  1.0  4.0\n1    29.0      8.0  1.0  4.0\n2    27.0     19.0  1.0  4.0\n3    39.0     25.0  1.0  4.0\n4    22.0     53.0  1.0  3.0\n..    ...      ...  ...  ...\n195  95.0     19.0  0.0  4.0\n196  30.0      1.0  1.0  1.0\n197  33.0      4.0  1.0  4.0\n198   3.0      1.0  1.0 -1.0\n199  29.0     47.0  1.0  2.0\n\n[200 rows x 4 columns]\n```\n\n\n:::\n:::\n\n\nVisualize the imputation. Has it worked?\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsns.heatmap(X_reduced.isnull(), cbar=False, cmap=\"viridis\")\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EDA_Practical1_Final_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n:::\n\n\nNow impute Missing Values with KNNImputer (for Numeric Data)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_reduced_KNN = heart_df.loc[:, ['chol', 'thalach','sex','cp']]\n\nknn_imputer = KNNImputer(n_neighbors=5)\nX_reduced_KNN[numeric_features] = knn_imputer.fit_transform(X_reduced_KNN[numeric_features])\n```\n:::\n\n\nWhat difference can we see in the datasets? Compare Summary Statistics Before and After Imputation\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_reduced_KNN.describe()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             chol     thalach         sex          cp\ncount  200.000000  200.000000  180.000000  180.000000\nmean    36.811000   23.669000    0.966667    3.500000\nstd     28.606273   20.140448    0.180006    0.794534\nmin      1.000000    1.000000    0.000000    1.000000\n25%      6.000000    3.750000    1.000000    3.000000\n50%     31.500000   19.000000    1.000000    4.000000\n75%     58.250000   41.250000    1.000000    4.000000\nmax    100.000000   60.000000    1.000000    4.000000\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_reduced.describe()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             chol    thalach         sex          cp\ncount  200.000000  200.00000  200.000000  200.000000\nmean    35.655000   23.20000    0.770000    3.050000\nstd     28.449655   20.09275    0.615626    1.549031\nmin      1.000000    1.00000   -1.000000   -1.000000\n25%      6.000000    3.75000    1.000000    3.000000\n50%     29.000000   19.00000    1.000000    4.000000\n75%     57.250000   41.25000    1.000000    4.000000\nmax    100.000000   60.00000    1.000000    4.000000\n```\n\n\n:::\n:::\n\n\n## Dummy variables\n\nAs well as imputation we are interested in other transformations. For example, hot encoding categorical data. For extra insight <https://www.kaggle.com/code/marcinrutecki/one-hot-encoding-everything-you-need-to-know>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_reduced_Encoder = heart_df.loc[:, ['chol', 'thalach','sex','cp']]\nX_reduced_Encoder[categorical_features] = X_reduced_Encoder[categorical_features].astype('category')\n\n# Apply OneHotEncoder\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nencoded_categorical = encoder.fit_transform(X_reduced_Encoder[categorical_features])\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nencoded_df = pd.DataFrame(\n    encoded_categorical,\n    columns=encoder.get_feature_names_out(categorical_features)\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Combine the encoded features with the original dataset (drop original categorical columns)\nvisualised_df = pd.concat([X_reduced_Encoder.drop(columns=categorical_features), encoded_df], axis=1)\nprint(visualised_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     chol  thalach  sex_0.0  sex_1.0  ...  cp_2.0  cp_3.0  cp_4.0  cp_nan\n0    60.0     12.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n1     NaN      8.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n2    27.0     19.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n3    39.0     25.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n4    22.0     53.0      0.0      1.0  ...     0.0     1.0     0.0     0.0\n..    ...      ...      ...      ...  ...     ...     ...     ...     ...\n195  95.0      NaN      1.0      0.0  ...     0.0     0.0     1.0     0.0\n196  30.0      1.0      0.0      1.0  ...     0.0     0.0     0.0     0.0\n197  33.0      4.0      0.0      1.0  ...     0.0     0.0     1.0     0.0\n198   3.0      1.0      0.0      1.0  ...     0.0     0.0     0.0     1.0\n199   NaN     47.0      0.0      1.0  ...     1.0     0.0     0.0     0.0\n\n[200 rows x 10 columns]\n```\n\n\n:::\n:::\n\n\nAs you have seen we have gone back to original data so one of the new columns created is nan! To circumvent this apply this to a dataframe AFTER imputation!!!\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n#Impute dataset and then do hot encoding: \n```\n:::\n\n\n",
    "supporting": [
      "EDA_Practical1_Final_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}